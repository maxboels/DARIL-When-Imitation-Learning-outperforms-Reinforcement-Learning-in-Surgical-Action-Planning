debug: false

# Experiment configuration
experiment:
  train:
    max_videos: 40  # default: 40
  test:
    max_videos: 2  # default: 10
    test_on_train: false
    
  # Experiment 1: Autoregressive IL with pre-trained embeddings
  autoregressive_il:
    enabled: true
    train: false
    evaluate: true
    visualization: true
    model_type_preference: 'combined'  # 'current', 'next', 'combined'
    save_all_best_models: true         # Save separate models for each task
    # il_model_path: null
    il_model_path: "best_checkpoints/autoregressive_il_best_combined_epoch_8.pt"
    # il_model_path: "results/2025-06-24_11-09-35/fold0/logs/checkpoints/autoregressive_il_best_epoch_1.pt"
    # il_model_path: "results/2025-06-10_19-45-51/logs/checkpoints/autoregressive_il_best_epoch_1.pt"  # Path to the best IL model    
    # IVT-specific settings
    ivt_saving:
      enabled: true
      current_weight: 0.4              # Weight for current recognition in combined score
      next_weight: 0.6                 # Weight for next prediction in combined score (prioritize this)
      save_frequency: 1                # Save every N epochs (1 = every epoch)
      keep_top_k: 3                    # Keep top K models for each task      
    evaluation_model: 'combined'       # Which model to use for final evaluation
    recognition:                       # We use the results from the Distil-Swin codebase
      enabled: false
      ivt_map: 36.1 # this is the IVT map we get when running the Distil-Swin model, which is 36.1% for 5 folds cross-validation

  # Experiment 2: Conditional World Model (PRESERVED for other experiments)
  world_model:
    enabled: false
    # wm_model_path: null
    wm_model_path: "results/fixed_rl_2025-06-13_19-22-25/logs/checkpoints/world_model_best_epoch_1.pt"

  # Experiment 3: Direct Video RL (PRESERVED for other experiments)
  rl_experiments:
    enabled: false
    eval_episodes: 10

  # Experiment 4: Inverse Reinforcement Learning (PRESERVED for other experiments)
  irl_enhancement:
    enabled: false
    train: false
    evaluate: true
    irl_model_path: null
    num_epochs: 25            # DataLoader epochs instead of iterations default is 25
    task_focus: "next_action_prediction"  # Explicit task definition
    match_il_temporal_structure: true     # Ensure alignment
    scenarios_to_train: ['high_complexity', 'rare_actions', 'critical_moments', 'phase_transitions']
    policy_adjustment:
      num_epochs: 15           # Number of epochs for policy adjustment
    learning_rate: 5e-5
    early_stopping:
      patience: 5
      min_delta: 0.001
    maxent_irl:
      num_iterations: 10       # default 100
      learning_rate: 5e-5
    lightweight_gail:
      num_epochs: 5            # default 50
      learning_rate: 1e-4
      adjustment_strength: 0.1  # How much to adjust IL predictions
    evaluation:
      min_improvement_threshold: 0.02  # 2% minimum improvement to be significant

# ENHANCED: Training parameters optimized for IVT metric tracking
training:
  epochs: 15                    # OPTIMIZED: Sufficient for temporal learning with good features
  batch_size: 16                 # OPTIMIZED: Larger batches possible with good embeddings
  learning_rate: 8e-5           # OPTIMIZED: Lower LR for fine-tuning scenario
  log_every_n_steps: 20         # OPTIMIZED: More frequent logging for shorter training
  
  # Enhanced optimizer configuration
  optimizer_type: "adamw"
  weight_decay: 0.025            # INCREASED: More regularization for pre-trained features
  gradient_clip_val: 0.8        # REDUCED: Gradients should be smaller with good features
  
  # Enhanced scheduler configuration  
  scheduler:
    type: "cosine_with_warmup"  # OPTIMIZED: Better for fine-tuning
    warmup_epochs: 3            # REDUCED: Short warmup since features are good
    eta_min: 8e-6               # Minimum LR
    
  # Enhanced parameter group settings for pre-trained scenario
  parameter_groups:
    bilstm_recognition:
      lr_multiplier: 1.0        # Focus on temporal learning
      weight_decay_multiplier: 0.3
    gpt2_backbone:
      lr_multiplier: 0.3        # Increased for temporal modeling
      weight_decay_multiplier: 0.8
    frame_processing:
      lr_multiplier: 0.5        # Conservative - don't disturb good embeddings
      weight_decay_multiplier: 1.5
    action_prediction:
      lr_multiplier: 1.5        # Moderate - features help a lot
      weight_decay_multiplier: 0.2
    auxiliary_heads:
      lr_multiplier: 0.8        # Standard
      weight_decay_multiplier: 1.0
  
  # Enhanced learning rate monitoring
  lr_monitoring:
    track_gradients: true
    track_parameters: true
    log_frequency: 50
    save_plots: true
    generate_recommendations: true
  
  # Data augmentation optimized for pre-trained features
  augmentation:
    temporal_crop_ratio: 0.85   # More temporal variation
    temporal_jitter_std: 0.02   # Gentle - features are robust
    sequence_shuffle_prob: 0.1  # Learn temporal invariance
    frame_dropout_prob: 0.05    # Regularization technique
    
  # ENHANCED: Early stopping based on IVT metrics
  early_stopping:
    patience: 8                 # Quick stopping with good features
    min_delta: 0.001           # Sensitive to plateaus
    # monitor: "val_action_mAP"
    monitor: 'ivt_next_mAP'  # Use IVT metrics for early stopping
    mode: 'max'

  # ENHANCED: IVT metrics monitoring
  monitor_ivt_metrics: true
  log_ivt_components: true            # Log I, V, T, IV, IT components separately
  tensorboard_ivt_tracking: true      # Track IVT metrics in tensorboard

  # Logging and checkpointing
  num_workers: 2               # REDUCED: Less overhead
  pin_memory: true
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  eval_epoch_interval: 1
  save_model: true
  
  # ENHANCED: Checkpointing with IVT focus
  checkpointing:
    save_best_only: false       # Save intermediate models
    save_top_k: 5              # Keep more checkpoints
    monitor: "ivt_next_mAP"     # Monitor next action prediction mAP
    mode: "max"
    save_every_n_epochs: 5     # Regular checkpoints
    
    # ENHANCED: IVT-specific checkpointing
    save_ivt_milestones: true        # Save when IVT metrics reach milestones
    ivt_milestones: [0.34, 0.35, 0.36, 0.37]  # Save at these mAP levels

  min_current_map_threshold: 0.34
  min_next_map_threshold: 0.34
  min_combined_threshold: 0.34

# ENHANCED: Evaluation configuration with IVT focus
evaluation:
  prediction_horizon: 8         # OPTIMIZED: Shorter horizon with good features
  
  # Enhanced evaluation for IL
  comprehensive_evaluation:
    per_video_analysis: true
    statistical_significance: true
    confidence_intervals: true
    baseline_comparison: 36.1   # Compare against Swin baseline
    
    # ENHANCED: IVT-specific evaluation
    evaluate_ivt_components: true     # Evaluate I, V, T, IV, IT separately
    ivt_video_wise: true             # Use video-wise IVT computation
    report_both_tasks: true          # Report both current and next prediction
  
  # Supervised evaluation (preserved)
  supervised:
    action_prediction: true
    
  # RL evaluation (preserved for other experiments)
  rl:
    rollout_horizon: 15
    use_best_actions: true
  
  # ENHANCED: Model comparison
  comparison:
    statistical_tests: true
    effect_size_threshold: 0.15  # More sensitive
    compare_all_saved_models: true   # Compare current vs next vs combined models
    
  # ENHANCED: Planning evaluation (for next prediction models)
  planning_evaluation:
    enabled: true
    planning_horizons: [1, 2, 3, 5, 10, 20]  # Planning horizons in seconds
    use_best_next_model: true        # Use best next prediction model for planning
  
  # General evaluation (preserved)
  world_model:
    use_memory: false
    overall_horizon: 1

visualization:
  plot_top: 5
  enhanced_planning_analysis: true
  simple_planning_analysis: true
  sparsity_analysis: true
  qualitative_evaluation: true

# RL training configuration (PRESERVED for other experiments)
rl_training:
  action_space_type: 'continuous'  # continuous (defaut), discrete, or hierarchical
  outcome_based_rewards: true
  rl_horizon: 30  # INCREASED episode length
  reward_mode: 'dense'
  normalize_rewards: true
  early_termination: true
  timesteps: 50000 #   # SIGNIFICANTLY INCREASED for better convergence
  
  # IMPROVED Reward weights for environment
  reward_weights:
    expert_matching: 10.0      # NEW: Most important - match expert actions
    action_sparsity: 1.0       # NEW: Appropriate action density  
    world_model_rewards: 0.5   # REDUCED: Lower weight for WM predictions
    completion_bonus: 5.0      # NEW: Episode completion reward
    consistency_bonus: 1.0     # NEW: Action consistency
    
    # Legacy weights (for compatibility)
    phase_completion: 1.0
    risk_penalty: -0.5
  
  # OPTIMIZED Algorithm-specific settings
  ppo:
    learning_rate: 5e-5        # REDUCED for stability
    n_steps: 512               # INCREASED for better estimates
    batch_size: 64             # INCREASED
    n_epochs: 10               # INCREASED
    gamma: 0.95                # ADJUSTED for immediate rewards
    gae_lambda: 0.9
    clip_range: 0.1            # REDUCED for stability
    ent_coef: 0.05             # INCREASED for exploration
    vf_coef: 0.5
    max_grad_norm: 0.5         # ADDED gradient clipping
    
  a2c:
    learning_rate: 1e-4        # REDUCED for stability
    n_steps: 32
    gamma: 0.95                # ADJUSTED
    gae_lambda: 0.9
    ent_coef: 0.05             # INCREASED for exploration
    vf_coef: 0.25
    max_grad_norm: 0.5         # ADDED gradient clipping

# ENHANCED: Data configuration for IVT metrics
data:
  context_length: 20            # OPTIMIZED: Shorter context with good features
  future_length: 20             # OPTIMIZED: Shorter future for temporal learning
  train_shift: 1
  padding_value: 0.0
  max_horizon: 20                # OPTIMIZED: More realistic with strong features
  
  # Feature processing for pre-trained embeddings
  normalize_embeddings: true    # Normalize Swin features
  embedding_noise: 0.01         # Small noise for regularization
  
  # ENHANCED: IVT-specific data handling
  ivt_evaluation_mode: true         # Enable IVT-compatible data processing
  preserve_video_structure: true    # Keep video boundaries for proper IVT evaluation
  
  paths:
    data_dir: "/home/maxboels/datasets/CholecT50"
    # data_dir: "/nfs/home/mboels/datasets/CholecT50"
    class_labels_file_path: "./data/labels.json"
    # Folds: 0, 1, 2, 3, 4
    fold: 0
    metadata_file: "embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv"
    # metadata_file: "embeddings_f1_swin_bas_129.csv"
    # metadata_file: "embeddings_f3_swin_bas_129.csv"
    # metadata_file: "embeddings_f4_swin_bas_129.csv"

    video_global_outcome_file: "embeddings_f1_swin_bas_129_with_enhanced_global_metrics.csv"
  
  frame_risk_agg: 'max'

# Training mode (preserved for other experiments)
training_mode: 'rl'

# Preprocessing (preserved for other experiments)
preprocess:
  extract_rewards: false # already in the passed .csv file bellow
  analyze_rewards: false  # Enable reward analysis during data loading
  rewards:
    grounded:
      phase_completion: true
      phase_transition: true  
      phase_progression: true
      global_progression: true
    imitation:
      action_distribution: true
    expert_knowledge:
      risk_score: true
      frame_risk_agg: 'max'

# Model configurations
models:
  # OPTIMIZED Method 1: Autoregressive IL Model for pre-trained embeddings
  autoregressive_il:
    hidden_dim: 384             # REDUCED: Smaller since features do heavy lifting
    embedding_dim: 1024         # Keep same (Swin output dimension)
    n_layer: 3                  # REDUCED: Less transformation needed
    num_action_classes: 100
    num_phase_classes: 7
    dropout: 0.3                # INCREASED: Stronger regularization
    max_length: 256             # REDUCED: Focus on shorter sequences
    
    # Enhanced regularization for pre-trained scenario
    layer_dropout: 0.15         # Layer-wise dropout
    attention_dropout: 0.2      # Attention-specific dropout
    frame_projection_dropout: 0.2  # Regularize projection layer
    
    # BiLSTM settings optimized for temporal learning
    bilstm_layers: 1            # Simpler temporal modeling
    bilstm_dropout: 0.25        # Moderate regularization
    bilstm_bidirectional: true  # Keep bidirectional for recognition
    
    # ENHANCED: Task-specific tuning
    current_task_weight: 2.0         # Weight for current action recognition loss
    next_task_weight: 1.5            # Weight for next action prediction loss
    consistency_weight: 0.1          # Weight for consistency between tasks
  
  # Method 2: Conditional World Model (PRESERVED for other experiments)
  conditional_world_model:
    hidden_dim: 512           # try a smaller hidden dimension to avoid overfitting
    embedding_dim: 1024
    action_embedding_dim: 128
    n_layer: 4                # REDUCED number of layers to avoid overfitting
    num_action_classes: 100
    num_phase_classes: 7
    dropout: 0.1
    max_sequence_length: 512

# ENHANCED: Loss weighting optimized for IVT metrics
loss_weights:
  recognition: 2.0              # Current action recognition (higher for stability)
  generation: 1.5               # Next action prediction (main evaluation task)
  frame_reconstruction: 0.2     # Lower for IVT focus
  phase_prediction: 0.5         # Moderate weight
  consistency: 0.1              # Task consistency

# Fair evaluation (preserved)
fair_evaluation:
  enabled: true
  include_traditional_metrics: true
  include_clinical_metrics: true
  
  # Enhanced for IL evaluation
  per_video_analysis:
    enabled: true
    save_individual_results: true
    statistical_aggregation: true
    baseline_comparison: 36.1   # Swin baseline
  
  clinical_outcome_weights:
    phase_progression: 2.0
    innovation: 0.5

# Supervised Learning Configuration (preserved)
supervised_learning:
  data_augmentation: true       # ENABLED: Use augmentation for small dataset

# Research comparison settings (preserved)
research_comparison:
  methods: ['autoregressive_il', 'conditional_world_model', 'direct_video_rl']

# Advanced configurations (preserved)
advanced:
  mixed_precision: false

# Hardware optimization (optimized for IL scenario)
hardware:
  persistent_workers: false     # Less overhead for small dataset
  mixed_precision: false        # Keep full precision for analysis

# RL Debugging Configuration (PRESERVED for other experiments)
rl_debugging:
  enabled: true
  save_training_curves: true
  monitor_expert_matching: true
  log_action_distributions: true
  convergence_analysis: true
  
  # Logging frequency
  episode_log_frequency: 10    # Log every 10 episodes
  eval_frequency: 1000         # Evaluate every 1000 steps
  
  # Debugging thresholds
  reward_improvement_threshold: 0.1
  expert_matching_threshold: 0.5
  
  # Save paths
  debug_dir: "rl_debug"
  plot_dir: "rl_plots"

# NEW: Enhanced debugging for IL with pre-trained features
il_debugging:
  enabled: true
  
  # Monitor fast convergence scenario
  convergence_tracking:
    early_plateau_detection: true
    overfitting_detection: true
    feature_utilization_analysis: true
    
  # Compare with Swin baseline
  baseline_comparison:
    swin_recognition_mAP: 36.1  # Your reported Swin performance
    improvement_threshold: 1.0   # Expect modest improvement
    
  # Temporal learning assessment
  temporal_evaluation:
    short_term_prediction: true  # 1-2 steps ahead
    medium_term_prediction: true # 3-5 steps ahead
    temporal_consistency: true
    sequence_modeling_analysis: true

# ENHANCED: Logging configuration for IVT tracking
logging:
  level: "INFO"
  log_ivt_details: true              # Log detailed IVT metrics
  save_ivt_plots: true               # Save IVT performance plots
  ivt_component_analysis: true       # Analyze I, V, T components separately
  
  # Progress tracking
  log_best_scores: true              # Log when new best scores achieved
  log_model_saves: true              # Log when models are saved

# ENHANCED: Performance targets with IVT focus
il_performance_targets:
  # Current action recognition targets
  current_recognition_mAP_target: 35.0     # Target for current action mAP
  current_recognition_minimum: 30.0        # Minimum acceptable performance
  
  # Next action prediction targets (main evaluation)
  next_prediction_mAP_target: 32.0         # Target for next action mAP  
  next_prediction_minimum: 25.0            # Minimum acceptable performance
  
  # Combined performance targets
  combined_score_target: 33.0              # Target for combined score
  improvement_over_baseline: 2.0           # Must beat baseline by this much
  
  # Training targets
  convergence_epoch: 8                     # Should converge by this epoch
  maximum_overfitting: 5.0                 # Train-val gap should be <5%
  
  # IVT component targets
  ivt_instrument_target: 85.0              # Target for I component
  ivt_verb_target: 65.0                    # Target for V component  
  ivt_target_target: 45.0                  # Target for T component