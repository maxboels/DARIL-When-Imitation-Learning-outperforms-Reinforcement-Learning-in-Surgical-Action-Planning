data:
  context_length: 20
  frame_risk_agg: max
  max_horizon: 15
  max_videos: 2
  padding_value: 0.0
  paths:
    class_labels_file_path: ./data/labels.json
    data_dir: /home/maxboels/datasets/CholecT50
    fold: 0
    metadata_file: embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
    video_global_outcome_file: embeddings_f0_swin_bas_129_with_enhanced_global_metrics.csv
  risk_score_path: null
  train_shift: 1
debug: false
evaluation:
  max_videos: 5
  world_model:
    eval_horizons:
    - 1
    - 3
    - 5
    - 10
    - 15
    max_horizon: 15
    overall_horizon: 1
    rollout_horizon: 15
    top_ks:
    - 1
    - 3
    - 5
    - 10
    use_memory: false
experiment:
  analyze_results: true
  calculate_action_rewards: false
  max_videos: 2
  pretrain_reward_model: false
  recognition:
    best_model_path: ./best_checkpoints/best_model_epoch2_map0.5008.pt
    inference: false
    train: false
  rl_comparison:
    baseline_imitation: true
    enabled: true
    run_ppo: true
    run_sac: true
    run_td_mpc2: false
  run_evaluation: true
  run_tdmpc: false
  train_action_policy: false
  world_model:
    best_model_path: logs/2025-05-08_14-14-22/checkpoints/world_model_best_epoch_2.pt
    inference: true
    train: false
models:
  recognition:
    transformer:
      dropout: 0.1
      embedding_dim: 1024
      hidden_dim: 768
      num_action_classes: 100
      num_heads: 8
      num_instrument_classes: 6
      num_layers: 2
  reward:
    context_length: 5
    hidden_dim: 256
    input_dim: 1024
    num_heads: 4
    num_layers: 2
  world_model:
    action_conditioning: true
    action_embedding_dim: 100
    action_learning: true
    embedding_dim: 1024
    hidden_dim: 768
    imitation_learning: true
    loss_weights:
      w_R: null
      w_a: 1.0
      w_q: null
      w_r: 1.0
      w_z: 1.0
    n_layer: 6
    num_action_classes: 100
    num_outcomes: 1
    num_phase_classes: 7
    outcome_learning: false
    phase_learning: true
    reward_learning: true
    target_heads:
    - _a
    targets_dims:
      _R: 1
      _a: 100
      _q: 1
      _r: 1
      _z: 512
    use_head: true
preprocess:
  extract_rewards: true
  rewards:
    expert_knowledge:
      frame_risk_agg: max
      risk_score: true
    grounded:
      global_progression: true
      phase_completion: true
      phase_progression: true
      phase_transition: true
    imitation:
      action_distribution: true
  value:
    global_outcome: true
rl_experiments:
  algorithms:
  - imitation_learning
  - ppo
  - sac
  enabled: true
  eval_episodes: 10
  eval_horizon: 30
  horizon: 50
  ppo:
    batch_size: 64
    clip_range: 0.2
    gae_lambda: 0.95
    gamma: 0.99
    learning_rate: 3e-4
    n_epochs: 10
    n_steps: 2048
  reward_weights:
    _r_action_probability: 0.3
    _r_global_progression: 0.8
    _r_phase_completion: 1.0
    _r_phase_initiation: 0.5
    _r_phase_progression: 1.0
    _r_risk: -0.5
  sac:
    batch_size: 256
    buffer_size: 100000
    gamma: 0.99
    gradient_steps: 1
    learning_rate: 3e-4
    learning_starts: 1000
    tau: 0.005
    train_freq: 1
  timesteps: 50000
training:
  batch_size: 16
  checkpoint_dir: checkpoints
  epochs: 5
  eval_epoch_interval: 1
  gradient_clip_val: 1.0
  learning_rate: 3.0e-05
  log_dir: logs
  log_every_n_steps: 80
  num_workers: 4
  pin_memory: true
  save_checkpoint_every_n_epochs: 2
  save_model: true
  scheduler:
    type: cosine
    warmup_steps: 1000
  weight_decay: 0.1
