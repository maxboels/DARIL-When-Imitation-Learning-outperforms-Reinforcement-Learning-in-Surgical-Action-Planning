2025-05-28 18:56:47,030 - INFO - Log file: logs/2025-05-28_18-56-47/il_vs_rl_comparison.log
2025-05-28 18:56:47,032 - INFO - üöÄ Starting IL vs RL Comparison Experiment
2025-05-28 18:56:47,032 - INFO - Device: cuda
2025-05-28 18:56:47,032 - INFO - Results will be saved to: logs/2025-05-28_18-56-47/comparison_results
2025-05-28 18:56:47,032 - INFO - üìä Loading dataset...
2025-05-28 18:56:47,032 - INFO - Loading train data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-05-28 18:56:48,656 - INFO - [Train] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-05-28 18:56:48,657 - INFO - [Train] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-05-28 18:56:48,661 - INFO - [Train] Found 40 videos in metadata file
2025-05-28 18:56:48,661 - INFO - [Train] Loading data for 40 videos
2025-05-28 18:58:56,745 - INFO - Loading test data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-05-28 18:58:57,342 - INFO - [Test] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-05-28 18:58:57,342 - INFO - [Test] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-05-28 18:58:57,344 - INFO - [Test] Found 10 videos in metadata file
2025-05-28 18:58:57,344 - INFO - [Test] Loading data for 10 videos
2025-05-28 18:59:37,147 - INFO - Loaded 40 training videos and 10 test videos
2025-05-28 18:59:37,148 - INFO - üéì Training Imitation Learning Model...
2025-05-28 18:59:37,148 - INFO - Training supervised imitation learning model
2025-05-28 19:00:05,544 - INFO - Model parameters: 48,924,145 total, 48,924,145 trainable
2025-05-28 19:00:05,557 - INFO - Starting supervised training for autoregressive action prediction
2025-05-28 19:00:07,114 - INFO - Batch 0/4933 | Loss: 3.2355
2025-05-28 19:00:11,969 - INFO - Batch 100/4933 | Loss: 0.7929
2025-05-28 19:00:16,255 - INFO - Batch 200/4933 | Loss: 0.5577
2025-05-28 19:00:21,419 - INFO - Batch 300/4933 | Loss: 0.5746
2025-05-28 19:00:26,713 - INFO - Batch 400/4933 | Loss: 0.5535
2025-05-28 19:00:31,412 - INFO - Batch 500/4933 | Loss: 0.4127
2025-05-28 19:00:36,068 - INFO - Batch 600/4933 | Loss: 0.4550
2025-05-28 19:00:40,866 - INFO - Batch 700/4933 | Loss: 0.3613
2025-05-28 19:00:45,907 - INFO - Batch 800/4933 | Loss: 0.3586
2025-05-28 19:00:51,785 - INFO - Batch 900/4933 | Loss: 0.2976
2025-05-28 19:00:58,459 - INFO - Batch 1000/4933 | Loss: 0.3591
2025-05-28 19:01:05,438 - INFO - Batch 1100/4933 | Loss: 0.2449
2025-05-28 19:01:11,745 - INFO - Batch 1200/4933 | Loss: 0.3050
2025-05-28 19:01:18,059 - INFO - Batch 1300/4933 | Loss: 0.4432
2025-05-28 19:01:24,584 - INFO - Batch 1400/4933 | Loss: 0.2757
2025-05-28 19:01:31,112 - INFO - Batch 1500/4933 | Loss: 0.2364
2025-05-28 19:01:37,545 - INFO - Batch 1600/4933 | Loss: 0.3079
2025-05-28 19:01:43,939 - INFO - Batch 1700/4933 | Loss: 0.2242
2025-05-28 19:01:50,457 - INFO - Batch 1800/4933 | Loss: 0.2460
2025-05-28 19:01:57,050 - INFO - Batch 1900/4933 | Loss: 0.2141
2025-05-28 19:02:03,354 - INFO - Batch 2000/4933 | Loss: 0.2370
2025-05-28 19:02:09,888 - INFO - Batch 2100/4933 | Loss: 0.2483
2025-05-28 19:02:14,992 - INFO - Batch 2200/4933 | Loss: 0.3607
2025-05-28 19:02:20,204 - INFO - Batch 2300/4933 | Loss: 0.2269
2025-05-28 19:02:27,854 - INFO - Batch 2400/4933 | Loss: 0.2253
2025-05-28 19:02:34,846 - INFO - Batch 2500/4933 | Loss: 0.3478
2025-05-28 19:02:39,810 - INFO - Batch 2600/4933 | Loss: 0.2854
2025-05-28 19:02:44,533 - INFO - Batch 2700/4933 | Loss: 0.1941
2025-05-28 19:02:48,918 - INFO - Batch 2800/4933 | Loss: 0.2349
2025-05-28 19:02:53,352 - INFO - Batch 2900/4933 | Loss: 0.2096
2025-05-28 19:02:58,064 - INFO - Batch 3000/4933 | Loss: 0.2361
2025-05-28 19:03:02,624 - INFO - Batch 3100/4933 | Loss: 0.2773
2025-05-28 19:03:07,273 - INFO - Batch 3200/4933 | Loss: 0.2455
2025-05-28 19:03:11,848 - INFO - Batch 3300/4933 | Loss: 0.1991
2025-05-28 19:03:16,633 - INFO - Batch 3400/4933 | Loss: 0.2417
2025-05-28 19:03:21,250 - INFO - Batch 3500/4933 | Loss: 0.1969
2025-05-28 19:03:25,700 - INFO - Batch 3600/4933 | Loss: 0.2075
2025-05-28 19:03:30,276 - INFO - Batch 3700/4933 | Loss: 0.1992
2025-05-28 19:03:34,769 - INFO - Batch 3800/4933 | Loss: 0.2093
2025-05-28 19:03:39,761 - INFO - Batch 3900/4933 | Loss: 0.1769
2025-05-28 19:03:44,222 - INFO - Batch 4000/4933 | Loss: 0.2626
2025-05-28 19:03:49,154 - INFO - Batch 4100/4933 | Loss: 0.1607
2025-05-28 19:03:54,092 - INFO - Batch 4200/4933 | Loss: 0.2142
2025-05-28 19:03:58,721 - INFO - Batch 4300/4933 | Loss: 0.2074
2025-05-28 19:04:03,712 - INFO - Batch 4400/4933 | Loss: 0.1761
2025-05-28 19:04:08,777 - INFO - Batch 4500/4933 | Loss: 0.2078
2025-05-28 19:04:13,563 - INFO - Batch 4600/4933 | Loss: 0.2165
2025-05-28 19:04:18,553 - INFO - Batch 4700/4933 | Loss: 0.1681
2025-05-28 19:04:23,225 - INFO - Batch 4800/4933 | Loss: 0.1952
2025-05-28 19:04:28,161 - INFO - Batch 4900/4933 | Loss: 0.1751
2025-05-28 19:32:46,642 - INFO - New best model saved: logs/2025-05-28_18-56-47/checkpoints/supervised_best_epoch_1.pt
2025-05-28 19:32:46,643 - INFO - Epoch 1/5 | Train Loss: 0.3187 | Val Loss: 0.2015 | Val Action Acc: 0.0000
2025-05-28 19:32:47,636 - INFO - Batch 0/4933 | Loss: 0.2217
2025-05-28 19:32:53,447 - INFO - Batch 100/4933 | Loss: 0.1821
2025-05-28 19:32:59,629 - INFO - Batch 200/4933 | Loss: 0.2207
2025-05-28 19:33:05,686 - INFO - Batch 300/4933 | Loss: 0.1762
2025-05-28 19:33:11,516 - INFO - Batch 400/4933 | Loss: 0.1743
2025-05-28 19:33:17,529 - INFO - Batch 500/4933 | Loss: 0.1805
2025-05-28 19:33:23,523 - INFO - Batch 600/4933 | Loss: 0.2624
2025-05-28 19:33:29,316 - INFO - Batch 700/4933 | Loss: 0.1905
2025-05-28 19:33:35,211 - INFO - Batch 800/4933 | Loss: 0.1642
2025-05-28 19:33:40,951 - INFO - Batch 900/4933 | Loss: 0.1668
2025-05-28 19:33:46,723 - INFO - Batch 1000/4933 | Loss: 0.2009
2025-05-28 19:33:51,260 - INFO - Batch 1100/4933 | Loss: 0.1669
2025-05-28 19:33:54,977 - INFO - Batch 1200/4933 | Loss: 0.1505
2025-05-28 19:34:00,161 - INFO - Batch 1300/4933 | Loss: 0.1731
2025-05-28 19:34:07,141 - INFO - Batch 1400/4933 | Loss: 0.2352
2025-05-28 19:34:13,787 - INFO - Batch 1500/4933 | Loss: 0.1993
2025-05-28 19:34:17,885 - INFO - Batch 1600/4933 | Loss: 0.1779
2025-05-28 19:34:21,554 - INFO - Batch 1700/4933 | Loss: 0.1626
2025-05-28 19:34:25,217 - INFO - Batch 1800/4933 | Loss: 0.2206
2025-05-28 19:34:28,709 - INFO - Batch 1900/4933 | Loss: 0.1622
2025-05-28 19:34:32,244 - INFO - Batch 2000/4933 | Loss: 0.1843
2025-05-28 19:34:35,572 - INFO - Batch 2100/4933 | Loss: 0.2072
2025-05-28 19:34:39,119 - INFO - Batch 2200/4933 | Loss: 0.2170
2025-05-28 19:34:42,639 - INFO - Batch 2300/4933 | Loss: 0.1422
2025-05-28 19:34:46,114 - INFO - Batch 2400/4933 | Loss: 0.1367
2025-05-28 19:34:49,432 - INFO - Batch 2500/4933 | Loss: 0.1813
2025-05-28 19:34:52,889 - INFO - Batch 2600/4933 | Loss: 0.1522
2025-05-28 19:34:56,537 - INFO - Batch 2700/4933 | Loss: 0.1492
2025-05-28 19:35:00,037 - INFO - Batch 2800/4933 | Loss: 0.1423
2025-05-28 19:35:03,397 - INFO - Batch 2900/4933 | Loss: 0.1724
2025-05-28 19:35:06,850 - INFO - Batch 3000/4933 | Loss: 0.1756
2025-05-28 19:35:10,311 - INFO - Batch 3100/4933 | Loss: 0.1382
2025-05-28 19:35:13,648 - INFO - Batch 3200/4933 | Loss: 0.1377
2025-05-28 19:35:17,065 - INFO - Batch 3300/4933 | Loss: 0.1503
2025-05-28 19:35:20,514 - INFO - Batch 3400/4933 | Loss: 0.1749
2025-05-28 19:35:24,040 - INFO - Batch 3500/4933 | Loss: 0.1433
2025-05-28 19:35:27,570 - INFO - Batch 3600/4933 | Loss: 0.1954
2025-05-28 19:35:31,050 - INFO - Batch 3700/4933 | Loss: 0.1527
2025-05-28 19:35:34,520 - INFO - Batch 3800/4933 | Loss: 0.2549
2025-05-28 19:35:37,907 - INFO - Batch 3900/4933 | Loss: 0.2218
2025-05-28 19:35:41,365 - INFO - Batch 4000/4933 | Loss: 0.2166
2025-05-28 19:35:44,829 - INFO - Batch 4100/4933 | Loss: 0.1783
2025-05-28 19:35:48,266 - INFO - Batch 4200/4933 | Loss: 0.1589
2025-05-28 19:35:51,816 - INFO - Batch 4300/4933 | Loss: 0.1370
2025-05-28 19:35:55,364 - INFO - Batch 4400/4933 | Loss: 0.1612
2025-05-28 19:35:58,764 - INFO - Batch 4500/4933 | Loss: 0.1316
2025-05-28 19:36:02,288 - INFO - Batch 4600/4933 | Loss: 0.1523
2025-05-28 19:36:05,931 - INFO - Batch 4700/4933 | Loss: 0.1932
2025-05-28 19:36:09,236 - INFO - Batch 4800/4933 | Loss: 0.1803
2025-05-28 19:36:12,669 - INFO - Batch 4900/4933 | Loss: 0.1275
2025-05-28 20:03:08,991 - INFO - New best model saved: logs/2025-05-28_18-56-47/checkpoints/supervised_best_epoch_2.pt
2025-05-28 20:03:08,991 - INFO - Epoch 2/5 | Train Loss: 0.1846 | Val Loss: 0.1968 | Val Action Acc: 0.0000
2025-05-28 20:03:09,660 - INFO - Batch 0/4933 | Loss: 0.1907
2025-05-28 20:03:13,129 - INFO - Batch 100/4933 | Loss: 0.2328
2025-05-28 20:03:16,609 - INFO - Batch 200/4933 | Loss: 0.2090
2025-05-28 20:03:20,083 - INFO - Batch 300/4933 | Loss: 0.1775
2025-05-28 20:03:23,528 - INFO - Batch 400/4933 | Loss: 0.1324
2025-05-28 20:03:26,901 - INFO - Batch 500/4933 | Loss: 0.1314
2025-05-28 20:03:30,279 - INFO - Batch 600/4933 | Loss: 0.1342
2025-05-28 20:03:33,835 - INFO - Batch 700/4933 | Loss: 0.1670
2025-05-28 20:03:37,382 - INFO - Batch 800/4933 | Loss: 0.1605
2025-05-28 20:03:40,830 - INFO - Batch 900/4933 | Loss: 0.1546
2025-05-28 20:03:44,228 - INFO - Batch 1000/4933 | Loss: 0.2389
2025-05-28 20:03:47,670 - INFO - Batch 1100/4933 | Loss: 0.1591
2025-05-28 20:03:51,006 - INFO - Batch 1200/4933 | Loss: 0.1748
2025-05-28 20:03:54,487 - INFO - Batch 1300/4933 | Loss: 0.2028
2025-05-28 20:03:58,114 - INFO - Batch 1400/4933 | Loss: 0.1689
2025-05-28 20:04:01,844 - INFO - Batch 1500/4933 | Loss: 0.1954
2025-05-28 20:04:05,443 - INFO - Batch 1600/4933 | Loss: 0.1584
2025-05-28 20:04:08,842 - INFO - Batch 1700/4933 | Loss: 0.2089
2025-05-28 20:04:12,333 - INFO - Batch 1800/4933 | Loss: 0.1532
2025-05-28 20:04:15,814 - INFO - Batch 1900/4933 | Loss: 0.1639
2025-05-28 20:04:19,319 - INFO - Batch 2000/4933 | Loss: 0.1778
2025-05-28 20:04:22,790 - INFO - Batch 2100/4933 | Loss: 0.2139
2025-05-28 20:04:26,207 - INFO - Batch 2200/4933 | Loss: 0.1802
2025-05-28 20:04:29,636 - INFO - Batch 2300/4933 | Loss: 0.1776
2025-05-28 20:04:33,099 - INFO - Batch 2400/4933 | Loss: 0.1496
2025-05-28 20:04:36,964 - INFO - Batch 2500/4933 | Loss: 0.1587
2025-05-28 20:04:40,460 - INFO - Batch 2600/4933 | Loss: 0.1410
2025-05-28 20:04:44,062 - INFO - Batch 2700/4933 | Loss: 0.1245
2025-05-28 20:04:49,365 - INFO - Batch 2800/4933 | Loss: 0.1550
2025-05-28 20:04:55,211 - INFO - Batch 2900/4933 | Loss: 0.1457
2025-05-28 20:05:02,739 - INFO - Batch 3000/4933 | Loss: 0.1567
2025-05-28 20:05:08,361 - INFO - Batch 3100/4933 | Loss: 0.1246
2025-05-28 20:05:14,160 - INFO - Batch 3200/4933 | Loss: 0.1432
2025-05-28 20:05:19,996 - INFO - Batch 3300/4933 | Loss: 0.1477
2025-05-28 20:05:25,753 - INFO - Batch 3400/4933 | Loss: 0.1591
2025-05-28 20:05:31,650 - INFO - Batch 3500/4933 | Loss: 0.2172
2025-05-28 20:05:37,254 - INFO - Batch 3600/4933 | Loss: 0.1517
2025-05-28 20:05:43,261 - INFO - Batch 3700/4933 | Loss: 0.1387
2025-05-28 20:05:48,953 - INFO - Batch 3800/4933 | Loss: 0.1614
2025-05-28 20:05:54,840 - INFO - Batch 3900/4933 | Loss: 0.1321
2025-05-28 20:06:00,550 - INFO - Batch 4000/4933 | Loss: 0.2022
2025-05-28 20:06:06,344 - INFO - Batch 4100/4933 | Loss: 0.1275
2025-05-28 20:06:10,766 - INFO - Batch 4200/4933 | Loss: 0.1611
2025-05-28 20:06:14,348 - INFO - Batch 4300/4933 | Loss: 0.1262
2025-05-28 20:06:19,059 - INFO - Batch 4400/4933 | Loss: 0.2797
2025-05-28 20:06:26,049 - INFO - Batch 4500/4933 | Loss: 0.1816
2025-05-28 20:06:32,477 - INFO - Batch 4600/4933 | Loss: 0.1899
2025-05-28 20:06:36,818 - INFO - Batch 4700/4933 | Loss: 0.1799
2025-05-28 20:06:40,153 - INFO - Batch 4800/4933 | Loss: 0.2263
2025-05-28 20:06:43,528 - INFO - Batch 4900/4933 | Loss: 0.1714
2025-05-28 20:33:49,011 - INFO - New best model saved: logs/2025-05-28_18-56-47/checkpoints/supervised_best_epoch_3.pt
2025-05-28 20:33:49,012 - INFO - Epoch 3/5 | Train Loss: 0.1572 | Val Loss: 0.1939 | Val Action Acc: 0.0000
2025-05-28 20:33:49,877 - INFO - Batch 0/4933 | Loss: 0.2076
2025-05-28 20:33:53,316 - INFO - Batch 100/4933 | Loss: 0.1552
2025-05-28 20:33:56,697 - INFO - Batch 200/4933 | Loss: 0.1420
2025-05-28 20:34:00,318 - INFO - Batch 300/4933 | Loss: 0.1263
2025-05-28 20:34:03,815 - INFO - Batch 400/4933 | Loss: 0.1346
2025-05-28 20:34:07,181 - INFO - Batch 500/4933 | Loss: 0.1610
2025-05-28 20:34:10,633 - INFO - Batch 600/4933 | Loss: 0.1300
2025-05-28 20:34:14,256 - INFO - Batch 700/4933 | Loss: 0.1313
2025-05-28 20:34:17,838 - INFO - Batch 800/4933 | Loss: 0.1464
2025-05-28 20:34:21,433 - INFO - Batch 900/4933 | Loss: 0.1333
2025-05-28 20:34:25,003 - INFO - Batch 1000/4933 | Loss: 0.1772
2025-05-28 20:34:28,409 - INFO - Batch 1100/4933 | Loss: 0.1684
2025-05-28 20:34:31,882 - INFO - Batch 1200/4933 | Loss: 0.1803
2025-05-28 20:34:35,353 - INFO - Batch 1300/4933 | Loss: 0.1286
2025-05-28 20:34:38,987 - INFO - Batch 1400/4933 | Loss: 0.1172
2025-05-28 20:34:42,760 - INFO - Batch 1500/4933 | Loss: 0.1413
2025-05-28 20:34:46,107 - INFO - Batch 1600/4933 | Loss: 0.1202
2025-05-28 20:34:49,592 - INFO - Batch 1700/4933 | Loss: 0.1080
2025-05-28 20:34:52,939 - INFO - Batch 1800/4933 | Loss: 0.1574
2025-05-28 20:34:56,434 - INFO - Batch 1900/4933 | Loss: 0.1338
2025-05-28 20:34:59,939 - INFO - Batch 2000/4933 | Loss: 0.1151
2025-05-28 20:35:03,585 - INFO - Batch 2100/4933 | Loss: 0.1372
2025-05-28 20:35:06,962 - INFO - Batch 2200/4933 | Loss: 0.1453
2025-05-28 20:35:10,180 - INFO - Batch 2300/4933 | Loss: 0.1221
2025-05-28 20:35:13,515 - INFO - Batch 2400/4933 | Loss: 0.1180
2025-05-28 20:35:16,892 - INFO - Batch 2500/4933 | Loss: 0.1373
2025-05-28 20:35:20,367 - INFO - Batch 2600/4933 | Loss: 0.1606
2025-05-28 20:35:23,808 - INFO - Batch 2700/4933 | Loss: 0.1704
2025-05-28 20:35:27,283 - INFO - Batch 2800/4933 | Loss: 0.1180
2025-05-28 20:35:30,814 - INFO - Batch 2900/4933 | Loss: 0.1151
2025-05-28 20:35:34,240 - INFO - Batch 3000/4933 | Loss: 0.1100
2025-05-28 20:35:37,632 - INFO - Batch 3100/4933 | Loss: 0.1426
2025-05-28 20:35:41,328 - INFO - Batch 3200/4933 | Loss: 0.1321
2025-05-28 20:35:44,934 - INFO - Batch 3300/4933 | Loss: 0.1365
2025-05-28 20:35:48,810 - INFO - Batch 3400/4933 | Loss: 0.1155
2025-05-28 20:35:53,051 - INFO - Batch 3500/4933 | Loss: 0.1539
2025-05-28 20:35:59,036 - INFO - Batch 3600/4933 | Loss: 0.1219
2025-05-28 20:36:04,803 - INFO - Batch 3700/4933 | Loss: 0.1078
2025-05-28 20:36:10,553 - INFO - Batch 3800/4933 | Loss: 0.1450
2025-05-28 20:36:16,217 - INFO - Batch 3900/4933 | Loss: 0.1009
2025-05-28 20:36:21,934 - INFO - Batch 4000/4933 | Loss: 0.1490
2025-05-28 20:36:27,811 - INFO - Batch 4100/4933 | Loss: 0.1162
2025-05-28 20:36:33,525 - INFO - Batch 4200/4933 | Loss: 0.1558
2025-05-28 20:36:39,368 - INFO - Batch 4300/4933 | Loss: 0.1758
2025-05-28 20:36:45,142 - INFO - Batch 4400/4933 | Loss: 0.1270
2025-05-28 20:36:50,837 - INFO - Batch 4500/4933 | Loss: 0.1612
2025-05-28 20:36:56,592 - INFO - Batch 4600/4933 | Loss: 0.1534
2025-05-28 20:37:02,332 - INFO - Batch 4700/4933 | Loss: 0.1396
2025-05-28 20:37:07,988 - INFO - Batch 4800/4933 | Loss: 0.1697
2025-05-28 20:37:12,215 - INFO - Batch 4900/4933 | Loss: 0.1720
2025-05-28 21:05:20,455 - INFO - Epoch 4/5 | Train Loss: 0.1434 | Val Loss: 0.1978 | Val Action Acc: 0.0000
2025-05-28 21:05:21,146 - INFO - Batch 0/4933 | Loss: 0.1064
2025-05-28 21:05:24,784 - INFO - Batch 100/4933 | Loss: 0.1367
2025-05-28 21:05:28,231 - INFO - Batch 200/4933 | Loss: 0.1274
2025-05-28 21:05:31,758 - INFO - Batch 300/4933 | Loss: 0.1451
2025-05-28 21:05:34,900 - INFO - Batch 400/4933 | Loss: 0.1720
2025-05-28 21:05:38,260 - INFO - Batch 500/4933 | Loss: 0.1176
2025-05-28 21:05:41,578 - INFO - Batch 600/4933 | Loss: 0.1592
2025-05-28 21:05:45,140 - INFO - Batch 700/4933 | Loss: 0.1462
2025-05-28 21:05:48,513 - INFO - Batch 800/4933 | Loss: 0.1153
2025-05-28 21:05:51,836 - INFO - Batch 900/4933 | Loss: 0.1145
2025-05-28 21:05:55,216 - INFO - Batch 1000/4933 | Loss: 0.1720
2025-05-28 21:05:58,530 - INFO - Batch 1100/4933 | Loss: 0.1260
2025-05-28 21:06:01,966 - INFO - Batch 1200/4933 | Loss: 0.1176
2025-05-28 21:06:05,591 - INFO - Batch 1300/4933 | Loss: 0.1279
2025-05-28 21:06:08,921 - INFO - Batch 1400/4933 | Loss: 0.1177
2025-05-28 21:06:12,237 - INFO - Batch 1500/4933 | Loss: 0.1433
2025-05-28 21:06:15,590 - INFO - Batch 1600/4933 | Loss: 0.1284
2025-05-28 21:06:18,938 - INFO - Batch 1700/4933 | Loss: 0.1163
2025-05-28 21:06:22,429 - INFO - Batch 1800/4933 | Loss: 0.1147
2025-05-28 21:06:25,918 - INFO - Batch 1900/4933 | Loss: 0.1577
2025-05-28 21:06:29,504 - INFO - Batch 2000/4933 | Loss: 0.1559
2025-05-28 21:06:33,060 - INFO - Batch 2100/4933 | Loss: 0.1133
2025-05-28 21:06:36,442 - INFO - Batch 2200/4933 | Loss: 0.1429
2025-05-28 21:06:39,902 - INFO - Batch 2300/4933 | Loss: 0.1183
2025-05-28 21:06:43,258 - INFO - Batch 2400/4933 | Loss: 0.1336
2025-05-28 21:06:46,841 - INFO - Batch 2500/4933 | Loss: 0.1230
2025-05-28 21:06:50,411 - INFO - Batch 2600/4933 | Loss: 0.1431
2025-05-28 21:06:53,812 - INFO - Batch 2700/4933 | Loss: 0.1173
2025-05-28 21:06:57,192 - INFO - Batch 2800/4933 | Loss: 0.1281
2025-05-28 21:07:00,739 - INFO - Batch 2900/4933 | Loss: 0.1470
2025-05-28 21:07:04,369 - INFO - Batch 3000/4933 | Loss: 0.1636
2025-05-28 21:07:07,843 - INFO - Batch 3100/4933 | Loss: 0.1238
2025-05-28 21:07:11,407 - INFO - Batch 3200/4933 | Loss: 0.1438
2025-05-28 21:07:14,704 - INFO - Batch 3300/4933 | Loss: 0.1588
2025-05-28 21:07:18,083 - INFO - Batch 3400/4933 | Loss: 0.1725
2025-05-28 21:07:21,433 - INFO - Batch 3500/4933 | Loss: 0.1903
2025-05-28 21:07:24,964 - INFO - Batch 3600/4933 | Loss: 0.1175
2025-05-28 21:07:28,474 - INFO - Batch 3700/4933 | Loss: 0.1356
2025-05-28 21:07:31,737 - INFO - Batch 3800/4933 | Loss: 0.1270
2025-05-28 21:07:35,085 - INFO - Batch 3900/4933 | Loss: 0.1322
2025-05-28 21:07:38,558 - INFO - Batch 4000/4933 | Loss: 0.1148
2025-05-28 21:07:42,083 - INFO - Batch 4100/4933 | Loss: 0.1286
2025-05-28 21:07:45,504 - INFO - Batch 4200/4933 | Loss: 0.0968
2025-05-28 21:07:48,866 - INFO - Batch 4300/4933 | Loss: 0.1473
2025-05-28 21:07:52,331 - INFO - Batch 4400/4933 | Loss: 0.0964
2025-05-28 21:07:55,904 - INFO - Batch 4500/4933 | Loss: 0.1303
2025-05-28 21:07:59,250 - INFO - Batch 4600/4933 | Loss: 0.1347
2025-05-28 21:08:02,641 - INFO - Batch 4700/4933 | Loss: 0.1074
2025-05-28 21:08:06,086 - INFO - Batch 4800/4933 | Loss: 0.1349
2025-05-28 21:08:09,537 - INFO - Batch 4900/4933 | Loss: 0.1273
2025-05-28 21:35:42,366 - INFO - Epoch 5/5 | Train Loss: 0.1353 | Val Loss: 0.1964 | Val Action Acc: 0.0000
2025-05-28 21:35:43,308 - INFO - Training plots saved to logs/2025-05-28_18-56-47
2025-05-28 21:35:43,759 - INFO - Training completed. Best model: logs/2025-05-28_18-56-47/checkpoints/supervised_best_epoch_3.pt
2025-05-28 21:35:43,759 - INFO - Final model: logs/2025-05-28_18-56-47/checkpoints/final_model.pt
2025-05-28 21:35:43,760 - INFO - ‚úÖ IL training completed. Model saved: logs/2025-05-28_18-56-47/checkpoints/supervised_best_epoch_3.pt
2025-05-28 21:35:48,466 - INFO - ü§ñ Training RL Models...
2025-05-28 21:35:48,475 - ERROR - ‚ùå Comparison experiment failed: No module named 'trainer'
