2025-06-02 09:29:17,292 - INFO - Log file: logs/2025-06-02_09-29-17/il_vs_rl_comparison.log
2025-06-02 09:29:17,294 - INFO - üöÄ Starting IL vs RL Comparison Experiment
2025-06-02 09:29:17,294 - INFO - Device: cuda
2025-06-02 09:29:17,294 - INFO - Results will be saved to: logs/2025-06-02_09-29-17/comparison_results
2025-06-02 09:29:17,294 - INFO - üìä Loading dataset...
2025-06-02 09:29:17,294 - INFO - Loading train data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-06-02 09:29:19,329 - INFO - [Train] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 09:29:19,330 - INFO - [Train] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 09:29:19,334 - INFO - [Train] Found 40 videos in metadata file
2025-06-02 09:29:19,334 - INFO - [Train] Loading data for 40 videos
2025-06-02 09:31:08,192 - INFO - Loading test data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-06-02 09:31:08,622 - INFO - [Test] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 09:31:08,622 - INFO - [Test] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 09:31:08,624 - INFO - [Test] Found 10 videos in metadata file
2025-06-02 09:31:08,624 - INFO - [Test] Loading data for 10 videos
2025-06-02 09:31:38,797 - INFO - Loaded 40 training videos and 10 test videos
2025-06-02 09:31:38,799 - INFO - üéì Training Imitation Learning Model...
2025-06-02 09:31:38,799 - INFO - Training supervised imitation learning model
2025-06-02 09:32:03,584 - INFO - Model parameters: 48,924,145 total, 48,924,145 trainable
2025-06-02 09:32:03,595 - INFO - Starting supervised training for autoregressive action prediction
2025-06-02 09:32:05,005 - INFO - Batch 0/4933 | Loss: 3.2355
2025-06-02 09:32:08,372 - INFO - Batch 100/4933 | Loss: 0.7929
2025-06-02 09:32:11,616 - INFO - Batch 200/4933 | Loss: 0.5577
2025-06-02 09:32:14,639 - INFO - Batch 300/4933 | Loss: 0.5746
2025-06-02 09:32:17,975 - INFO - Batch 400/4933 | Loss: 0.5535
2025-06-02 09:32:20,963 - INFO - Batch 500/4933 | Loss: 0.4127
2025-06-02 09:32:24,061 - INFO - Batch 600/4933 | Loss: 0.4550
2025-06-02 09:32:27,259 - INFO - Batch 700/4933 | Loss: 0.3613
2025-06-02 09:32:30,349 - INFO - Batch 800/4933 | Loss: 0.3586
2025-06-02 09:32:33,492 - INFO - Batch 900/4933 | Loss: 0.2976
2025-06-02 09:32:37,361 - INFO - Batch 1000/4933 | Loss: 0.3591
2025-06-02 09:32:41,833 - INFO - Batch 1100/4933 | Loss: 0.2449
2025-06-02 09:32:46,307 - INFO - Batch 1200/4933 | Loss: 0.3050
2025-06-02 09:32:50,853 - INFO - Batch 1300/4933 | Loss: 0.4432
2025-06-02 09:32:55,408 - INFO - Batch 1400/4933 | Loss: 0.2757
2025-06-02 09:32:59,800 - INFO - Batch 1500/4933 | Loss: 0.2364
2025-06-02 09:33:04,356 - INFO - Batch 1600/4933 | Loss: 0.3079
2025-06-02 09:33:08,957 - INFO - Batch 1700/4933 | Loss: 0.2242
2025-06-02 09:33:13,529 - INFO - Batch 1800/4933 | Loss: 0.2460
2025-06-02 09:33:17,948 - INFO - Batch 1900/4933 | Loss: 0.2141
2025-06-02 09:33:22,438 - INFO - Batch 2000/4933 | Loss: 0.2370
2025-06-02 09:33:26,930 - INFO - Batch 2100/4933 | Loss: 0.2483
2025-06-02 09:33:31,108 - INFO - Batch 2200/4933 | Loss: 0.3607
2025-06-02 09:33:34,182 - INFO - Batch 2300/4933 | Loss: 0.2269
2025-06-02 09:33:37,498 - INFO - Batch 2400/4933 | Loss: 0.2253
2025-06-02 09:33:42,651 - INFO - Batch 2500/4933 | Loss: 0.3478
2025-06-02 09:33:48,052 - INFO - Batch 2600/4933 | Loss: 0.2854
2025-06-02 09:33:53,368 - INFO - Batch 2700/4933 | Loss: 0.1941
2025-06-02 09:33:58,545 - INFO - Batch 2800/4933 | Loss: 0.2349
2025-06-02 09:34:03,608 - INFO - Batch 2900/4933 | Loss: 0.2096
2025-06-02 09:34:08,739 - INFO - Batch 3000/4933 | Loss: 0.2361
2025-06-02 09:34:12,441 - INFO - Batch 3100/4933 | Loss: 0.2773
2025-06-02 09:34:15,463 - INFO - Batch 3200/4933 | Loss: 0.2455
2025-06-02 09:34:18,389 - INFO - Batch 3300/4933 | Loss: 0.1991
2025-06-02 09:34:21,442 - INFO - Batch 3400/4933 | Loss: 0.2417
2025-06-02 09:34:24,407 - INFO - Batch 3500/4933 | Loss: 0.1969
2025-06-02 09:34:27,406 - INFO - Batch 3600/4933 | Loss: 0.2075
2025-06-02 09:34:30,477 - INFO - Batch 3700/4933 | Loss: 0.1992
2025-06-02 09:34:33,426 - INFO - Batch 3800/4933 | Loss: 0.2093
2025-06-02 09:34:36,333 - INFO - Batch 3900/4933 | Loss: 0.1769
2025-06-02 09:34:39,266 - INFO - Batch 4000/4933 | Loss: 0.2626
2025-06-02 09:34:42,256 - INFO - Batch 4100/4933 | Loss: 0.1607
2025-06-02 09:34:45,143 - INFO - Batch 4200/4933 | Loss: 0.2142
2025-06-02 09:34:48,129 - INFO - Batch 4300/4933 | Loss: 0.2074
2025-06-02 09:34:51,207 - INFO - Batch 4400/4933 | Loss: 0.1761
2025-06-02 09:34:54,242 - INFO - Batch 4500/4933 | Loss: 0.2078
2025-06-02 09:34:57,238 - INFO - Batch 4600/4933 | Loss: 0.2165
2025-06-02 09:35:00,262 - INFO - Batch 4700/4933 | Loss: 0.1681
2025-06-02 09:35:03,283 - INFO - Batch 4800/4933 | Loss: 0.1952
2025-06-02 09:35:06,364 - INFO - Batch 4900/4933 | Loss: 0.1751
2025-06-02 10:02:33,616 - INFO - New best model saved: logs/2025-06-02_09-29-17/checkpoints/supervised_best_epoch_1.pt
2025-06-02 10:02:33,616 - INFO - Epoch 1/5 | Train Loss: 0.3187 | Val Loss: 0.2015 | Val Action Acc: 0.0000
2025-06-02 10:02:35,093 - INFO - Batch 0/4933 | Loss: 0.2217
2025-06-02 10:02:38,336 - INFO - Batch 100/4933 | Loss: 0.1821
2025-06-02 10:02:41,559 - INFO - Batch 200/4933 | Loss: 0.2207
2025-06-02 10:02:46,154 - INFO - Batch 300/4933 | Loss: 0.1762
2025-06-02 10:02:50,720 - INFO - Batch 400/4933 | Loss: 0.1743
2025-06-02 10:02:55,358 - INFO - Batch 500/4933 | Loss: 0.1805
2025-06-02 10:02:59,892 - INFO - Batch 600/4933 | Loss: 0.2624
2025-06-02 10:03:04,615 - INFO - Batch 700/4933 | Loss: 0.1905
2025-06-02 10:03:09,295 - INFO - Batch 800/4933 | Loss: 0.1642
2025-06-02 10:03:14,108 - INFO - Batch 900/4933 | Loss: 0.1668
2025-06-02 10:03:18,733 - INFO - Batch 1000/4933 | Loss: 0.2009
2025-06-02 10:03:23,263 - INFO - Batch 1100/4933 | Loss: 0.1669
2025-06-02 10:03:27,994 - INFO - Batch 1200/4933 | Loss: 0.1505
2025-06-02 10:03:32,555 - INFO - Batch 1300/4933 | Loss: 0.1731
2025-06-02 10:03:36,574 - INFO - Batch 1400/4933 | Loss: 0.2352
2025-06-02 10:03:39,892 - INFO - Batch 1500/4933 | Loss: 0.1993
2025-06-02 10:03:44,442 - INFO - Batch 1600/4933 | Loss: 0.1779
2025-06-02 10:03:50,045 - INFO - Batch 1700/4933 | Loss: 0.1626
2025-06-02 10:03:55,651 - INFO - Batch 1800/4933 | Loss: 0.2206
2025-06-02 10:04:00,848 - INFO - Batch 1900/4933 | Loss: 0.1622
2025-06-02 10:04:06,151 - INFO - Batch 2000/4933 | Loss: 0.1843
2025-06-02 10:04:10,827 - INFO - Batch 2100/4933 | Loss: 0.2072
2025-06-02 10:04:14,163 - INFO - Batch 2200/4933 | Loss: 0.2170
2025-06-02 10:04:17,645 - INFO - Batch 2300/4933 | Loss: 0.1422
2025-06-02 10:04:20,838 - INFO - Batch 2400/4933 | Loss: 0.1367
2025-06-02 10:04:24,015 - INFO - Batch 2500/4933 | Loss: 0.1813
2025-06-02 10:04:27,215 - INFO - Batch 2600/4933 | Loss: 0.1522
2025-06-02 10:04:30,300 - INFO - Batch 2700/4933 | Loss: 0.1492
2025-06-02 10:04:33,394 - INFO - Batch 2800/4933 | Loss: 0.1423
2025-06-02 10:04:36,843 - INFO - Batch 2900/4933 | Loss: 0.1724
2025-06-02 10:04:40,311 - INFO - Batch 3000/4933 | Loss: 0.1756
2025-06-02 10:04:43,516 - INFO - Batch 3100/4933 | Loss: 0.1382
2025-06-02 10:04:46,675 - INFO - Batch 3200/4933 | Loss: 0.1377
2025-06-02 10:04:49,943 - INFO - Batch 3300/4933 | Loss: 0.1503
2025-06-02 10:04:53,195 - INFO - Batch 3400/4933 | Loss: 0.1749
2025-06-02 10:04:56,329 - INFO - Batch 3500/4933 | Loss: 0.1433
2025-06-02 10:04:59,584 - INFO - Batch 3600/4933 | Loss: 0.1954
2025-06-02 10:05:03,493 - INFO - Batch 3700/4933 | Loss: 0.1527
2025-06-02 10:05:06,843 - INFO - Batch 3800/4933 | Loss: 0.2549
2025-06-02 10:05:10,085 - INFO - Batch 3900/4933 | Loss: 0.2218
2025-06-02 10:05:13,321 - INFO - Batch 4000/4933 | Loss: 0.2166
2025-06-02 10:05:16,423 - INFO - Batch 4100/4933 | Loss: 0.1783
2025-06-02 10:05:19,502 - INFO - Batch 4200/4933 | Loss: 0.1589
2025-06-02 10:05:22,629 - INFO - Batch 4300/4933 | Loss: 0.1370
2025-06-02 10:05:25,710 - INFO - Batch 4400/4933 | Loss: 0.1612
2025-06-02 10:05:28,874 - INFO - Batch 4500/4933 | Loss: 0.1316
2025-06-02 10:05:32,054 - INFO - Batch 4600/4933 | Loss: 0.1523
2025-06-02 10:05:35,265 - INFO - Batch 4700/4933 | Loss: 0.1932
2025-06-02 10:05:38,761 - INFO - Batch 4800/4933 | Loss: 0.1803
2025-06-02 10:05:41,978 - INFO - Batch 4900/4933 | Loss: 0.1275
2025-06-02 10:33:24,748 - INFO - New best model saved: logs/2025-06-02_09-29-17/checkpoints/supervised_best_epoch_2.pt
2025-06-02 10:33:24,749 - INFO - Epoch 2/5 | Train Loss: 0.1846 | Val Loss: 0.1968 | Val Action Acc: 0.0000
2025-06-02 10:33:25,624 - INFO - Batch 0/4933 | Loss: 0.1907
2025-06-02 10:33:30,362 - INFO - Batch 100/4933 | Loss: 0.2328
2025-06-02 10:33:35,095 - INFO - Batch 200/4933 | Loss: 0.2090
2025-06-02 10:33:39,871 - INFO - Batch 300/4933 | Loss: 0.1775
2025-06-02 10:33:45,012 - INFO - Batch 400/4933 | Loss: 0.1324
2025-06-02 10:33:50,039 - INFO - Batch 500/4933 | Loss: 0.1314
2025-06-02 10:33:55,133 - INFO - Batch 600/4933 | Loss: 0.1342
2025-06-02 10:33:59,285 - INFO - Batch 700/4933 | Loss: 0.1670
2025-06-02 10:34:02,368 - INFO - Batch 800/4933 | Loss: 0.1605
2025-06-02 10:34:05,849 - INFO - Batch 900/4933 | Loss: 0.1546
2025-06-02 10:34:11,385 - INFO - Batch 1000/4933 | Loss: 0.2389
2025-06-02 10:34:17,161 - INFO - Batch 1100/4933 | Loss: 0.1591
2025-06-02 10:34:22,540 - INFO - Batch 1200/4933 | Loss: 0.1748
2025-06-02 10:34:27,641 - INFO - Batch 1300/4933 | Loss: 0.2028
2025-06-02 10:34:32,782 - INFO - Batch 1400/4933 | Loss: 0.1689
2025-06-02 10:34:36,865 - INFO - Batch 1500/4933 | Loss: 0.1954
2025-06-02 10:34:39,951 - INFO - Batch 1600/4933 | Loss: 0.1584
2025-06-02 10:34:43,091 - INFO - Batch 1700/4933 | Loss: 0.2089
2025-06-02 10:34:46,310 - INFO - Batch 1800/4933 | Loss: 0.1532
2025-06-02 10:34:49,747 - INFO - Batch 1900/4933 | Loss: 0.1639
2025-06-02 10:34:53,078 - INFO - Batch 2000/4933 | Loss: 0.1778
2025-06-02 10:34:56,427 - INFO - Batch 2100/4933 | Loss: 0.2139
2025-06-02 10:34:59,804 - INFO - Batch 2200/4933 | Loss: 0.1802
2025-06-02 10:35:03,068 - INFO - Batch 2300/4933 | Loss: 0.1776
2025-06-02 10:35:06,255 - INFO - Batch 2400/4933 | Loss: 0.1496
2025-06-02 10:35:09,503 - INFO - Batch 2500/4933 | Loss: 0.1587
2025-06-02 10:35:12,639 - INFO - Batch 2600/4933 | Loss: 0.1410
2025-06-02 10:35:15,697 - INFO - Batch 2700/4933 | Loss: 0.1245
2025-06-02 10:35:18,811 - INFO - Batch 2800/4933 | Loss: 0.1550
2025-06-02 10:35:22,056 - INFO - Batch 2900/4933 | Loss: 0.1457
2025-06-02 10:35:25,212 - INFO - Batch 3000/4933 | Loss: 0.1567
2025-06-02 10:35:28,264 - INFO - Batch 3100/4933 | Loss: 0.1246
2025-06-02 10:35:31,504 - INFO - Batch 3200/4933 | Loss: 0.1432
2025-06-02 10:35:34,837 - INFO - Batch 3300/4933 | Loss: 0.1477
2025-06-02 10:35:38,309 - INFO - Batch 3400/4933 | Loss: 0.1591
2025-06-02 10:35:41,559 - INFO - Batch 3500/4933 | Loss: 0.2172
2025-06-02 10:35:44,868 - INFO - Batch 3600/4933 | Loss: 0.1517
2025-06-02 10:35:47,971 - INFO - Batch 3700/4933 | Loss: 0.1387
2025-06-02 10:35:51,305 - INFO - Batch 3800/4933 | Loss: 0.1614
2025-06-02 10:35:54,895 - INFO - Batch 3900/4933 | Loss: 0.1321
2025-06-02 10:35:58,442 - INFO - Batch 4000/4933 | Loss: 0.2022
2025-06-02 10:36:01,613 - INFO - Batch 4100/4933 | Loss: 0.1275
2025-06-02 10:36:04,865 - INFO - Batch 4200/4933 | Loss: 0.1611
2025-06-02 10:36:08,150 - INFO - Batch 4300/4933 | Loss: 0.1262
2025-06-02 10:36:11,423 - INFO - Batch 4400/4933 | Loss: 0.2797
2025-06-02 10:36:14,570 - INFO - Batch 4500/4933 | Loss: 0.1816
2025-06-02 10:36:17,605 - INFO - Batch 4600/4933 | Loss: 0.1899
2025-06-02 10:36:20,836 - INFO - Batch 4700/4933 | Loss: 0.1799
2025-06-02 10:36:24,013 - INFO - Batch 4800/4933 | Loss: 0.2263
2025-06-02 10:36:27,275 - INFO - Batch 4900/4933 | Loss: 0.1714
2025-06-02 11:04:12,188 - INFO - New best model saved: logs/2025-06-02_09-29-17/checkpoints/supervised_best_epoch_3.pt
2025-06-02 11:04:12,189 - INFO - Epoch 3/5 | Train Loss: 0.1572 | Val Loss: 0.1939 | Val Action Acc: 0.0000
2025-06-02 11:04:12,942 - INFO - Batch 0/4933 | Loss: 0.2076
2025-06-02 11:04:17,681 - INFO - Batch 100/4933 | Loss: 0.1552
2025-06-02 11:04:22,258 - INFO - Batch 200/4933 | Loss: 0.1420
2025-06-02 11:04:27,007 - INFO - Batch 300/4933 | Loss: 0.1263
2025-06-02 11:04:31,547 - INFO - Batch 400/4933 | Loss: 0.1346
2025-06-02 11:04:36,456 - INFO - Batch 500/4933 | Loss: 0.1610
2025-06-02 11:04:41,139 - INFO - Batch 600/4933 | Loss: 0.1300
2025-06-02 11:04:45,127 - INFO - Batch 700/4933 | Loss: 0.1313
2025-06-02 11:04:48,510 - INFO - Batch 800/4933 | Loss: 0.1464
2025-06-02 11:04:52,916 - INFO - Batch 900/4933 | Loss: 0.1333
2025-06-02 11:04:58,624 - INFO - Batch 1000/4933 | Loss: 0.1772
2025-06-02 11:05:06,082 - INFO - Batch 1100/4933 | Loss: 0.1684
2025-06-02 11:05:11,256 - INFO - Batch 1200/4933 | Loss: 0.1803
2025-06-02 11:05:16,462 - INFO - Batch 1300/4933 | Loss: 0.1286
2025-06-02 11:05:20,331 - INFO - Batch 1400/4933 | Loss: 0.1172
2025-06-02 11:05:23,621 - INFO - Batch 1500/4933 | Loss: 0.1413
2025-06-02 11:05:26,997 - INFO - Batch 1600/4933 | Loss: 0.1202
2025-06-02 11:05:30,148 - INFO - Batch 1700/4933 | Loss: 0.1080
2025-06-02 11:05:33,331 - INFO - Batch 1800/4933 | Loss: 0.1574
2025-06-02 11:05:36,467 - INFO - Batch 1900/4933 | Loss: 0.1338
2025-06-02 11:05:39,819 - INFO - Batch 2000/4933 | Loss: 0.1151
2025-06-02 11:05:43,192 - INFO - Batch 2100/4933 | Loss: 0.1372
2025-06-02 11:05:46,615 - INFO - Batch 2200/4933 | Loss: 0.1453
2025-06-02 11:05:49,866 - INFO - Batch 2300/4933 | Loss: 0.1221
2025-06-02 11:05:53,154 - INFO - Batch 2400/4933 | Loss: 0.1180
2025-06-02 11:05:56,712 - INFO - Batch 2500/4933 | Loss: 0.1373
2025-06-02 11:06:00,084 - INFO - Batch 2600/4933 | Loss: 0.1606
2025-06-02 11:06:03,351 - INFO - Batch 2700/4933 | Loss: 0.1704
2025-06-02 11:06:06,623 - INFO - Batch 2800/4933 | Loss: 0.1180
2025-06-02 11:06:10,124 - INFO - Batch 2900/4933 | Loss: 0.1151
2025-06-02 11:06:13,517 - INFO - Batch 3000/4933 | Loss: 0.1100
2025-06-02 11:06:16,807 - INFO - Batch 3100/4933 | Loss: 0.1426
2025-06-02 11:06:20,358 - INFO - Batch 3200/4933 | Loss: 0.1321
2025-06-02 11:06:23,766 - INFO - Batch 3300/4933 | Loss: 0.1365
2025-06-02 11:06:26,959 - INFO - Batch 3400/4933 | Loss: 0.1155
2025-06-02 11:06:30,360 - INFO - Batch 3500/4933 | Loss: 0.1539
2025-06-02 11:06:33,789 - INFO - Batch 3600/4933 | Loss: 0.1219
2025-06-02 11:06:37,407 - INFO - Batch 3700/4933 | Loss: 0.1078
2025-06-02 11:06:40,697 - INFO - Batch 3800/4933 | Loss: 0.1450
2025-06-02 11:06:43,984 - INFO - Batch 3900/4933 | Loss: 0.1009
2025-06-02 11:06:47,431 - INFO - Batch 4000/4933 | Loss: 0.1490
2025-06-02 11:06:50,788 - INFO - Batch 4100/4933 | Loss: 0.1162
2025-06-02 11:06:54,113 - INFO - Batch 4200/4933 | Loss: 0.1558
2025-06-02 11:06:57,381 - INFO - Batch 4300/4933 | Loss: 0.1758
2025-06-02 11:07:00,678 - INFO - Batch 4400/4933 | Loss: 0.1270
2025-06-02 11:07:04,220 - INFO - Batch 4500/4933 | Loss: 0.1612
2025-06-02 11:07:07,944 - INFO - Batch 4600/4933 | Loss: 0.1534
2025-06-02 11:07:11,456 - INFO - Batch 4700/4933 | Loss: 0.1396
2025-06-02 11:07:14,998 - INFO - Batch 4800/4933 | Loss: 0.1697
2025-06-02 11:07:18,399 - INFO - Batch 4900/4933 | Loss: 0.1720
2025-06-02 11:35:26,942 - INFO - Epoch 4/5 | Train Loss: 0.1434 | Val Loss: 0.1978 | Val Action Acc: 0.0000
2025-06-02 11:35:27,551 - INFO - Batch 0/4933 | Loss: 0.1064
2025-06-02 11:35:30,923 - INFO - Batch 100/4933 | Loss: 0.1367
2025-06-02 11:35:34,481 - INFO - Batch 200/4933 | Loss: 0.1274
2025-06-02 11:35:37,838 - INFO - Batch 300/4933 | Loss: 0.1451
2025-06-02 11:35:41,025 - INFO - Batch 400/4933 | Loss: 0.1720
2025-06-02 11:35:44,239 - INFO - Batch 500/4933 | Loss: 0.1176
2025-06-02 11:35:47,408 - INFO - Batch 600/4933 | Loss: 0.1592
2025-06-02 11:35:50,626 - INFO - Batch 700/4933 | Loss: 0.1462
2025-06-02 11:35:53,867 - INFO - Batch 800/4933 | Loss: 0.1153
2025-06-02 11:35:57,310 - INFO - Batch 900/4933 | Loss: 0.1145
2025-06-02 11:36:00,702 - INFO - Batch 1000/4933 | Loss: 0.1720
2025-06-02 11:36:03,907 - INFO - Batch 1100/4933 | Loss: 0.1260
2025-06-02 11:36:07,076 - INFO - Batch 1200/4933 | Loss: 0.1176
2025-06-02 11:36:10,363 - INFO - Batch 1300/4933 | Loss: 0.1279
2025-06-02 11:36:13,653 - INFO - Batch 1400/4933 | Loss: 0.1177
2025-06-02 11:36:16,840 - INFO - Batch 1500/4933 | Loss: 0.1433
2025-06-02 11:36:20,091 - INFO - Batch 1600/4933 | Loss: 0.1284
2025-06-02 11:36:23,355 - INFO - Batch 1700/4933 | Loss: 0.1163
2025-06-02 11:36:26,480 - INFO - Batch 1800/4933 | Loss: 0.1147
2025-06-02 11:36:29,832 - INFO - Batch 1900/4933 | Loss: 0.1577
2025-06-02 11:36:32,845 - INFO - Batch 2000/4933 | Loss: 0.1559
2025-06-02 11:36:36,068 - INFO - Batch 2100/4933 | Loss: 0.1133
2025-06-02 11:36:39,214 - INFO - Batch 2200/4933 | Loss: 0.1429
2025-06-02 11:36:42,274 - INFO - Batch 2300/4933 | Loss: 0.1183
2025-06-02 11:36:45,280 - INFO - Batch 2400/4933 | Loss: 0.1336
2025-06-02 11:36:48,434 - INFO - Batch 2500/4933 | Loss: 0.1230
2025-06-02 11:36:51,519 - INFO - Batch 2600/4933 | Loss: 0.1431
2025-06-02 11:36:54,735 - INFO - Batch 2700/4933 | Loss: 0.1173
2025-06-02 11:36:58,006 - INFO - Batch 2800/4933 | Loss: 0.1281
2025-06-02 11:37:01,033 - INFO - Batch 2900/4933 | Loss: 0.1470
2025-06-02 11:37:04,127 - INFO - Batch 3000/4933 | Loss: 0.1636
2025-06-02 11:37:07,153 - INFO - Batch 3100/4933 | Loss: 0.1238
2025-06-02 11:37:10,159 - INFO - Batch 3200/4933 | Loss: 0.1438
2025-06-02 11:37:13,269 - INFO - Batch 3300/4933 | Loss: 0.1588
2025-06-02 11:37:16,603 - INFO - Batch 3400/4933 | Loss: 0.1725
2025-06-02 11:37:19,848 - INFO - Batch 3500/4933 | Loss: 0.1903
2025-06-02 11:37:23,059 - INFO - Batch 3600/4933 | Loss: 0.1175
2025-06-02 11:37:26,131 - INFO - Batch 3700/4933 | Loss: 0.1356
2025-06-02 11:37:29,238 - INFO - Batch 3800/4933 | Loss: 0.1270
2025-06-02 11:37:32,318 - INFO - Batch 3900/4933 | Loss: 0.1322
2025-06-02 11:37:35,483 - INFO - Batch 4000/4933 | Loss: 0.1148
2025-06-02 11:37:38,756 - INFO - Batch 4100/4933 | Loss: 0.1286
2025-06-02 11:37:41,950 - INFO - Batch 4200/4933 | Loss: 0.0968
2025-06-02 11:37:45,025 - INFO - Batch 4300/4933 | Loss: 0.1473
2025-06-02 11:37:48,508 - INFO - Batch 4400/4933 | Loss: 0.0964
2025-06-02 11:37:53,015 - INFO - Batch 4500/4933 | Loss: 0.1303
2025-06-02 11:37:57,532 - INFO - Batch 4600/4933 | Loss: 0.1347
2025-06-02 11:38:02,536 - INFO - Batch 4700/4933 | Loss: 0.1074
2025-06-02 11:38:07,263 - INFO - Batch 4800/4933 | Loss: 0.1349
2025-06-02 11:38:11,923 - INFO - Batch 4900/4933 | Loss: 0.1273
2025-06-02 12:07:07,262 - INFO - Epoch 5/5 | Train Loss: 0.1353 | Val Loss: 0.1964 | Val Action Acc: 0.0000
2025-06-02 12:07:09,821 - INFO - Training plots saved to logs/2025-06-02_09-29-17
2025-06-02 12:07:10,503 - INFO - Training completed. Best model: logs/2025-06-02_09-29-17/checkpoints/supervised_best_epoch_3.pt
2025-06-02 12:07:10,503 - INFO - Final model: logs/2025-06-02_09-29-17/checkpoints/final_model.pt
2025-06-02 12:07:10,504 - INFO - ‚úÖ IL training completed. Model saved: logs/2025-06-02_09-29-17/checkpoints/supervised_best_epoch_3.pt
2025-06-02 12:07:13,728 - INFO - ü§ñ Training RL Models...
2025-06-02 12:07:13,960 - ERROR - ‚ùå Comparison experiment failed: module 'cv2.dnn' has no attribute 'DictValue'
