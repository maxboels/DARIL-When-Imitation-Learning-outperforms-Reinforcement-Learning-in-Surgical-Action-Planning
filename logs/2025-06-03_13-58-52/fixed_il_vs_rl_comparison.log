2025-06-03 13:58:52,039 - INFO - Log file: logs/2025-06-03_13-58-52/fixed_il_vs_rl_comparison.log
2025-06-03 13:58:52,041 - INFO - ðŸš€ Starting FIXED IL vs RL Comparison Experiment
2025-06-03 13:58:52,041 - INFO - Device: cuda
2025-06-03 13:58:52,041 - INFO - Action Space: Continuous Box(0,1,(100,)) with binary thresholding
2025-06-03 13:58:52,041 - INFO - Results will be saved to: logs/2025-06-03_13-58-52/comparison_results
2025-06-03 13:58:52,041 - INFO - ðŸ“Š Loading dataset...
2025-06-03 13:58:52,041 - INFO - Loading train data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-06-03 13:58:53,532 - INFO - [Train] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-03 13:58:53,532 - INFO - [Train] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-03 13:58:53,537 - INFO - [Train] Found 40 videos in metadata file
2025-06-03 13:58:53,537 - INFO - [Train] Loading data for 40 videos
2025-06-03 14:00:26,814 - INFO - Loading test data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-06-03 14:00:27,288 - INFO - [Test] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-03 14:00:27,288 - INFO - [Test] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-03 14:00:27,289 - INFO - [Test] Found 10 videos in metadata file
2025-06-03 14:00:27,289 - INFO - [Test] Loading data for 10 videos
2025-06-03 14:00:54,969 - INFO - Loaded 40 training videos and 10 test videos
2025-06-03 14:00:54,970 - INFO - ðŸŽ“ Training Imitation Learning Model...
2025-06-03 14:00:54,970 - INFO - Training supervised imitation learning model
2025-06-03 14:01:19,234 - INFO - Model parameters: 48,924,145 total, 48,924,145 trainable
2025-06-03 14:01:19,244 - INFO - Starting supervised training for autoregressive action prediction
2025-06-03 14:01:20,953 - INFO - Batch 0/4933 | Loss: 3.2355
2025-06-03 14:01:24,789 - INFO - Batch 100/4933 | Loss: 0.8056
2025-06-03 14:01:28,756 - INFO - Batch 200/4933 | Loss: 0.5615
2025-06-03 14:01:32,344 - INFO - Batch 300/4933 | Loss: 0.5676
2025-06-03 14:01:36,156 - INFO - Batch 400/4933 | Loss: 0.5626
2025-06-03 14:01:39,763 - INFO - Batch 500/4933 | Loss: 0.4033
2025-06-03 14:01:43,491 - INFO - Batch 600/4933 | Loss: 0.4680
2025-06-03 14:01:47,335 - INFO - Batch 700/4933 | Loss: 0.3856
2025-06-03 14:01:50,923 - INFO - Batch 800/4933 | Loss: 0.3523
2025-06-03 14:01:54,468 - INFO - Batch 900/4933 | Loss: 0.2948
2025-06-03 14:01:58,221 - INFO - Batch 1000/4933 | Loss: 0.3657
2025-06-03 14:02:01,848 - INFO - Batch 1100/4933 | Loss: 0.2373
2025-06-03 14:02:05,496 - INFO - Batch 1200/4933 | Loss: 0.3108
2025-06-03 14:02:09,267 - INFO - Batch 1300/4933 | Loss: 0.4467
2025-06-03 14:02:12,960 - INFO - Batch 1400/4933 | Loss: 0.2635
2025-06-03 14:02:16,543 - INFO - Batch 1500/4933 | Loss: 0.2302
2025-06-03 14:02:20,192 - INFO - Batch 1600/4933 | Loss: 0.3083
2025-06-03 14:02:23,782 - INFO - Batch 1700/4933 | Loss: 0.2233
2025-06-03 14:02:27,451 - INFO - Batch 1800/4933 | Loss: 0.2382
2025-06-03 14:02:31,222 - INFO - Batch 1900/4933 | Loss: 0.2206
2025-06-03 14:02:35,190 - INFO - Batch 2000/4933 | Loss: 0.2320
2025-06-03 14:02:38,949 - INFO - Batch 2100/4933 | Loss: 0.2379
2025-06-03 14:02:42,658 - INFO - Batch 2200/4933 | Loss: 0.3813
2025-06-03 14:02:46,144 - INFO - Batch 2300/4933 | Loss: 0.2265
2025-06-03 14:02:49,813 - INFO - Batch 2400/4933 | Loss: 0.2232
2025-06-03 14:02:53,453 - INFO - Batch 2500/4933 | Loss: 0.3504
2025-06-03 14:02:57,060 - INFO - Batch 2600/4933 | Loss: 0.2675
2025-06-03 14:03:00,716 - INFO - Batch 2700/4933 | Loss: 0.1881
2025-06-03 14:03:04,521 - INFO - Batch 2800/4933 | Loss: 0.2263
2025-06-03 14:03:08,279 - INFO - Batch 2900/4933 | Loss: 0.2039
2025-06-03 14:03:12,236 - INFO - Batch 3000/4933 | Loss: 0.2400
2025-06-03 14:03:16,020 - INFO - Batch 3100/4933 | Loss: 0.2745
2025-06-03 14:03:19,732 - INFO - Batch 3200/4933 | Loss: 0.2331
2025-06-03 14:03:23,454 - INFO - Batch 3300/4933 | Loss: 0.1943
2025-06-03 14:03:26,819 - INFO - Batch 3400/4933 | Loss: 0.2272
2025-06-03 14:03:30,341 - INFO - Batch 3500/4933 | Loss: 0.2051
2025-06-03 14:03:34,016 - INFO - Batch 3600/4933 | Loss: 0.2272
2025-06-03 14:03:37,704 - INFO - Batch 3700/4933 | Loss: 0.1986
2025-06-03 14:03:41,246 - INFO - Batch 3800/4933 | Loss: 0.2037
2025-06-03 14:03:44,674 - INFO - Batch 3900/4933 | Loss: 0.1740
2025-06-03 14:03:48,145 - INFO - Batch 4000/4933 | Loss: 0.2595
2025-06-03 14:03:51,731 - INFO - Batch 4100/4933 | Loss: 0.1603
2025-06-03 14:03:55,327 - INFO - Batch 4200/4933 | Loss: 0.2096
2025-06-03 14:03:58,953 - INFO - Batch 4300/4933 | Loss: 0.2179
2025-06-03 14:04:02,835 - INFO - Batch 4400/4933 | Loss: 0.1767
2025-06-03 14:04:06,567 - INFO - Batch 4500/4933 | Loss: 0.2023
2025-06-03 14:04:09,999 - INFO - Batch 4600/4933 | Loss: 0.2213
2025-06-03 14:04:13,521 - INFO - Batch 4700/4933 | Loss: 0.1685
2025-06-03 14:04:17,188 - INFO - Batch 4800/4933 | Loss: 0.1925
2025-06-03 14:04:20,676 - INFO - Batch 4900/4933 | Loss: 0.1734
2025-06-03 14:32:36,010 - INFO - New best model saved: logs/2025-06-03_13-58-52/checkpoints/supervised_best_epoch_1.pt
2025-06-03 14:32:36,010 - INFO - Epoch 1/3 | Train Loss: 0.3115 | Val Loss: 0.2007 | Val Action Acc: 0.0000
2025-06-03 14:32:36,553 - INFO - Batch 0/4933 | Loss: 0.2141
2025-06-03 14:32:40,043 - INFO - Batch 100/4933 | Loss: 0.1721
2025-06-03 14:32:43,464 - INFO - Batch 200/4933 | Loss: 0.2211
2025-06-03 14:32:47,065 - INFO - Batch 300/4933 | Loss: 0.1474
2025-06-03 14:32:50,638 - INFO - Batch 400/4933 | Loss: 0.1741
2025-06-03 14:32:54,149 - INFO - Batch 500/4933 | Loss: 0.1718
2025-06-03 14:32:57,512 - INFO - Batch 600/4933 | Loss: 0.2529
2025-06-03 14:33:00,974 - INFO - Batch 700/4933 | Loss: 0.1841
2025-06-03 14:33:04,521 - INFO - Batch 800/4933 | Loss: 0.1624
2025-06-03 14:33:08,106 - INFO - Batch 900/4933 | Loss: 0.1568
2025-06-03 14:33:11,593 - INFO - Batch 1000/4933 | Loss: 0.1804
2025-06-03 14:33:15,119 - INFO - Batch 1100/4933 | Loss: 0.1618
2025-06-03 14:33:18,750 - INFO - Batch 1200/4933 | Loss: 0.1494
2025-06-03 14:33:22,242 - INFO - Batch 1300/4933 | Loss: 0.1639
2025-06-03 14:33:25,699 - INFO - Batch 1400/4933 | Loss: 0.2192
2025-06-03 14:33:29,126 - INFO - Batch 1500/4933 | Loss: 0.1906
2025-06-03 14:33:32,739 - INFO - Batch 1600/4933 | Loss: 0.1714
2025-06-03 14:33:36,396 - INFO - Batch 1700/4933 | Loss: 0.1637
2025-06-03 14:33:39,744 - INFO - Batch 1800/4933 | Loss: 0.2041
2025-06-03 14:33:43,363 - INFO - Batch 1900/4933 | Loss: 0.1621
2025-06-03 14:33:46,838 - INFO - Batch 2000/4933 | Loss: 0.1592
2025-06-03 14:33:50,314 - INFO - Batch 2100/4933 | Loss: 0.1893
2025-06-03 14:33:53,815 - INFO - Batch 2200/4933 | Loss: 0.1809
2025-06-03 14:33:57,449 - INFO - Batch 2300/4933 | Loss: 0.1389
2025-06-03 14:34:00,934 - INFO - Batch 2400/4933 | Loss: 0.1368
2025-06-03 14:34:04,417 - INFO - Batch 2500/4933 | Loss: 0.1713
2025-06-03 14:34:07,812 - INFO - Batch 2600/4933 | Loss: 0.1481
2025-06-03 14:34:11,295 - INFO - Batch 2700/4933 | Loss: 0.1421
2025-06-03 14:34:14,703 - INFO - Batch 2800/4933 | Loss: 0.1366
2025-06-03 14:34:18,127 - INFO - Batch 2900/4933 | Loss: 0.1652
2025-06-03 14:34:21,549 - INFO - Batch 3000/4933 | Loss: 0.1600
2025-06-03 14:34:25,071 - INFO - Batch 3100/4933 | Loss: 0.1353
2025-06-03 14:34:28,427 - INFO - Batch 3200/4933 | Loss: 0.1348
2025-06-03 14:34:32,001 - INFO - Batch 3300/4933 | Loss: 0.1443
2025-06-03 14:34:35,456 - INFO - Batch 3400/4933 | Loss: 0.1635
2025-06-03 14:34:39,064 - INFO - Batch 3500/4933 | Loss: 0.1319
2025-06-03 14:34:42,774 - INFO - Batch 3600/4933 | Loss: 0.1908
2025-06-03 14:34:46,410 - INFO - Batch 3700/4933 | Loss: 0.1491
2025-06-03 14:34:49,682 - INFO - Batch 3800/4933 | Loss: 0.2398
2025-06-03 14:34:53,196 - INFO - Batch 3900/4933 | Loss: 0.2269
2025-06-03 14:34:56,803 - INFO - Batch 4000/4933 | Loss: 0.2202
2025-06-03 14:35:00,344 - INFO - Batch 4100/4933 | Loss: 0.1964
2025-06-03 14:35:03,771 - INFO - Batch 4200/4933 | Loss: 0.1598
2025-06-03 14:35:07,118 - INFO - Batch 4300/4933 | Loss: 0.1316
2025-06-03 14:35:10,716 - INFO - Batch 4400/4933 | Loss: 0.1600
2025-06-03 14:35:14,198 - INFO - Batch 4500/4933 | Loss: 0.1306
2025-06-03 14:35:17,799 - INFO - Batch 4600/4933 | Loss: 0.1473
2025-06-03 14:35:21,431 - INFO - Batch 4700/4933 | Loss: 0.1900
2025-06-03 14:35:25,011 - INFO - Batch 4800/4933 | Loss: 0.1878
2025-06-03 14:35:28,483 - INFO - Batch 4900/4933 | Loss: 0.1239
2025-06-03 15:03:08,863 - INFO - New best model saved: logs/2025-06-03_13-58-52/checkpoints/supervised_best_epoch_2.pt
2025-06-03 15:03:08,863 - INFO - Epoch 2/3 | Train Loss: 0.1804 | Val Loss: 0.1955 | Val Action Acc: 0.0000
2025-06-03 15:03:09,530 - INFO - Batch 0/4933 | Loss: 0.1891
2025-06-03 15:03:13,126 - INFO - Batch 100/4933 | Loss: 0.1725
2025-06-03 15:03:16,674 - INFO - Batch 200/4933 | Loss: 0.2125
2025-06-03 15:03:20,505 - INFO - Batch 300/4933 | Loss: 0.1665
2025-06-03 15:03:24,083 - INFO - Batch 400/4933 | Loss: 0.1354
2025-06-03 15:03:27,765 - INFO - Batch 500/4933 | Loss: 0.1324
2025-06-03 15:03:31,412 - INFO - Batch 600/4933 | Loss: 0.1285
2025-06-03 15:03:35,059 - INFO - Batch 700/4933 | Loss: 0.1625
2025-06-03 15:03:38,644 - INFO - Batch 800/4933 | Loss: 0.1576
2025-06-03 15:03:42,197 - INFO - Batch 900/4933 | Loss: 0.1592
2025-06-03 15:03:45,930 - INFO - Batch 1000/4933 | Loss: 0.2618
2025-06-03 15:03:49,477 - INFO - Batch 1100/4933 | Loss: 0.1465
2025-06-03 15:03:52,993 - INFO - Batch 1200/4933 | Loss: 0.1805
2025-06-03 15:03:56,664 - INFO - Batch 1300/4933 | Loss: 0.1837
2025-06-03 15:04:00,240 - INFO - Batch 1400/4933 | Loss: 0.1725
2025-06-03 15:04:03,853 - INFO - Batch 1500/4933 | Loss: 0.1905
2025-06-03 15:04:07,290 - INFO - Batch 1600/4933 | Loss: 0.1515
2025-06-03 15:04:10,930 - INFO - Batch 1700/4933 | Loss: 0.2070
2025-06-03 15:04:14,617 - INFO - Batch 1800/4933 | Loss: 0.1545
2025-06-03 15:04:18,354 - INFO - Batch 1900/4933 | Loss: 0.1565
2025-06-03 15:04:21,808 - INFO - Batch 2000/4933 | Loss: 0.1737
2025-06-03 15:04:25,374 - INFO - Batch 2100/4933 | Loss: 0.2178
2025-06-03 15:04:28,890 - INFO - Batch 2200/4933 | Loss: 0.1743
2025-06-03 15:04:32,661 - INFO - Batch 2300/4933 | Loss: 0.1761
2025-06-03 15:04:36,421 - INFO - Batch 2400/4933 | Loss: 0.1441
2025-06-03 15:04:39,946 - INFO - Batch 2500/4933 | Loss: 0.1526
2025-06-03 15:04:43,656 - INFO - Batch 2600/4933 | Loss: 0.1339
2025-06-03 15:04:47,347 - INFO - Batch 2700/4933 | Loss: 0.1209
2025-06-03 15:04:50,975 - INFO - Batch 2800/4933 | Loss: 0.1519
2025-06-03 15:04:54,492 - INFO - Batch 2900/4933 | Loss: 0.1459
2025-06-03 15:04:58,124 - INFO - Batch 3000/4933 | Loss: 0.1529
2025-06-03 15:05:03,759 - INFO - Batch 3100/4933 | Loss: 0.1232
2025-06-03 15:05:07,153 - INFO - Batch 3200/4933 | Loss: 0.1329
2025-06-03 15:05:10,541 - INFO - Batch 3300/4933 | Loss: 0.1483
2025-06-03 15:05:13,977 - INFO - Batch 3400/4933 | Loss: 0.1544
2025-06-03 15:05:17,437 - INFO - Batch 3500/4933 | Loss: 0.1638
2025-06-03 15:05:20,991 - INFO - Batch 3600/4933 | Loss: 0.1534
2025-06-03 15:05:24,560 - INFO - Batch 3700/4933 | Loss: 0.1355
2025-06-03 15:05:28,252 - INFO - Batch 3800/4933 | Loss: 0.1796
2025-06-03 15:05:31,583 - INFO - Batch 3900/4933 | Loss: 0.1208
2025-06-03 15:05:34,902 - INFO - Batch 4000/4933 | Loss: 0.2047
2025-06-03 15:05:38,528 - INFO - Batch 4100/4933 | Loss: 0.1307
2025-06-03 15:05:42,141 - INFO - Batch 4200/4933 | Loss: 0.1616
2025-06-03 15:05:45,574 - INFO - Batch 4300/4933 | Loss: 0.1238
2025-06-03 15:05:49,016 - INFO - Batch 4400/4933 | Loss: 0.2601
2025-06-03 15:05:52,584 - INFO - Batch 4500/4933 | Loss: 0.1697
2025-06-03 15:05:56,056 - INFO - Batch 4600/4933 | Loss: 0.1849
2025-06-03 15:05:59,482 - INFO - Batch 4700/4933 | Loss: 0.1821
2025-06-03 15:06:02,924 - INFO - Batch 4800/4933 | Loss: 0.2206
2025-06-03 15:06:06,345 - INFO - Batch 4900/4933 | Loss: 0.1673
