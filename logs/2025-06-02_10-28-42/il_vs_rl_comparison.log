2025-06-02 10:28:42,811 - INFO - Log file: logs/2025-06-02_10-28-42/il_vs_rl_comparison.log
2025-06-02 10:28:42,812 - INFO - 🚀 Starting IL vs RL Comparison Experiment
2025-06-02 10:28:42,812 - INFO - Device: cuda
2025-06-02 10:28:42,813 - INFO - Results will be saved to: logs/2025-06-02_10-28-42/comparison_results
2025-06-02 10:28:42,813 - INFO - 📊 Loading dataset...
2025-06-02 10:28:42,813 - INFO - Loading train data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-06-02 10:28:43,717 - INFO - [Train] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 10:28:43,717 - INFO - [Train] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_train_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 10:28:43,722 - INFO - [Train] Found 40 videos in metadata file
2025-06-02 10:28:43,722 - INFO - [Train] Loading data for 40 videos
2025-06-02 10:29:52,137 - INFO - Loading test data from /nfs/home/mboels/datasets/CholecT50 with fold 0
2025-06-02 10:29:52,397 - INFO - [Test] Start processing metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 10:29:52,398 - INFO - [Test] Rewards extraction skipped, using metadata file /nfs/home/mboels/datasets/CholecT50/embeddings_test_set/fold0/embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv
2025-06-02 10:29:52,399 - INFO - [Test] Found 10 videos in metadata file
2025-06-02 10:29:52,399 - INFO - [Test] Loading data for 10 videos
2025-06-02 10:30:13,169 - INFO - Loaded 40 training videos and 10 test videos
2025-06-02 10:30:13,170 - INFO - 🎓 Training Imitation Learning Model...
2025-06-02 10:30:13,170 - INFO - Training supervised imitation learning model
2025-06-02 10:30:33,502 - INFO - Model parameters: 48,924,145 total, 48,924,145 trainable
2025-06-02 10:30:33,512 - INFO - Starting supervised training for autoregressive action prediction
2025-06-02 10:30:34,673 - INFO - Batch 0/4933 | Loss: 3.2355
2025-06-02 10:30:38,157 - INFO - Batch 100/4933 | Loss: 0.7928
2025-06-02 10:30:41,488 - INFO - Batch 200/4933 | Loss: 0.5577
2025-06-02 10:30:44,645 - INFO - Batch 300/4933 | Loss: 0.5748
2025-06-02 10:30:47,632 - INFO - Batch 400/4933 | Loss: 0.5538
2025-06-02 10:30:50,481 - INFO - Batch 500/4933 | Loss: 0.4128
2025-06-02 10:30:54,095 - INFO - Batch 600/4933 | Loss: 0.4547
2025-06-02 10:30:57,679 - INFO - Batch 700/4933 | Loss: 0.3619
2025-06-02 10:31:00,554 - INFO - Batch 800/4933 | Loss: 0.3574
2025-06-02 10:31:03,455 - INFO - Batch 900/4933 | Loss: 0.2983
2025-06-02 10:31:06,719 - INFO - Batch 1000/4933 | Loss: 0.3578
2025-06-02 10:31:09,344 - INFO - Batch 1100/4933 | Loss: 0.2454
2025-06-02 10:31:11,884 - INFO - Batch 1200/4933 | Loss: 0.3042
2025-06-02 10:31:13,958 - INFO - Batch 1300/4933 | Loss: 0.4389
2025-06-02 10:31:16,145 - INFO - Batch 1400/4933 | Loss: 0.2760
2025-06-02 10:31:18,438 - INFO - Batch 1500/4933 | Loss: 0.2369
2025-06-02 10:31:21,136 - INFO - Batch 1600/4933 | Loss: 0.3061
2025-06-02 10:31:23,527 - INFO - Batch 1700/4933 | Loss: 0.2233
2025-06-02 10:31:26,125 - INFO - Batch 1800/4933 | Loss: 0.2416
2025-06-02 10:31:28,536 - INFO - Batch 1900/4933 | Loss: 0.2148
2025-06-02 10:31:31,173 - INFO - Batch 2000/4933 | Loss: 0.2377
2025-06-02 10:31:33,731 - INFO - Batch 2100/4933 | Loss: 0.2485
2025-06-02 10:31:35,839 - INFO - Batch 2200/4933 | Loss: 0.3617
2025-06-02 10:31:38,019 - INFO - Batch 2300/4933 | Loss: 0.2283
2025-06-02 10:31:40,639 - INFO - Batch 2400/4933 | Loss: 0.2250
2025-06-02 10:31:43,109 - INFO - Batch 2500/4933 | Loss: 0.3494
2025-06-02 10:31:45,747 - INFO - Batch 2600/4933 | Loss: 0.2868
2025-06-02 10:31:48,122 - INFO - Batch 2700/4933 | Loss: 0.1919
2025-06-02 10:31:50,477 - INFO - Batch 2800/4933 | Loss: 0.2340
2025-06-02 10:31:53,166 - INFO - Batch 2900/4933 | Loss: 0.2096
2025-06-02 10:31:55,615 - INFO - Batch 3000/4933 | Loss: 0.2365
2025-06-02 10:31:58,000 - INFO - Batch 3100/4933 | Loss: 0.2779
2025-06-02 10:32:00,425 - INFO - Batch 3200/4933 | Loss: 0.2458
2025-06-02 10:32:02,944 - INFO - Batch 3300/4933 | Loss: 0.1985
2025-06-02 10:32:05,370 - INFO - Batch 3400/4933 | Loss: 0.2397
2025-06-02 10:32:08,188 - INFO - Batch 3500/4933 | Loss: 0.1970
2025-06-02 10:32:11,084 - INFO - Batch 3600/4933 | Loss: 0.2044
2025-06-02 10:32:13,670 - INFO - Batch 3700/4933 | Loss: 0.1989
2025-06-02 10:32:16,706 - INFO - Batch 3800/4933 | Loss: 0.2122
2025-06-02 10:32:20,021 - INFO - Batch 3900/4933 | Loss: 0.1769
2025-06-02 10:32:23,391 - INFO - Batch 4000/4933 | Loss: 0.2628
2025-06-02 10:32:26,721 - INFO - Batch 4100/4933 | Loss: 0.1605
2025-06-02 10:32:29,991 - INFO - Batch 4200/4933 | Loss: 0.2145
2025-06-02 10:32:33,023 - INFO - Batch 4300/4933 | Loss: 0.2058
2025-06-02 10:32:36,398 - INFO - Batch 4400/4933 | Loss: 0.1758
2025-06-02 10:32:39,853 - INFO - Batch 4500/4933 | Loss: 0.2076
2025-06-02 10:32:43,308 - INFO - Batch 4600/4933 | Loss: 0.2167
2025-06-02 10:32:46,923 - INFO - Batch 4700/4933 | Loss: 0.1677
2025-06-02 10:32:50,212 - INFO - Batch 4800/4933 | Loss: 0.1951
2025-06-02 10:32:53,736 - INFO - Batch 4900/4933 | Loss: 0.1741
2025-06-02 10:47:59,316 - INFO - New best model saved: logs/2025-06-02_10-28-42/checkpoints/supervised_best_epoch_1.pt
2025-06-02 10:47:59,316 - INFO - Epoch 1/5 | Train Loss: 0.3187 | Val Loss: 0.2016 | Val Action Acc: 0.0000
2025-06-02 10:47:59,706 - INFO - Batch 0/4933 | Loss: 0.2216
2025-06-02 10:48:03,764 - INFO - Batch 100/4933 | Loss: 0.1804
2025-06-02 10:48:07,602 - INFO - Batch 200/4933 | Loss: 0.2244
2025-06-02 10:48:11,276 - INFO - Batch 300/4933 | Loss: 0.1782
2025-06-02 10:48:15,466 - INFO - Batch 400/4933 | Loss: 0.1743
2025-06-02 10:48:19,221 - INFO - Batch 500/4933 | Loss: 0.1827
2025-06-02 10:48:22,957 - INFO - Batch 600/4933 | Loss: 0.2659
2025-06-02 10:48:26,606 - INFO - Batch 700/4933 | Loss: 0.1924
2025-06-02 10:48:29,986 - INFO - Batch 800/4933 | Loss: 0.1699
2025-06-02 10:48:33,560 - INFO - Batch 900/4933 | Loss: 0.1677
2025-06-02 10:48:37,247 - INFO - Batch 1000/4933 | Loss: 0.1886
2025-06-02 10:48:40,891 - INFO - Batch 1100/4933 | Loss: 0.1672
2025-06-02 10:48:44,613 - INFO - Batch 1200/4933 | Loss: 0.1504
2025-06-02 10:48:48,162 - INFO - Batch 1300/4933 | Loss: 0.1740
2025-06-02 10:48:51,677 - INFO - Batch 1400/4933 | Loss: 0.2354
2025-06-02 10:48:55,177 - INFO - Batch 1500/4933 | Loss: 0.2002
2025-06-02 10:48:58,485 - INFO - Batch 1600/4933 | Loss: 0.1790
2025-06-02 10:49:02,032 - INFO - Batch 1700/4933 | Loss: 0.1634
2025-06-02 10:49:05,642 - INFO - Batch 1800/4933 | Loss: 0.2172
2025-06-02 10:49:09,394 - INFO - Batch 1900/4933 | Loss: 0.1625
2025-06-02 10:49:12,988 - INFO - Batch 2000/4933 | Loss: 0.1847
2025-06-02 10:49:16,715 - INFO - Batch 2100/4933 | Loss: 0.2084
2025-06-02 10:49:20,524 - INFO - Batch 2200/4933 | Loss: 0.2155
2025-06-02 10:49:24,513 - INFO - Batch 2300/4933 | Loss: 0.1416
2025-06-02 10:49:28,293 - INFO - Batch 2400/4933 | Loss: 0.1360
2025-06-02 10:49:31,998 - INFO - Batch 2500/4933 | Loss: 0.1788
2025-06-02 10:49:35,954 - INFO - Batch 2600/4933 | Loss: 0.1528
2025-06-02 10:49:39,977 - INFO - Batch 2700/4933 | Loss: 0.1470
2025-06-02 10:49:43,872 - INFO - Batch 2800/4933 | Loss: 0.1419
2025-06-02 10:49:47,821 - INFO - Batch 2900/4933 | Loss: 0.1737
2025-06-02 10:49:51,867 - INFO - Batch 3000/4933 | Loss: 0.1733
2025-06-02 10:49:55,872 - INFO - Batch 3100/4933 | Loss: 0.1375
2025-06-02 10:49:59,739 - INFO - Batch 3200/4933 | Loss: 0.1389
2025-06-02 10:50:03,599 - INFO - Batch 3300/4933 | Loss: 0.1494
2025-06-02 10:50:07,454 - INFO - Batch 3400/4933 | Loss: 0.1741
2025-06-02 10:50:11,374 - INFO - Batch 3500/4933 | Loss: 0.1420
2025-06-02 10:50:15,465 - INFO - Batch 3600/4933 | Loss: 0.1953
2025-06-02 10:50:19,328 - INFO - Batch 3700/4933 | Loss: 0.1551
2025-06-02 10:50:23,092 - INFO - Batch 3800/4933 | Loss: 0.2520
2025-06-02 10:50:26,919 - INFO - Batch 3900/4933 | Loss: 0.2230
2025-06-02 10:50:30,825 - INFO - Batch 4000/4933 | Loss: 0.2164
2025-06-02 10:50:34,623 - INFO - Batch 4100/4933 | Loss: 0.1739
2025-06-02 10:50:38,338 - INFO - Batch 4200/4933 | Loss: 0.1598
2025-06-02 10:50:42,190 - INFO - Batch 4300/4933 | Loss: 0.1342
2025-06-02 10:50:46,402 - INFO - Batch 4400/4933 | Loss: 0.1612
2025-06-02 10:50:50,619 - INFO - Batch 4500/4933 | Loss: 0.1314
2025-06-02 10:50:54,565 - INFO - Batch 4600/4933 | Loss: 0.1510
2025-06-02 10:50:58,534 - INFO - Batch 4700/4933 | Loss: 0.1917
2025-06-02 10:51:02,403 - INFO - Batch 4800/4933 | Loss: 0.1809
2025-06-02 10:51:06,232 - INFO - Batch 4900/4933 | Loss: 0.1271
2025-06-02 11:06:10,093 - INFO - New best model saved: logs/2025-06-02_10-28-42/checkpoints/supervised_best_epoch_2.pt
2025-06-02 11:06:10,098 - INFO - Epoch 2/5 | Train Loss: 0.1846 | Val Loss: 0.1972 | Val Action Acc: 0.0000
2025-06-02 11:06:10,460 - INFO - Batch 0/4933 | Loss: 0.1862
2025-06-02 11:06:14,584 - INFO - Batch 100/4933 | Loss: 0.2281
2025-06-02 11:06:18,713 - INFO - Batch 200/4933 | Loss: 0.2071
2025-06-02 11:06:22,898 - INFO - Batch 300/4933 | Loss: 0.1770
2025-06-02 11:06:26,764 - INFO - Batch 400/4933 | Loss: 0.1325
2025-06-02 11:06:30,730 - INFO - Batch 500/4933 | Loss: 0.1316
2025-06-02 11:06:34,808 - INFO - Batch 600/4933 | Loss: 0.1317
2025-06-02 11:06:38,883 - INFO - Batch 700/4933 | Loss: 0.1682
2025-06-02 11:06:42,927 - INFO - Batch 800/4933 | Loss: 0.1622
2025-06-02 11:06:46,948 - INFO - Batch 900/4933 | Loss: 0.1556
2025-06-02 11:06:51,013 - INFO - Batch 1000/4933 | Loss: 0.2472
2025-06-02 11:06:55,089 - INFO - Batch 1100/4933 | Loss: 0.1579
2025-06-02 11:06:59,353 - INFO - Batch 1200/4933 | Loss: 0.1751
2025-06-02 11:07:03,462 - INFO - Batch 1300/4933 | Loss: 0.2091
2025-06-02 11:07:07,509 - INFO - Batch 1400/4933 | Loss: 0.1710
2025-06-02 11:07:11,584 - INFO - Batch 1500/4933 | Loss: 0.1977
2025-06-02 11:07:15,526 - INFO - Batch 1600/4933 | Loss: 0.1584
2025-06-02 11:07:19,504 - INFO - Batch 1700/4933 | Loss: 0.2136
2025-06-02 11:07:23,696 - INFO - Batch 1800/4933 | Loss: 0.1523
2025-06-02 11:07:27,615 - INFO - Batch 1900/4933 | Loss: 0.1673
2025-06-02 11:07:31,743 - INFO - Batch 2000/4933 | Loss: 0.1786
2025-06-02 11:07:35,677 - INFO - Batch 2100/4933 | Loss: 0.2161
2025-06-02 11:07:39,678 - INFO - Batch 2200/4933 | Loss: 0.1775
2025-06-02 11:07:43,729 - INFO - Batch 2300/4933 | Loss: 0.1785
2025-06-02 11:07:47,740 - INFO - Batch 2400/4933 | Loss: 0.1507
2025-06-02 11:07:51,553 - INFO - Batch 2500/4933 | Loss: 0.1556
2025-06-02 11:07:55,505 - INFO - Batch 2600/4933 | Loss: 0.1423
2025-06-02 11:07:59,477 - INFO - Batch 2700/4933 | Loss: 0.1237
2025-06-02 11:08:03,423 - INFO - Batch 2800/4933 | Loss: 0.1580
2025-06-02 11:08:07,753 - INFO - Batch 2900/4933 | Loss: 0.1461
2025-06-02 11:08:11,737 - INFO - Batch 3000/4933 | Loss: 0.1562
2025-06-02 11:08:15,649 - INFO - Batch 3100/4933 | Loss: 0.1249
2025-06-02 11:08:19,529 - INFO - Batch 3200/4933 | Loss: 0.1428
2025-06-02 11:08:23,407 - INFO - Batch 3300/4933 | Loss: 0.1477
2025-06-02 11:08:27,212 - INFO - Batch 3400/4933 | Loss: 0.1618
2025-06-02 11:08:31,305 - INFO - Batch 3500/4933 | Loss: 0.2222
2025-06-02 11:08:35,337 - INFO - Batch 3600/4933 | Loss: 0.1649
2025-06-02 11:08:39,256 - INFO - Batch 3700/4933 | Loss: 0.1382
2025-06-02 11:08:43,070 - INFO - Batch 3800/4933 | Loss: 0.1597
2025-06-02 11:08:46,924 - INFO - Batch 3900/4933 | Loss: 0.1281
2025-06-02 11:08:50,627 - INFO - Batch 4000/4933 | Loss: 0.2049
2025-06-02 11:08:54,374 - INFO - Batch 4100/4933 | Loss: 0.1290
2025-06-02 11:08:58,019 - INFO - Batch 4200/4933 | Loss: 0.1622
2025-06-02 11:09:01,707 - INFO - Batch 4300/4933 | Loss: 0.1304
2025-06-02 11:09:05,343 - INFO - Batch 4400/4933 | Loss: 0.2740
2025-06-02 11:09:08,965 - INFO - Batch 4500/4933 | Loss: 0.1837
2025-06-02 11:09:12,592 - INFO - Batch 4600/4933 | Loss: 0.1898
2025-06-02 11:09:16,330 - INFO - Batch 4700/4933 | Loss: 0.1770
2025-06-02 11:09:20,276 - INFO - Batch 4800/4933 | Loss: 0.2239
2025-06-02 11:09:23,983 - INFO - Batch 4900/4933 | Loss: 0.1704
2025-06-02 11:24:26,015 - INFO - New best model saved: logs/2025-06-02_10-28-42/checkpoints/supervised_best_epoch_3.pt
2025-06-02 11:24:26,016 - INFO - Epoch 3/5 | Train Loss: 0.1573 | Val Loss: 0.1939 | Val Action Acc: 0.0000
2025-06-02 11:24:26,410 - INFO - Batch 0/4933 | Loss: 0.2082
2025-06-02 11:24:30,612 - INFO - Batch 100/4933 | Loss: 0.1560
2025-06-02 11:24:34,693 - INFO - Batch 200/4933 | Loss: 0.1425
2025-06-02 11:24:38,806 - INFO - Batch 300/4933 | Loss: 0.1280
2025-06-02 11:24:42,821 - INFO - Batch 400/4933 | Loss: 0.1379
2025-06-02 11:24:46,882 - INFO - Batch 500/4933 | Loss: 0.1613
2025-06-02 11:24:50,994 - INFO - Batch 600/4933 | Loss: 0.1285
2025-06-02 11:24:55,067 - INFO - Batch 700/4933 | Loss: 0.1319
2025-06-02 11:24:58,971 - INFO - Batch 800/4933 | Loss: 0.1431
2025-06-02 11:25:02,836 - INFO - Batch 900/4933 | Loss: 0.1343
2025-06-02 11:25:06,715 - INFO - Batch 1000/4933 | Loss: 0.1771
2025-06-02 11:25:10,627 - INFO - Batch 1100/4933 | Loss: 0.1676
2025-06-02 11:25:14,667 - INFO - Batch 1200/4933 | Loss: 0.1802
2025-06-02 11:25:18,653 - INFO - Batch 1300/4933 | Loss: 0.1282
2025-06-02 11:25:22,707 - INFO - Batch 1400/4933 | Loss: 0.1172
2025-06-02 11:25:26,699 - INFO - Batch 1500/4933 | Loss: 0.1392
2025-06-02 11:25:30,670 - INFO - Batch 1600/4933 | Loss: 0.1205
2025-06-02 11:25:34,533 - INFO - Batch 1700/4933 | Loss: 0.1080
2025-06-02 11:25:38,280 - INFO - Batch 1800/4933 | Loss: 0.1588
2025-06-02 11:25:41,920 - INFO - Batch 1900/4933 | Loss: 0.1327
2025-06-02 11:25:45,569 - INFO - Batch 2000/4933 | Loss: 0.1152
2025-06-02 11:25:49,122 - INFO - Batch 2100/4933 | Loss: 0.1368
2025-06-02 11:25:52,909 - INFO - Batch 2200/4933 | Loss: 0.1461
2025-06-02 11:25:56,658 - INFO - Batch 2300/4933 | Loss: 0.1227
2025-06-02 11:26:00,367 - INFO - Batch 2400/4933 | Loss: 0.1190
2025-06-02 11:26:04,137 - INFO - Batch 2500/4933 | Loss: 0.1377
2025-06-02 11:26:07,942 - INFO - Batch 2600/4933 | Loss: 0.1605
2025-06-02 11:26:11,796 - INFO - Batch 2700/4933 | Loss: 0.1694
2025-06-02 11:26:15,628 - INFO - Batch 2800/4933 | Loss: 0.1182
2025-06-02 11:26:19,327 - INFO - Batch 2900/4933 | Loss: 0.1158
2025-06-02 11:26:23,119 - INFO - Batch 3000/4933 | Loss: 0.1098
2025-06-02 11:26:26,808 - INFO - Batch 3100/4933 | Loss: 0.1431
2025-06-02 11:26:30,488 - INFO - Batch 3200/4933 | Loss: 0.1313
2025-06-02 11:26:34,237 - INFO - Batch 3300/4933 | Loss: 0.1367
2025-06-02 11:26:38,238 - INFO - Batch 3400/4933 | Loss: 0.1148
2025-06-02 11:26:42,266 - INFO - Batch 3500/4933 | Loss: 0.1598
2025-06-02 11:26:46,227 - INFO - Batch 3600/4933 | Loss: 0.1219
2025-06-02 11:26:50,169 - INFO - Batch 3700/4933 | Loss: 0.1079
2025-06-02 11:26:54,069 - INFO - Batch 3800/4933 | Loss: 0.1433
2025-06-02 11:26:57,950 - INFO - Batch 3900/4933 | Loss: 0.1011
2025-06-02 11:27:01,857 - INFO - Batch 4000/4933 | Loss: 0.1487
2025-06-02 11:27:05,779 - INFO - Batch 4100/4933 | Loss: 0.1177
2025-06-02 11:27:09,774 - INFO - Batch 4200/4933 | Loss: 0.1577
2025-06-02 11:27:13,870 - INFO - Batch 4300/4933 | Loss: 0.1656
2025-06-02 11:27:17,737 - INFO - Batch 4400/4933 | Loss: 0.1308
2025-06-02 11:27:21,709 - INFO - Batch 4500/4933 | Loss: 0.1617
2025-06-02 11:27:25,695 - INFO - Batch 4600/4933 | Loss: 0.1530
2025-06-02 11:27:29,641 - INFO - Batch 4700/4933 | Loss: 0.1394
2025-06-02 11:27:33,541 - INFO - Batch 4800/4933 | Loss: 0.1659
2025-06-02 11:27:37,571 - INFO - Batch 4900/4933 | Loss: 0.1698
2025-06-02 11:42:46,965 - INFO - Epoch 4/5 | Train Loss: 0.1434 | Val Loss: 0.1987 | Val Action Acc: 0.0000
2025-06-02 11:42:47,360 - INFO - Batch 0/4933 | Loss: 0.1081
2025-06-02 11:42:51,218 - INFO - Batch 100/4933 | Loss: 0.1365
2025-06-02 11:42:55,095 - INFO - Batch 200/4933 | Loss: 0.1289
2025-06-02 11:42:58,944 - INFO - Batch 300/4933 | Loss: 0.1454
2025-06-02 11:43:02,916 - INFO - Batch 400/4933 | Loss: 0.1739
2025-06-02 11:43:07,016 - INFO - Batch 500/4933 | Loss: 0.1170
2025-06-02 11:43:11,053 - INFO - Batch 600/4933 | Loss: 0.1586
2025-06-02 11:43:15,081 - INFO - Batch 700/4933 | Loss: 0.1468
2025-06-02 11:43:19,030 - INFO - Batch 800/4933 | Loss: 0.1146
2025-06-02 11:43:23,089 - INFO - Batch 900/4933 | Loss: 0.1147
2025-06-02 11:43:27,055 - INFO - Batch 1000/4933 | Loss: 0.1780
2025-06-02 11:43:30,900 - INFO - Batch 1100/4933 | Loss: 0.1264
2025-06-02 11:43:34,968 - INFO - Batch 1200/4933 | Loss: 0.1172
2025-06-02 11:43:38,964 - INFO - Batch 1300/4933 | Loss: 0.1277
2025-06-02 11:43:42,913 - INFO - Batch 1400/4933 | Loss: 0.1282
2025-06-02 11:43:46,932 - INFO - Batch 1500/4933 | Loss: 0.1406
2025-06-02 11:43:50,973 - INFO - Batch 1600/4933 | Loss: 0.1257
2025-06-02 11:43:55,163 - INFO - Batch 1700/4933 | Loss: 0.1172
2025-06-02 11:43:59,322 - INFO - Batch 1800/4933 | Loss: 0.1146
2025-06-02 11:44:03,459 - INFO - Batch 1900/4933 | Loss: 0.1577
2025-06-02 11:44:07,438 - INFO - Batch 2000/4933 | Loss: 0.1598
2025-06-02 11:44:11,261 - INFO - Batch 2100/4933 | Loss: 0.1134
2025-06-02 11:44:15,096 - INFO - Batch 2200/4933 | Loss: 0.1423
2025-06-02 11:44:19,128 - INFO - Batch 2300/4933 | Loss: 0.1158
2025-06-02 11:44:23,038 - INFO - Batch 2400/4933 | Loss: 0.1335
2025-06-02 11:44:26,992 - INFO - Batch 2500/4933 | Loss: 0.1227
2025-06-02 11:44:30,932 - INFO - Batch 2600/4933 | Loss: 0.1432
2025-06-02 11:44:34,948 - INFO - Batch 2700/4933 | Loss: 0.1174
2025-06-02 11:44:38,989 - INFO - Batch 2800/4933 | Loss: 0.1294
2025-06-02 11:44:43,073 - INFO - Batch 2900/4933 | Loss: 0.1490
2025-06-02 11:44:47,121 - INFO - Batch 3000/4933 | Loss: 0.1604
2025-06-02 11:44:51,151 - INFO - Batch 3100/4933 | Loss: 0.1246
2025-06-02 11:44:55,232 - INFO - Batch 3200/4933 | Loss: 0.1440
2025-06-02 11:44:59,255 - INFO - Batch 3300/4933 | Loss: 0.1558
2025-06-02 11:45:03,246 - INFO - Batch 3400/4933 | Loss: 0.1565
2025-06-02 11:45:07,371 - INFO - Batch 3500/4933 | Loss: 0.1923
2025-06-02 11:45:11,320 - INFO - Batch 3600/4933 | Loss: 0.1165
2025-06-02 11:45:15,323 - INFO - Batch 3700/4933 | Loss: 0.1365
2025-06-02 11:45:19,393 - INFO - Batch 3800/4933 | Loss: 0.1275
2025-06-02 11:45:23,480 - INFO - Batch 3900/4933 | Loss: 0.1327
2025-06-02 11:45:27,370 - INFO - Batch 4000/4933 | Loss: 0.1155
2025-06-02 11:45:31,093 - INFO - Batch 4100/4933 | Loss: 0.1274
2025-06-02 11:45:34,854 - INFO - Batch 4200/4933 | Loss: 0.0960
2025-06-02 11:45:38,600 - INFO - Batch 4300/4933 | Loss: 0.1449
2025-06-02 11:45:42,461 - INFO - Batch 4400/4933 | Loss: 0.0970
2025-06-02 11:45:46,338 - INFO - Batch 4500/4933 | Loss: 0.1313
2025-06-02 11:45:50,106 - INFO - Batch 4600/4933 | Loss: 0.1346
2025-06-02 11:45:53,994 - INFO - Batch 4700/4933 | Loss: 0.1084
2025-06-02 11:45:57,745 - INFO - Batch 4800/4933 | Loss: 0.1345
2025-06-02 11:46:01,537 - INFO - Batch 4900/4933 | Loss: 0.1265
2025-06-02 12:01:06,481 - INFO - Epoch 5/5 | Train Loss: 0.1353 | Val Loss: 0.1963 | Val Action Acc: 0.0000
2025-06-02 12:01:07,044 - INFO - Training plots saved to logs/2025-06-02_10-28-42
2025-06-02 12:01:07,400 - INFO - Training completed. Best model: logs/2025-06-02_10-28-42/checkpoints/supervised_best_epoch_3.pt
2025-06-02 12:01:07,401 - INFO - Final model: logs/2025-06-02_10-28-42/checkpoints/final_model.pt
2025-06-02 12:01:07,401 - INFO - ✅ IL training completed. Model saved: logs/2025-06-02_10-28-42/checkpoints/supervised_best_epoch_3.pt
2025-06-02 12:01:09,949 - INFO - 🤖 Training RL Models...
2025-06-02 12:01:10,068 - ERROR - ❌ Comparison experiment failed: module 'cv2.dnn' has no attribute 'DictValue'
