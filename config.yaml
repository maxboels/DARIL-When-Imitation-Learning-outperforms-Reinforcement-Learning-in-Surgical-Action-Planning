# Debug mode
debug: false
experiment:
  max_videos: 2  # Set to null to use all videos
  pretrain_next_frame: true
  pretrain_reward_model: false
  calculate_action_rewards: false
  train_action_policy: false
  run_tdmpc: false
  analyze_results: false
  run_evaluation: false

# Training parameters
training:
  epochs: 1
  batch_size: 16
  learning_rate: 0.0001
  weight_decay: 0.0
  gradient_clip_val: 1.0
  num_workers: 4
  pin_memory: true
  log_every_n_steps: 100
  log_dir: "logs"
  save_checkpoint_every_n_epochs: 1
  checkpoint_dir: "checkpoints"

eval:
  world_model:
    use_memory: false
    horizon: 15
  action_prediction:
    top_ks: [1, 3, 5, 10]
    horizons: [1, 3, 5, 10, 15]
    evaluate_on_train: True       # Set to False to skip training data evaluation

# Data paths configuration
data:
  context_length: 5
  train_shift: 1
  padding_value: 0.0
  horizon: 15
  paths:
    data_dir: "/home/maxboels/datasets/CholecT50"
    fold: 0
    metadata_file: "embeddings_f0_swin_bas_129_with_enhanced.csv"
    video_global_outcome_file: null
  risk_score_path: null
  max_videos: 2                     # Set to null to use all videos
  frame_risk_agg: 'max'

# Model configurations
models:
  # GPT-2 model configuration
  world_model:
    hidden_dim: 768
    embedding_dim: 1024
    n_layer: 6
    use_head: true
    targets_dims:
      next_z: 512
      next_a: 100
      next_r: 1
      next_q: 1
      final_R: 1
    target_heads: "next_a" # "next_r", "next_q", "final_R"]
    # Loss weights
    loss_weights:
      w_z: 1.0  # Embedding prediction (default: 0.8)
      w_a: 1.0  # Action prediction (default: 0.1)
      w_r: null  # Reward prediction (0.1)
      w_q: null  # Q-value prediction (0.1)
      w_R: null  # Final reward estimation (0.05)
  
  # Reward model configuration
  reward:
    input_dim: 1024
    context_length: 5
    hidden_dim: 256
    num_heads: 4
    num_layers: 2