{
    "VID02": {
        "video_id": "VID02",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.42638269057153944,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6894510703594722,
                    "mAP_present_only_with_null_verb": 0.38073844223374176,
                    "mAP_freq_weighted_with_null_verb": 0.6626269154274768,
                    "mAP_sample_wise_with_null_verb": 0.7056290122090757,
                    "mAP_standard_all_actions": 0.6894510703594722,
                    "mAP_present_only_all_actions": 0.38073844223374176,
                    "mAP_freq_weighted_all_actions": 0.6626269154274768,
                    "mAP_sample_wise_all_actions": 0.7056290122090757,
                    "exact_match_with_null_verb": 0.3783022190912293,
                    "hamming_accuracy_with_null_verb": 0.9897393448397322,
                    "precision_with_null_verb": 0.8386476416627797,
                    "recall_with_null_verb": 0.7627691327480965,
                    "f1_with_null_verb": 0.7957542693500188,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7227522908085438,
                    "mAP_present_only": 0.42638269057153944,
                    "mAP_freq_weighted": 0.6866201079061912,
                    "exact_match": 0.4166960197252554,
                    "hamming_accuracy": 0.9904034234409779,
                    "precision": 0.854227516335093,
                    "recall": 0.7744780529191916,
                    "f1": 0.8091225304403823,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.42638269057153944,
                    "exact_match": 0.4166960197252554,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID06": {
        "video_id": "VID06",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.5764606228660135,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.8321718897428473,
                    "mAP_present_only_with_null_verb": 0.5405729658094908,
                    "mAP_freq_weighted_with_null_verb": 0.7663324189928232,
                    "mAP_sample_wise_with_null_verb": 0.831135649780638,
                    "mAP_standard_all_actions": 0.8321718897428473,
                    "mAP_present_only_all_actions": 0.5405729658094908,
                    "mAP_freq_weighted_all_actions": 0.7663324189928232,
                    "mAP_sample_wise_all_actions": 0.831135649780638,
                    "exact_match_with_null_verb": 0.3841151881096145,
                    "hamming_accuracy_with_null_verb": 0.9906827682303763,
                    "precision_with_null_verb": 0.8890211405863649,
                    "recall_with_null_verb": 0.8188297522565502,
                    "f1_with_null_verb": 0.8503065984257536,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8509359169629399,
                    "mAP_present_only": 0.5764606228660135,
                    "mAP_freq_weighted": 0.7982511437103577,
                    "exact_match": 0.4444960520204366,
                    "hamming_accuracy": 0.9914320443517705,
                    "precision": 0.8924446215368639,
                    "recall": 0.8390057075462191,
                    "f1": 0.8636957735599068,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.5764606228660135,
                    "exact_match": 0.4444960520204366,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID111": {
        "video_id": "VID111",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.47852070629107035,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.8278151873033877,
                    "mAP_present_only_with_null_verb": 0.4407420251840951,
                    "mAP_freq_weighted_with_null_verb": 0.6112920233545243,
                    "mAP_sample_wise_with_null_verb": 0.6896507543502812,
                    "mAP_standard_all_actions": 0.8278151873033877,
                    "mAP_present_only_all_actions": 0.4407420251840951,
                    "mAP_freq_weighted_all_actions": 0.6112920233545243,
                    "mAP_sample_wise_all_actions": 0.6896507543502812,
                    "exact_match_with_null_verb": 0.4578088578088578,
                    "hamming_accuracy_with_null_verb": 0.9897529137529137,
                    "precision_with_null_verb": 0.7950671907828424,
                    "recall_with_null_verb": 0.7512960138248371,
                    "f1_with_null_verb": 0.7713701949948033,
                    "num_predictions": 2145,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8506704006093273,
                    "mAP_present_only": 0.47852070629107035,
                    "mAP_freq_weighted": 0.6548559258125269,
                    "exact_match": 0.5165501165501165,
                    "hamming_accuracy": 0.9912364231513168,
                    "precision": 0.7992994693769102,
                    "recall": 0.7946261010843552,
                    "f1": 0.7969438423740653,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.47852070629107035,
                    "exact_match": 0.5165501165501165,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID14": {
        "video_id": "VID14",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.33023937593838104,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 36,
                        "subset_action_sparsity": 0.6170212765957447
                    },
                    "mAP_standard_with_null_verb": 0.7058583561991314,
                    "mAP_present_only_with_null_verb": 0.30697160048568656,
                    "mAP_freq_weighted_with_null_verb": 0.6729235828097798,
                    "mAP_sample_wise_with_null_verb": 0.7560169924564051,
                    "mAP_standard_all_actions": 0.7058583561991314,
                    "mAP_present_only_all_actions": 0.30697160048568656,
                    "mAP_freq_weighted_all_actions": 0.6729235828097798,
                    "mAP_sample_wise_all_actions": 0.7560169924564051,
                    "exact_match_with_null_verb": 0.4127634660421546,
                    "hamming_accuracy_with_null_verb": 0.9902576112412178,
                    "precision_with_null_verb": 0.8686053926386497,
                    "recall_with_null_verb": 0.7809981323084512,
                    "f1_with_null_verb": 0.8186829688675437,
                    "num_predictions": 1708,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 41,
                    "action_sparsity_with_null_verb": 0.5900000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7434959312104439,
                    "mAP_present_only": 0.33023937593838104,
                    "mAP_freq_weighted": 0.7007088575101035,
                    "exact_match": 0.46545667447306793,
                    "hamming_accuracy": 0.9908004883153122,
                    "precision": 0.873649219536892,
                    "recall": 0.7991415974277203,
                    "f1": 0.8321218786901118,
                    "num_actions_total": 94,
                    "num_actions_present": 36,
                    "action_sparsity": 0.6170212765957447
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.33023937593838104,
                    "exact_match": 0.46545667447306793,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID23": {
        "video_id": "VID23",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.4329262522599716,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.7800272436048933,
                    "mAP_present_only_with_null_verb": 0.3871846567899782,
                    "mAP_freq_weighted_with_null_verb": 0.6574296180076737,
                    "mAP_sample_wise_with_null_verb": 0.7332226403856218,
                    "mAP_standard_all_actions": 0.7800272436048933,
                    "mAP_present_only_all_actions": 0.3871846567899782,
                    "mAP_freq_weighted_all_actions": 0.6574296180076737,
                    "mAP_sample_wise_all_actions": 0.7332226403856218,
                    "exact_match_with_null_verb": 0.382262996941896,
                    "hamming_accuracy_with_null_verb": 0.9893761467889908,
                    "precision_with_null_verb": 0.8573275178843326,
                    "recall_with_null_verb": 0.7658215956132678,
                    "f1_with_null_verb": 0.8046066742249112,
                    "num_predictions": 1635,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8112349208378645,
                    "mAP_present_only": 0.4329262522599716,
                    "mAP_freq_weighted": 0.685195027730448,
                    "exact_match": 0.4525993883792049,
                    "hamming_accuracy": 0.9903311861539462,
                    "precision": 0.8651944204448685,
                    "recall": 0.7877801106450654,
                    "f1": 0.8217327256346662,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.4329262522599716,
                    "exact_match": 0.4525993883792049,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID25": {
        "video_id": "VID25",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.6565770472050828,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.8658722188281929,
                    "mAP_present_only_with_null_verb": 0.5610469954930498,
                    "mAP_freq_weighted_with_null_verb": 0.7773330759008895,
                    "mAP_sample_wise_with_null_verb": 0.8023650999871792,
                    "mAP_standard_all_actions": 0.8658722188281929,
                    "mAP_present_only_all_actions": 0.5610469954930498,
                    "mAP_freq_weighted_all_actions": 0.7773330759008895,
                    "mAP_sample_wise_all_actions": 0.8023650999871792,
                    "exact_match_with_null_verb": 0.4316580554250822,
                    "hamming_accuracy_with_null_verb": 0.991286989196806,
                    "precision_with_null_verb": 0.8924929019837182,
                    "recall_with_null_verb": 0.8208436671193323,
                    "f1_with_null_verb": 0.8529345168730645,
                    "num_predictions": 2129,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.9020012552266674,
                    "mAP_present_only": 0.6565770472050828,
                    "mAP_freq_weighted": 0.8177370388495433,
                    "exact_match": 0.47534053546265853,
                    "hamming_accuracy": 0.9919350808990336,
                    "precision": 0.8968437704500241,
                    "recall": 0.8393841819687577,
                    "f1": 0.8657874518305171,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.6565770472050828,
                    "exact_match": 0.47534053546265853,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID50": {
        "video_id": "VID50",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.5615866201626938,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.9021318544492034,
                    "mAP_present_only_with_null_verb": 0.5118436358289083,
                    "mAP_freq_weighted_with_null_verb": 0.776580633426808,
                    "mAP_sample_wise_with_null_verb": 0.8474134796287359,
                    "mAP_standard_all_actions": 0.9021318544492034,
                    "mAP_present_only_all_actions": 0.5118436358289083,
                    "mAP_freq_weighted_all_actions": 0.776580633426808,
                    "mAP_sample_wise_all_actions": 0.8474134796287359,
                    "exact_match_with_null_verb": 0.49817184643510054,
                    "hamming_accuracy_with_null_verb": 0.991672760511883,
                    "precision_with_null_verb": 0.8792146494413744,
                    "recall_with_null_verb": 0.8561968194960761,
                    "f1_with_null_verb": 0.8673312884119542,
                    "num_predictions": 1094,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 18,
                    "action_sparsity_with_null_verb": 0.8200000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.9147381481127987,
                    "mAP_present_only": 0.5615866201626938,
                    "mAP_freq_weighted": 0.8118479086016409,
                    "exact_match": 0.526508226691042,
                    "hamming_accuracy": 0.9922886926757166,
                    "precision": 0.8845947469139489,
                    "recall": 0.8759269705278917,
                    "f1": 0.8802094981491027,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.5615866201626938,
                    "exact_match": 0.526508226691042,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID51": {
        "video_id": "VID51",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.4727835822426248,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 24,
                        "subset_action_sparsity": 0.7446808510638299
                    },
                    "mAP_standard_with_null_verb": 0.7792680275118016,
                    "mAP_present_only_with_null_verb": 0.4457518190062124,
                    "mAP_freq_weighted_with_null_verb": 0.7219155722854705,
                    "mAP_sample_wise_with_null_verb": 0.7702794024812947,
                    "mAP_standard_all_actions": 0.7792680275118016,
                    "mAP_present_only_all_actions": 0.4457518190062124,
                    "mAP_freq_weighted_all_actions": 0.7219155722854705,
                    "mAP_sample_wise_all_actions": 0.7702794024812947,
                    "exact_match_with_null_verb": 0.4031929347826087,
                    "hamming_accuracy_with_null_verb": 0.9898879076086956,
                    "precision_with_null_verb": 0.8428594196179684,
                    "recall_with_null_verb": 0.7975666069557303,
                    "f1_with_null_verb": 0.8185509889868772,
                    "num_predictions": 2944,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8015617656789681,
                    "mAP_present_only": 0.4727835822426248,
                    "mAP_freq_weighted": 0.7550266741740417,
                    "exact_match": 0.45210597826086957,
                    "hamming_accuracy": 0.9908649398704903,
                    "precision": 0.8513490641375435,
                    "recall": 0.8148537193748517,
                    "f1": 0.8320658544551647,
                    "num_actions_total": 94,
                    "num_actions_present": 24,
                    "action_sparsity": 0.7446808510638299
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.4727835822426248,
                    "exact_match": 0.45210597826086957,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID66": {
        "video_id": "VID66",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.6416745076973853,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 20,
                        "subset_action_sparsity": 0.7872340425531915
                    },
                    "mAP_standard_with_null_verb": 0.8546407242659836,
                    "mAP_present_only_with_null_verb": 0.5853944533303641,
                    "mAP_freq_weighted_with_null_verb": 0.8451793105860761,
                    "mAP_sample_wise_with_null_verb": 0.8564433558370658,
                    "mAP_standard_all_actions": 0.8546407242659836,
                    "mAP_present_only_all_actions": 0.5853944533303641,
                    "mAP_freq_weighted_all_actions": 0.8451793105860761,
                    "mAP_sample_wise_all_actions": 0.8564433558370658,
                    "exact_match_with_null_verb": 0.5208333333333334,
                    "hamming_accuracy_with_null_verb": 0.9929166666666667,
                    "precision_with_null_verb": 0.9088152030118316,
                    "recall_with_null_verb": 0.8513185812804607,
                    "f1_with_null_verb": 0.8778151040554586,
                    "num_predictions": 1824,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 23,
                    "action_sparsity_with_null_verb": 0.77,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8918456399356137,
                    "mAP_present_only": 0.6416745076973853,
                    "mAP_freq_weighted": 0.8666478742779006,
                    "exact_match": 0.5350877192982456,
                    "hamming_accuracy": 0.9931819242254573,
                    "precision": 0.9148481431152118,
                    "recall": 0.8620975208650221,
                    "f1": 0.886616715913046,
                    "num_actions_total": 94,
                    "num_actions_present": 20,
                    "action_sparsity": 0.7872340425531915
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.6416745076973853,
                    "exact_match": 0.5350877192982456,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID79": {
        "video_id": "VID79",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.364592789655982,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 31,
                        "subset_action_sparsity": 0.6702127659574468
                    },
                    "mAP_standard_with_null_verb": 0.7243815200917801,
                    "mAP_present_only_with_null_verb": 0.3455042224771667,
                    "mAP_freq_weighted_with_null_verb": 0.7675784460474653,
                    "mAP_sample_wise_with_null_verb": 0.7805048933174522,
                    "mAP_standard_all_actions": 0.7243815200917801,
                    "mAP_present_only_all_actions": 0.3455042224771667,
                    "mAP_freq_weighted_all_actions": 0.7675784460474653,
                    "mAP_sample_wise_all_actions": 0.7805048933174522,
                    "exact_match_with_null_verb": 0.434680726420621,
                    "hamming_accuracy_with_null_verb": 0.9903661394258934,
                    "precision_with_null_verb": 0.8732007967900435,
                    "recall_with_null_verb": 0.8118779408395338,
                    "f1_with_null_verb": 0.839687273071557,
                    "num_predictions": 3414,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 36,
                    "action_sparsity_with_null_verb": 0.64,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7478976221205899,
                    "mAP_present_only": 0.364592789655982,
                    "mAP_freq_weighted": 0.7962739744842455,
                    "exact_match": 0.46924428822495606,
                    "hamming_accuracy": 0.9909197422378442,
                    "precision": 0.8832941784876559,
                    "recall": 0.8236848664429333,
                    "f1": 0.8508784845304382,
                    "num_actions_total": 94,
                    "num_actions_present": 31,
                    "action_sparsity": 0.6702127659574468
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.364592789655982,
                    "exact_match": 0.46924428822495606,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    }
}