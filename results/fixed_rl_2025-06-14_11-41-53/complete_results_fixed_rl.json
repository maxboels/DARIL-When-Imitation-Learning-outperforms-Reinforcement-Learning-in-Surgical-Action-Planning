{
  "experiment_name": "fixed_rl_2025-06-14_11-41-53",
  "config": {
    "debug": false,
    "training_mode": "rl",
    "preprocess": {
      "extract_rewards": false,
      "rewards": {
        "grounded": {
          "phase_completion": true,
          "phase_transition": true,
          "phase_progression": true,
          "global_progression": true
        },
        "imitation": {
          "action_distribution": true
        },
        "expert_knowledge": {
          "risk_score": true,
          "frame_risk_agg": "max"
        }
      }
    },
    "experiment": {
      "max_videos": 50,
      "train": {
        "max_videos": 40
      },
      "test": {
        "max_videos": 10
      },
      "dual_world_model": {
        "train": true,
        "best_model_path": null
      },
      "autoregressive_il": {
        "enabled": false,
        "il_model_path": null
      },
      "world_model": {
        "enabled": true,
        "wm_model_path": null
      },
      "rl_experiments": {
        "enabled": true,
        "eval_episodes": 10
      }
    },
    "training": {
      "epochs": 5,
      "batch_size": 16,
      "learning_rate": 3e-05,
      "log_every_n_steps": 50,
      "scheduler": {
        "type": "cosine",
        "warmup_steps": 100
      },
      "weight_decay": 0.01,
      "gradient_clip_val": 1.0,
      "dropout": 0.1,
      "num_workers": 4,
      "pin_memory": true,
      "log_dir": "logs",
      "checkpoint_dir": "checkpoints",
      "eval_epoch_interval": 1,
      "save_model": true
    },
    "evaluation": {
      "prediction_horizon": 15,
      "supervised": {
        "action_prediction": true
      },
      "rl": {
        "rollout_horizon": 15,
        "use_best_actions": true
      },
      "comparison": {
        "statistical_tests": true,
        "effect_size_threshold": 0.2
      },
      "world_model": {
        "use_memory": false,
        "overall_horizon": 1
      }
    },
    "rl_training": {
      "outcome_based_rewards": true,
      "rl_horizon": 30,
      "reward_mode": "dense",
      "normalize_rewards": true,
      "early_termination": true,
      "timesteps": 50000,
      "reward_weights": {
        "expert_matching": 10.0,
        "action_sparsity": 1.0,
        "world_model_rewards": 0.5,
        "completion_bonus": 5.0,
        "consistency_bonus": 1.0,
        "phase_completion": 1.0,
        "risk_penalty": -0.5
      },
      "ppo": {
        "learning_rate": "5e-5",
        "n_steps": 512,
        "batch_size": 64,
        "n_epochs": 10,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "clip_range": 0.1,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      },
      "a2c": {
        "learning_rate": "1e-4",
        "n_steps": 32,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    "data": {
      "context_length": 20,
      "train_shift": 1,
      "padding_value": 0.0,
      "max_horizon": 15,
      "paths": {
        "data_dir": "/nfs/home/mboels/datasets/CholecT50",
        "class_labels_file_path": "./data/labels.json",
        "fold": 0,
        "metadata_file": "embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv",
        "video_global_outcome_file": "embeddings_f0_swin_bas_129_with_enhanced_global_metrics.csv"
      },
      "frame_risk_agg": "max"
    },
    "models": {
      "autoregressive_il": {
        "hidden_dim": 768,
        "embedding_dim": 1024,
        "n_layer": 6,
        "num_action_classes": 100,
        "num_phase_classes": 7,
        "dropout": 0.1,
        "max_length": 1024
      },
      "conditional_world_model": {
        "hidden_dim": 768,
        "embedding_dim": 1024,
        "action_embedding_dim": 128,
        "n_layer": 6,
        "num_action_classes": 100,
        "num_phase_classes": 7,
        "dropout": 0.1,
        "max_sequence_length": 512
      }
    },
    "fair_evaluation": {
      "enabled": true,
      "include_traditional_metrics": true,
      "include_clinical_metrics": true,
      "clinical_outcome_weights": {
        "phase_progression": 2.0,
        "innovation": 0.5
      }
    },
    "supervised_learning": {
      "data_augmentation": false
    },
    "research_comparison": {
      "methods": [
        "autoregressive_il",
        "conditional_world_model",
        "direct_video_rl"
      ]
    },
    "advanced": {
      "mixed_precision": false
    },
    "hardware": {
      "persistent_workers": true
    },
    "rl_debugging": {
      "enabled": true,
      "save_training_curves": true,
      "monitor_expert_matching": true,
      "log_action_distributions": true,
      "convergence_analysis": true,
      "episode_log_frequency": 10,
      "eval_frequency": 1000,
      "reward_improvement_threshold": 0.1,
      "expert_matching_threshold": 0.5,
      "debug_dir": "rl_debug",
      "plot_dir": "rl_plots"
    }
  },
  "timestamp": "2025-06-14_11-41-53",
  "results_dir": "results/fixed_rl_2025-06-14_11-41-53",
  "method_1_autoregressive_il": {
    "status": "skipped",
    "reason": "Autoregressive IL disabled in config"
  },
  "method_2_conditional_world_model": {
    "status": "success",
    "world_model_path": "results/fixed_rl_2025-06-14_11-41-53/logs/checkpoints/world_model_best_epoch_2.pt",
    "world_model_evaluation": {
      "overall_metrics": {
        "state_loss": 0.14105887609175313,
        "reward_risk_penalty_loss": 0.20828989370575215,
        "reward_phase_completion_loss": 0.011475789479669828,
        "reward_phase_initiation_loss": 0.01635777058217936,
        "reward_phase_progression_loss": 0.06696433697172163,
        "reward_action_probability_loss": 0.019549877182335522,
        "total_reward_loss": 0.06452753439997473,
        "phase_loss": 0.9434270008770298,
        "total_loss": 1.0239451966628998
      },
      "model_type": "ConditionalWorldModel",
      "evaluation_summary": {
        "best_metric": "state_loss",
        "best_value": 0.14105887609175313,
        "strength": "Action-conditioned state-reward prediction",
        "architecture": "ConditionalWorldModel with action conditioning"
      }
    },
    "world_model_pretrained": null,
    "rl_models": {
      "world_model_ppo": {
        "algorithm": "PPO_WorldModel",
        "mean_reward": 189.650341,
        "std_reward": 51.760654913434514,
        "model_path": "results/fixed_rl_2025-06-14_11-41-53/logs/rl_training/ppo_world_model_final.zip",
        "status": "success",
        "training_timesteps": 50000,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": -720.0,
          "std_reward": 0.0,
          "episodes": 1672,
          "last_reward": -720.0,
          "reward_trend": "stable",
          "avg_expert_matching": 0.6828181886927349,
          "std_expert_matching": 0.009342489486329828,
          "last_expert_matching": 0.6641379310344826
        },
        "monitoring_data": [
          {
            "timestep": 2500,
            "mean_reward": -508.8249568,
            "std_reward": 8.01605120512417
          },
          {
            "timestep": 5000,
            "mean_reward": -498.76461900000004,
            "std_reward": 9.313253636446168
          },
          {
            "timestep": 7500,
            "mean_reward": -419.8473914,
            "std_reward": 164.18034765375452
          },
          {
            "timestep": 10000,
            "mean_reward": -413.78710219999994,
            "std_reward": 174.26514891200796
          },
          {
            "timestep": 12500,
            "mean_reward": -500.49215979999997,
            "std_reward": 10.261106336502237
          },
          {
            "timestep": 15000,
            "mean_reward": 12.662191399999994,
            "std_reward": 221.82287445102673
          },
          {
            "timestep": 17500,
            "mean_reward": -431.08274199999994,
            "std_reward": 78.91911513844394
          },
          {
            "timestep": 20000,
            "mean_reward": -59.08076680000001,
            "std_reward": 262.4431545303144
          },
          {
            "timestep": 22500,
            "mean_reward": 12.218158600000004,
            "std_reward": 264.72619152766254
          },
          {
            "timestep": 25000,
            "mean_reward": 14.102844000000005,
            "std_reward": 120.86329433063938
          },
          {
            "timestep": 27500,
            "mean_reward": -342.14953959999997,
            "std_reward": 284.314230298305
          },
          {
            "timestep": 30000,
            "mean_reward": -500.9836972,
            "std_reward": 11.419673298629263
          },
          {
            "timestep": 32500,
            "mean_reward": -487.9844384,
            "std_reward": 25.693120285331585
          },
          {
            "timestep": 35000,
            "mean_reward": -58.34152280000001,
            "std_reward": 257.2800054705324
          },
          {
            "timestep": 37500,
            "mean_reward": 38.2130264,
            "std_reward": 221.33104147606824
          },
          {
            "timestep": 40000,
            "mean_reward": 197.2957542,
            "std_reward": 23.954297414917345
          },
          {
            "timestep": 42500,
            "mean_reward": 208.72446380000002,
            "std_reward": 14.09595335697705
          },
          {
            "timestep": 45000,
            "mean_reward": 297.88122899999996,
            "std_reward": 178.7891565815253
          },
          {
            "timestep": 47500,
            "mean_reward": 158.9349264,
            "std_reward": 98.88750118647802
          },
          {
            "timestep": 50000,
            "mean_reward": 199.968608,
            "std_reward": 21.208119180581736
          }
        ],
        "expert_matching_enabled": true,
        "reward_design": "expert_demonstration_matching",
        "optimal_threshold": 0.3,
        "threshold_map": 0.375
      },
      "direct_video_ppo": {
        "algorithm": "PPO_DirectVideo",
        "status": "failed",
        "error": "No module named 'rl_environment'"
      }
    },
    "model_type": "ConditionalWorldModel",
    "approach": "FIXED: Action-conditioned world model + improved RL (TRAINED)",
    "method_description": "World model-based RL with fixed rewards and debugging (trained WM)",
    "improvements": [
      "Expert demonstration matching rewards",
      "Proper action space handling",
      "Enhanced monitoring and debugging",
      "Optimized hyperparameters",
      "Trained world model from scratch"
    ]
  },
  "method_3_direct_video_rl": {
    "status": "success",
    "rl_models": {
      "ppo": {
        "algorithm": "PPO_DirectVideo",
        "approach": "Direct RL on video sequences (no world model)",
        "mean_reward": 42.9383094,
        "std_reward": 5.21386122947405,
        "model_path": "results/fixed_rl_2025-06-14_11-41-53/logs/direct_video_rl/ppo_direct_video.zip",
        "status": "success",
        "training_timesteps": 50000,
        "uses_world_model": false,
        "uses_real_frames": true,
        "simulation_based": false,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": 36.79642750150161,
          "episodes": 100,
          "last_length": 30,
          "last_reward": 49.462040302267006,
          "using_real_frames": true
        }
      },
      "a2c": {
        "algorithm": "A2C_DirectVideo",
        "approach": "Direct RL on video sequences (no world model)",
        "mean_reward": 47.0415,
        "std_reward": 4.915072349565244,
        "model_path": "results/fixed_rl_2025-06-14_11-41-53/logs/direct_video_rl/a2c_direct_video.zip",
        "status": "success",
        "training_timesteps": 50000,
        "uses_world_model": false,
        "uses_real_frames": true,
        "simulation_based": false,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": 36.65549333348795,
          "episodes": 100,
          "last_length": 30,
          "last_reward": 42.264545454545456,
          "using_real_frames": true
        }
      }
    },
    "model_type": "DirectVideoRL",
    "approach": "FIXED: Direct RL on video sequences with improved rewards",
    "method_description": "Model-free RL on offline video episodes with fixed reward design",
    "improvements": [
      "Expert demonstration matching rewards",
      "Proper continuous action space [0,1]",
      "Better episode termination",
      "Meaningful reward functions"
    ]
  },
  "comprehensive_evaluation": {
    "evaluator": "<evaluation.integrated_evaluation.IntegratedEvaluationFramework object at 0x7f3f4103f610>",
    "results": {
      "status": "success",
      "evaluation_type": "comprehensive_evaluation_with_proper_batches",
      "num_models": 3,
      "num_videos": 10,
      "horizon": 15,
      "video_results": {
        "VID02": {
          "video_id": "VID02",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.044689812794925766,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.6235441809291278,
                "mAP_present_only_with_null_verb": 0.039835826262140765,
                "mAP_freq_weighted_with_null_verb": 0.19893068805564057,
                "mAP_sample_wise_with_null_verb": 0.07518030551833592,
                "mAP_standard_all_actions": 0.6235441809291278,
                "mAP_present_only_all_actions": 0.039835826262140765,
                "mAP_freq_weighted_all_actions": 0.19893068805564057,
                "mAP_sample_wise_all_actions": 0.07518030551833592,
                "exact_match_with_null_verb": 0.006692497358224727,
                "hamming_accuracy_with_null_verb": 0.9559140542444523,
                "precision_with_null_verb": 0.5061495603130152,
                "recall_with_null_verb": 0.5132215327201598,
                "f1_with_null_verb": 0.5067427659885683,
                "num_predictions": 2839,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6622480293431695,
                "mAP_present_only": 0.044689812794925766,
                "mAP_freq_weighted": 0.20961375437794605,
                "exact_match": 0.007396970764353646,
                "hamming_accuracy": 0.9539956382604003,
                "precision": 0.5061518782518687,
                "recall": 0.514022241103994,
                "f1": 0.5065776975240471,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.07624039693822657,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.41278307856339064,
                "mAP_present_only_with_null_verb": 0.06700905459820786,
                "mAP_freq_weighted_with_null_verb": 0.34605042095844946,
                "mAP_sample_wise_with_null_verb": 0.04534586093932144,
                "mAP_standard_all_actions": 0.41278307856339064,
                "mAP_present_only_all_actions": 0.06700905459820786,
                "mAP_freq_weighted_all_actions": 0.34605042095844946,
                "mAP_sample_wise_all_actions": 0.04534586093932144,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8582951743571681,
                "precision_with_null_verb": 0.5002092723337831,
                "recall_with_null_verb": 0.5016871157541709,
                "f1_with_null_verb": 0.4750534021994887,
                "num_predictions": 2839,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.4376035224922377,
                "mAP_present_only": 0.07624039693822657,
                "mAP_freq_weighted": 0.364487290396703,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8602744448524727,
                "precision": 0.5001966688049252,
                "recall": 0.5015621518717966,
                "f1": 0.4756049506103877,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.060554709992767,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.6778251638261164,
                "mAP_present_only_with_null_verb": 0.05242695242975376,
                "mAP_freq_weighted_with_null_verb": 0.25984493100740347,
                "mAP_sample_wise_with_null_verb": 0.07761509145923483,
                "mAP_standard_all_actions": 0.6778251638261164,
                "mAP_present_only_all_actions": 0.05242695242975376,
                "mAP_freq_weighted_all_actions": 0.25984493100740347,
                "mAP_sample_wise_all_actions": 0.07761509145923483,
                "exact_match_with_null_verb": 0.07713983797111659,
                "hamming_accuracy_with_null_verb": 0.9856533990841846,
                "precision_with_null_verb": 0.4928266995420923,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49638743576234595,
                "num_predictions": 2839,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7201652327638032,
                "mAP_present_only": 0.060554709992767,
                "mAP_freq_weighted": 0.27401379913953483,
                "exact_match": 0.11165903487143361,
                "hamming_accuracy": 0.9856332391537326,
                "precision": 0.4928166195768663,
                "recall": 0.5,
                "f1": 0.49638232263567705,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.044689812794925766,
                "exact_match": 0.007396970764353646,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.07624039693822657,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.060554709992767,
                "exact_match": 0.11165903487143361,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID06": {
          "video_id": "VID06",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.06429867800614814,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.6578004708080908,
                "mAP_present_only_with_null_verb": 0.059334902693636296,
                "mAP_freq_weighted_with_null_verb": 0.3452503682697036,
                "mAP_sample_wise_with_null_verb": 0.08346279853800055,
                "mAP_standard_all_actions": 0.6578004708080908,
                "mAP_present_only_all_actions": 0.059334902693636296,
                "mAP_freq_weighted_all_actions": 0.3452503682697036,
                "mAP_sample_wise_all_actions": 0.08346279853800055,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9410404087320019,
                "precision_with_null_verb": 0.5043173336627731,
                "recall_with_null_verb": 0.5105745387380385,
                "f1_with_null_verb": 0.5031587039515601,
                "num_predictions": 2153,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "30",
                "action_sparsity_with_null_verb": 0.7,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6773592088102112,
                "mAP_present_only": 0.06429867800614814,
                "mAP_freq_weighted": 0.3688606328036313,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9386358470614976,
                "precision": 0.5044600279552434,
                "recall": 0.5117518442099303,
                "f1": 0.5029091480676292,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.09697334680797468,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.4575285095348158,
                "mAP_present_only_with_null_verb": 0.09176169844938589,
                "mAP_freq_weighted_with_null_verb": 0.4775986242438959,
                "mAP_sample_wise_with_null_verb": 0.05925923535692792,
                "mAP_standard_all_actions": 0.4575285095348158,
                "mAP_present_only_all_actions": 0.09176169844938589,
                "mAP_freq_weighted_all_actions": 0.4775986242438959,
                "mAP_sample_wise_all_actions": 0.05925923535692792,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8199117510450534,
                "precision_with_null_verb": 0.50477418128802,
                "recall_with_null_verb": 0.5396425172955988,
                "f1_with_null_verb": 0.4732675172458723,
                "num_predictions": 2153,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "30",
                "action_sparsity_with_null_verb": 0.7,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.47363092571284404,
                "mAP_present_only": 0.09697334680797468,
                "mAP_freq_weighted": 0.5079105158209819,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8222223320255754,
                "precision": 0.5059989124146039,
                "recall": 0.550160986825202,
                "f1": 0.47559980365934806,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.07026840883169194,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.7195992001856664,
                "mAP_present_only_with_null_verb": 0.06533066728555478,
                "mAP_freq_weighted_with_null_verb": 0.36896946827441457,
                "mAP_sample_wise_with_null_verb": 0.1064334699336702,
                "mAP_standard_all_actions": 0.7195992001856664,
                "mAP_present_only_all_actions": 0.06533066728555478,
                "mAP_freq_weighted_all_actions": 0.36896946827441457,
                "mAP_sample_wise_all_actions": 0.1064334699336702,
                "exact_match_with_null_verb": 0.030190431955411056,
                "hamming_accuracy_with_null_verb": 0.9825963771481654,
                "precision_with_null_verb": 0.4912981885740827,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4956109011767517,
                "num_predictions": 2153,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "30",
                "action_sparsity_with_null_verb": 0.7,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7428401981874893,
                "mAP_present_only": 0.07026840883169194,
                "mAP_freq_weighted": 0.39352942569724325,
                "exact_match": 0.04412447747329308,
                "hamming_accuracy": 0.9828443240999694,
                "precision": 0.4914221620499847,
                "recall": 0.5,
                "f1": 0.4956739730669936,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.06429867800614814,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.09697334680797468,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.07026840883169194,
                "exact_match": 0.04412447747329308,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID111": {
          "video_id": "VID111",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.04212182266694432,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.6721936460130383,
                "mAP_present_only_with_null_verb": 0.042047055217373876,
                "mAP_freq_weighted_with_null_verb": 0.13418625420709623,
                "mAP_sample_wise_with_null_verb": 0.09791341482963635,
                "mAP_standard_all_actions": 0.6721936460130383,
                "mAP_present_only_all_actions": 0.042047055217373876,
                "mAP_freq_weighted_all_actions": 0.13418625420709623,
                "mAP_sample_wise_all_actions": 0.09791341482963635,
                "exact_match_with_null_verb": 0.008391608391608392,
                "hamming_accuracy_with_null_verb": 0.9653146853146853,
                "precision_with_null_verb": 0.5134767431690895,
                "recall_with_null_verb": 0.5264117531287953,
                "f1_with_null_verb": 0.5167984269319923,
                "num_predictions": 2145,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6920536762412085,
                "mAP_present_only": 0.04212182266694432,
                "mAP_freq_weighted": 0.1421603729715982,
                "exact_match": 0.008391608391608392,
                "hamming_accuracy": 0.965139116202946,
                "precision": 0.5141430639593155,
                "recall": 0.5327621379303058,
                "f1": 0.5181700153726418,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08276510832373084,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.4826031666158103,
                "mAP_present_only_with_null_verb": 0.07794195384762173,
                "mAP_freq_weighted_with_null_verb": 0.21669706153819024,
                "mAP_sample_wise_with_null_verb": 0.03352374356038054,
                "mAP_standard_all_actions": 0.4826031666158103,
                "mAP_present_only_all_actions": 0.07794195384762173,
                "mAP_freq_weighted_all_actions": 0.21669706153819024,
                "mAP_sample_wise_all_actions": 0.03352374356038054,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8665361305361305,
                "precision_with_null_verb": 0.49493723206258117,
                "recall_with_null_verb": 0.4551772966739977,
                "f1_with_null_verb": 0.4672876572940029,
                "num_predictions": 2145,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5007354011499284,
                "mAP_present_only": 0.08276510832373084,
                "mAP_freq_weighted": 0.23882163453644087,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8717651143183058,
                "precision": 0.4957666501272654,
                "recall": 0.45946243971658984,
                "f1": 0.46895746558533635,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08714195956862358,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.7341492948046736,
                "mAP_present_only_with_null_verb": 0.0832734303609434,
                "mAP_freq_weighted_with_null_verb": 0.16516050198275062,
                "mAP_sample_wise_with_null_verb": 0.08244234273081831,
                "mAP_standard_all_actions": 0.7341492948046736,
                "mAP_present_only_all_actions": 0.0832734303609434,
                "mAP_freq_weighted_all_actions": 0.16516050198275062,
                "mAP_sample_wise_all_actions": 0.08244234273081831,
                "exact_match_with_null_verb": 0.24615384615384617,
                "hamming_accuracy_with_null_verb": 0.9877482517482518,
                "precision_with_null_verb": 0.4938741258741259,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4969181841209085,
                "num_predictions": 2145,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7572186062682509,
                "mAP_present_only": 0.08714195956862358,
                "mAP_freq_weighted": 0.17132116382549592,
                "exact_match": 0.29976689976689974,
                "hamming_accuracy": 0.9890046124088677,
                "precision": 0.49450230620443386,
                "recall": 0.5,
                "f1": 0.49723595724149283,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.04212182266694432,
                "exact_match": 0.008391608391608392,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08276510832373084,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08714195956862358,
                "exact_match": 0.29976689976689974,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID14": {
          "video_id": "VID14",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.04086861675676502,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "36",
                  "subset_action_sparsity": 0.6170212765957447
                },
                "mAP_standard_with_null_verb": 0.5555629614981311,
                "mAP_present_only_with_null_verb": 0.03795844267836877,
                "mAP_freq_weighted_with_null_verb": 0.1580249948946416,
                "mAP_sample_wise_with_null_verb": 0.06783655652651858,
                "mAP_standard_all_actions": 0.5555629614981311,
                "mAP_present_only_all_actions": 0.03795844267836877,
                "mAP_freq_weighted_all_actions": 0.1580249948946416,
                "mAP_sample_wise_all_actions": 0.06783655652651858,
                "exact_match_with_null_verb": 0.000585480093676815,
                "hamming_accuracy_with_null_verb": 0.9608196721311475,
                "precision_with_null_verb": 0.5084308363661646,
                "recall_with_null_verb": 0.5136500473823403,
                "f1_with_null_verb": 0.5098057783179831,
                "num_predictions": 1708,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "41",
                "action_sparsity_with_null_verb": 0.5900000000000001,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5794815979068462,
                "mAP_present_only": 0.04086861675676502,
                "mAP_freq_weighted": 0.16642589698288462,
                "exact_match": 0.000585480093676815,
                "hamming_accuracy": 0.9593340475360008,
                "precision": 0.5084587909977725,
                "recall": 0.5145704809596184,
                "f1": 0.5098926000814674,
                "num_actions_total": 94,
                "num_actions_present": "36",
                "action_sparsity": 0.6170212765957447
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.06665249146709938,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "36",
                  "subset_action_sparsity": 0.6170212765957447
                },
                "mAP_standard_with_null_verb": 0.375325386098271,
                "mAP_present_only_with_null_verb": 0.061769234386026765,
                "mAP_freq_weighted_with_null_verb": 0.31121975990769923,
                "mAP_sample_wise_with_null_verb": 0.051835843108589914,
                "mAP_standard_all_actions": 0.375325386098271,
                "mAP_present_only_all_actions": 0.061769234386026765,
                "mAP_freq_weighted_all_actions": 0.31121975990769923,
                "mAP_sample_wise_all_actions": 0.051835843108589914,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8434543325526932,
                "precision_with_null_verb": 0.5026052597461196,
                "recall_with_null_verb": 0.521425182991415,
                "f1_with_null_verb": 0.47543065968023734,
                "num_predictions": 1708,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "41",
                "action_sparsity_with_null_verb": 0.5900000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.3978669116256977,
                "mAP_present_only": 0.06665249146709938,
                "mAP_freq_weighted": 0.3280707736725746,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8478063182022024,
                "precision": 0.5030337556501256,
                "recall": 0.5243848248937754,
                "f1": 0.47732198762413586,
                "num_actions_total": 94,
                "num_actions_present": "36",
                "action_sparsity": 0.6170212765957447
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.05375059295095757,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "36",
                  "subset_action_sparsity": 0.6170212765957447
                },
                "mAP_standard_with_null_verb": 0.6109704877803622,
                "mAP_present_only_with_null_verb": 0.051147531171615086,
                "mAP_freq_weighted_with_null_verb": 0.18486066307259943,
                "mAP_sample_wise_with_null_verb": 0.0693102806231522,
                "mAP_standard_all_actions": 0.6109704877803622,
                "mAP_present_only_all_actions": 0.051147531171615086,
                "mAP_freq_weighted_all_actions": 0.18486066307259943,
                "mAP_sample_wise_all_actions": 0.0693102806231522,
                "exact_match_with_null_verb": 0.13056206088992975,
                "hamming_accuracy_with_null_verb": 0.984519906323185,
                "precision_with_null_verb": 0.4922599531615925,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4960997887631433,
                "num_predictions": 1708,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "41",
                "action_sparsity_with_null_verb": 0.5900000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.6376066100663242,
                "mAP_present_only": 0.05375059295095757,
                "mAP_freq_weighted": 0.19273040704195846,
                "exact_match": 0.14227166276346603,
                "hamming_accuracy": 0.984547062633913,
                "precision": 0.4922735313169565,
                "recall": 0.5,
                "f1": 0.4961066840749098,
                "num_actions_total": 94,
                "num_actions_present": "36",
                "action_sparsity": 0.6170212765957447
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.04086861675676502,
                "exact_match": 0.000585480093676815,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.06665249146709938,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.05375059295095757,
                "exact_match": 0.14227166276346603,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID23": {
          "video_id": "VID23",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.05341830238449025,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.6648323830248438,
                "mAP_present_only_with_null_verb": 0.04784639685433453,
                "mAP_freq_weighted_with_null_verb": 0.15364653918017585,
                "mAP_sample_wise_with_null_verb": 0.06664900846146943,
                "mAP_standard_all_actions": 0.6648323830248438,
                "mAP_present_only_all_actions": 0.04784639685433453,
                "mAP_freq_weighted_all_actions": 0.15364653918017585,
                "mAP_sample_wise_all_actions": 0.06664900846146943,
                "exact_match_with_null_verb": 0.0030581039755351682,
                "hamming_accuracy_with_null_verb": 0.9528012232415902,
                "precision_with_null_verb": 0.5029095323248616,
                "recall_with_null_verb": 0.5059177326190544,
                "f1_with_null_verb": 0.5023772675070782,
                "num_predictions": 1635,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6956263389574122,
                "mAP_present_only": 0.05341830238449025,
                "mAP_freq_weighted": 0.1634425311885644,
                "exact_match": 0.004892966360856269,
                "hamming_accuracy": 0.951233001496519,
                "precision": 0.5031405368043521,
                "recall": 0.5069684830575901,
                "f1": 0.5023806827690032,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08771737683567159,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.45513622678753896,
                "mAP_present_only_with_null_verb": 0.08108460254044815,
                "mAP_freq_weighted_with_null_verb": 0.2540988924189111,
                "mAP_sample_wise_with_null_verb": 0.048387951810778936,
                "mAP_standard_all_actions": 0.45513622678753896,
                "mAP_present_only_all_actions": 0.08108460254044815,
                "mAP_freq_weighted_all_actions": 0.2540988924189111,
                "mAP_sample_wise_all_actions": 0.048387951810778936,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8565015290519877,
                "precision_with_null_verb": 0.5004048925233746,
                "recall_with_null_verb": 0.5029773949372977,
                "f1_with_null_verb": 0.4760280374987829,
                "num_predictions": 1635,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.48170906167795174,
                "mAP_present_only": 0.08771737683567159,
                "mAP_freq_weighted": 0.26505291355543437,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8600169171709285,
                "precision": 0.5014220825525538,
                "recall": 0.5105511174290246,
                "f1": 0.4782505109476357,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08391680950274773,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.7132876234295037,
                "mAP_present_only_with_null_verb": 0.07512136590162527,
                "mAP_freq_weighted_with_null_verb": 0.21867968519086356,
                "mAP_sample_wise_with_null_verb": 0.06190815369763992,
                "mAP_standard_all_actions": 0.7132876234295037,
                "mAP_present_only_all_actions": 0.07512136590162527,
                "mAP_freq_weighted_all_actions": 0.21867968519086356,
                "mAP_sample_wise_all_actions": 0.06190815369763992,
                "exact_match_with_null_verb": 0.0581039755351682,
                "hamming_accuracy_with_null_verb": 0.9841651376146789,
                "precision_with_null_verb": 0.49208256880733947,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4960096914099707,
                "num_predictions": 1635,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7466152877348027,
                "mAP_present_only": 0.08391680950274773,
                "mAP_freq_weighted": 0.23138939239260017,
                "exact_match": 0.08440366972477065,
                "hamming_accuracy": 0.9845988678508686,
                "precision": 0.4922994339254343,
                "recall": 0.5,
                "f1": 0.4961198375151223,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.05341830238449025,
                "exact_match": 0.004892966360856269,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08771737683567159,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08391680950274773,
                "exact_match": 0.08440366972477065,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID25": {
          "video_id": "VID25",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.069889253259933,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7057739699100557,
                "mAP_present_only_with_null_verb": 0.06066911503867599,
                "mAP_freq_weighted_with_null_verb": 0.24143074767921754,
                "mAP_sample_wise_with_null_verb": 0.06008926445502213,
                "mAP_standard_all_actions": 0.7057739699100557,
                "mAP_present_only_all_actions": 0.06066911503867599,
                "mAP_freq_weighted_all_actions": 0.24143074767921754,
                "mAP_sample_wise_all_actions": 0.06008926445502213,
                "exact_match_with_null_verb": 0.0004697040864255519,
                "hamming_accuracy_with_null_verb": 0.9506528886801315,
                "precision_with_null_verb": 0.5038525164217471,
                "recall_with_null_verb": 0.5078588867613734,
                "f1_with_null_verb": 0.5035496460195417,
                "num_predictions": 2129,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7390178118984957,
                "mAP_present_only": 0.069889253259933,
                "mAP_freq_weighted": 0.2558557968410284,
                "exact_match": 0.0004697040864255519,
                "hamming_accuracy": 0.9486673395760671,
                "precision": 0.5039165962611747,
                "recall": 0.5085341816191361,
                "f1": 0.5033815845894241,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.12662597313173377,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.48867119308940316,
                "mAP_present_only_with_null_verb": 0.11027381957462755,
                "mAP_freq_weighted_with_null_verb": 0.4037782933048653,
                "mAP_sample_wise_with_null_verb": 0.04566876766303015,
                "mAP_standard_all_actions": 0.48867119308940316,
                "mAP_present_only_all_actions": 0.11027381957462755,
                "mAP_freq_weighted_all_actions": 0.4037782933048653,
                "mAP_sample_wise_all_actions": 0.04566876766303015,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8459840300610615,
                "precision_with_null_verb": 0.5011592652051736,
                "recall_with_null_verb": 0.5087046835622849,
                "f1_with_null_verb": 0.4747917656808848,
                "num_predictions": 2129,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5070121854868767,
                "mAP_present_only": 0.12662597313173377,
                "mAP_freq_weighted": 0.4281305545492316,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8476160019187912,
                "precision": 0.501570738168461,
                "recall": 0.5117716638960721,
                "f1": 0.47580350423235174,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.09597494375464491,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7619085223487247,
                "mAP_present_only_with_null_verb": 0.08426354749509549,
                "mAP_freq_weighted_with_null_verb": 0.298501302527214,
                "mAP_sample_wise_with_null_verb": 0.08142547136671649,
                "mAP_standard_all_actions": 0.7619085223487247,
                "mAP_present_only_all_actions": 0.08426354749509549,
                "mAP_freq_weighted_all_actions": 0.298501302527214,
                "mAP_sample_wise_all_actions": 0.08142547136671649,
                "exact_match_with_null_verb": 0.04744011272898074,
                "hamming_accuracy_with_null_verb": 0.9834288398309066,
                "precision_with_null_verb": 0.4917144199154533,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.495822597756896,
                "num_predictions": 2129,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7980369555196548,
                "mAP_present_only": 0.09597494375464491,
                "mAP_freq_weighted": 0.3154716267540064,
                "exact_match": 0.07045561296383279,
                "hamming_accuracy": 0.9835353727151894,
                "precision": 0.4917676863575947,
                "recall": 0.5,
                "f1": 0.4958496764133143,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.069889253259933,
                "exact_match": 0.0004697040864255519,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.12662597313173377,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.09597494375464491,
                "exact_match": 0.07045561296383279,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID50": {
          "video_id": "VID50",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09174980456039987,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.7755283477544015,
                "mAP_present_only_with_null_verb": 0.08626859863556313,
                "mAP_freq_weighted_with_null_verb": 0.3706098060799097,
                "mAP_sample_wise_with_null_verb": 0.05460098406300339,
                "mAP_standard_all_actions": 0.7755283477544015,
                "mAP_present_only_all_actions": 0.08626859863556313,
                "mAP_freq_weighted_all_actions": 0.3706098060799097,
                "mAP_sample_wise_all_actions": 0.05460098406300339,
                "exact_match_with_null_verb": 0.0009140767824497258,
                "hamming_accuracy_with_null_verb": 0.9407404021937843,
                "precision_with_null_verb": 0.498563674823963,
                "recall_with_null_verb": 0.4962654894671623,
                "f1_with_null_verb": 0.4947005610611157,
                "num_predictions": 1094,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7815744348187916,
                "mAP_present_only": 0.09174980456039987,
                "mAP_freq_weighted": 0.38809350694970657,
                "exact_match": 0.0009140767824497258,
                "hamming_accuracy": 0.9379108483410479,
                "precision": 0.49850843202554884,
                "recall": 0.4959102032074954,
                "f1": 0.49409713150227885,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.1650005608209441,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.5474513276939618,
                "mAP_present_only_with_null_verb": 0.15250737607756498,
                "mAP_freq_weighted_with_null_verb": 0.5227978721605843,
                "mAP_sample_wise_with_null_verb": 0.07059525696398732,
                "mAP_standard_all_actions": 0.5474513276939618,
                "mAP_present_only_all_actions": 0.15250737607756498,
                "mAP_freq_weighted_all_actions": 0.5227978721605843,
                "mAP_sample_wise_all_actions": 0.07059525696398732,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8369012797074954,
                "precision_with_null_verb": 0.5110812084043708,
                "recall_with_null_verb": 0.5909680916976456,
                "f1_with_null_verb": 0.4871382292927287,
                "num_predictions": 1094,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5706383933312246,
                "mAP_present_only": 0.1650005608209441,
                "mAP_freq_weighted": 0.5488775941601599,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8368567427749037,
                "precision": 0.5122721210499516,
                "recall": 0.6004999951548268,
                "f1": 0.48899220901804774,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.12048357087696708,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.8403714178160929,
                "mAP_present_only_with_null_verb": 0.11317454342273797,
                "mAP_freq_weighted_with_null_verb": 0.469081783168598,
                "mAP_sample_wise_with_null_verb": 0.07904644951829105,
                "mAP_standard_all_actions": 0.8403714178160929,
                "mAP_present_only_all_actions": 0.11317454342273797,
                "mAP_freq_weighted_all_actions": 0.469081783168598,
                "mAP_sample_wise_all_actions": 0.07904644951829105,
                "exact_match_with_null_verb": 0.05758683729433273,
                "hamming_accuracy_with_null_verb": 0.9835466179159049,
                "precision_with_null_verb": 0.49177330895795246,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49585253456221196,
                "num_predictions": 1094,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8502950758939518,
                "mAP_present_only": 0.12048357087696708,
                "mAP_freq_weighted": 0.49123365999636126,
                "exact_match": 0.07221206581352833,
                "hamming_accuracy": 0.983449375705006,
                "precision": 0.491724687852503,
                "recall": 0.5,
                "f1": 0.49582781781634555,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09174980456039987,
                "exact_match": 0.0009140767824497258,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.1650005608209441,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.12048357087696708,
                "exact_match": 0.07221206581352833,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID51": {
          "video_id": "VID51",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.05955519963372572,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.68559106398051,
                "mAP_present_only_with_null_verb": 0.05376228958796541,
                "mAP_freq_weighted_with_null_verb": 0.24185978958213983,
                "mAP_sample_wise_with_null_verb": 0.04533356654813689,
                "mAP_standard_all_actions": 0.68559106398051,
                "mAP_present_only_all_actions": 0.05376228958796541,
                "mAP_freq_weighted_all_actions": 0.24185978958213983,
                "mAP_sample_wise_all_actions": 0.04533356654813689,
                "exact_match_with_null_verb": 0.00033967391304347825,
                "hamming_accuracy_with_null_verb": 0.9617866847826086,
                "precision_with_null_verb": 0.503836981787008,
                "recall_with_null_verb": 0.5060647292303289,
                "f1_with_null_verb": 0.5041680436682605,
                "num_predictions": 2944,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7173332424596746,
                "mAP_present_only": 0.05955519963372572,
                "mAP_freq_weighted": 0.264163348675909,
                "exact_match": 0.0010190217391304348,
                "hamming_accuracy": 0.9609049780296022,
                "precision": 0.5041461822153359,
                "recall": 0.5072388702029018,
                "f1": 0.5044765906236064,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.07829152902269541,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.4608071630383928,
                "mAP_present_only_with_null_verb": 0.07174883806342329,
                "mAP_freq_weighted_with_null_verb": 0.34828715541715544,
                "mAP_sample_wise_with_null_verb": 0.053606869628706866,
                "mAP_standard_all_actions": 0.4608071630383928,
                "mAP_present_only_all_actions": 0.07174883806342329,
                "mAP_freq_weighted_all_actions": 0.34828715541715544,
                "mAP_sample_wise_all_actions": 0.053606869628706866,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8277105978260869,
                "precision_with_null_verb": 0.5044962252535496,
                "recall_with_null_verb": 0.541448263295896,
                "f1_with_null_verb": 0.4734233662588672,
                "num_predictions": 2944,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.4880744329419648,
                "mAP_present_only": 0.07829152902269541,
                "mAP_freq_weighted": 0.3800060511802471,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8324251271970398,
                "precision": 0.5046056528960154,
                "recall": 0.5431902282100891,
                "f1": 0.474530571125154,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08236514034403992,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.731166176126709,
                "mAP_present_only_with_null_verb": 0.07298681423003071,
                "mAP_freq_weighted_with_null_verb": 0.3034488108611098,
                "mAP_sample_wise_with_null_verb": 0.06556801885670335,
                "mAP_standard_all_actions": 0.731166176126709,
                "mAP_present_only_all_actions": 0.07298681423003071,
                "mAP_freq_weighted_all_actions": 0.3034488108611098,
                "mAP_sample_wise_all_actions": 0.06556801885670335,
                "exact_match_with_null_verb": 0.08186141304347826,
                "hamming_accuracy_with_null_verb": 0.9848539402173913,
                "precision_with_null_verb": 0.4924269701086956,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49618459084678296,
                "num_predictions": 2944,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7657102485984784,
                "mAP_present_only": 0.08236514034403992,
                "mAP_freq_weighted": 0.3319272287091587,
                "exact_match": 0.09612771739130435,
                "hamming_accuracy": 0.9854446114708603,
                "precision": 0.49272230573543013,
                "recall": 0.5,
                "f1": 0.4963344763069576,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.05955519963372572,
                "exact_match": 0.0010190217391304348,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.07829152902269541,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08236514034403992,
                "exact_match": 0.09612771739130435,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID66": {
          "video_id": "VID66",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.07635709587926623,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.7256946207753294,
                "mAP_present_only_with_null_verb": 0.06823748163186703,
                "mAP_freq_weighted_with_null_verb": 0.29310078083667107,
                "mAP_sample_wise_with_null_verb": 0.04910201719891055,
                "mAP_standard_all_actions": 0.7256946207753294,
                "mAP_present_only_all_actions": 0.06823748163186703,
                "mAP_freq_weighted_all_actions": 0.29310078083667107,
                "mAP_sample_wise_all_actions": 0.04910201719891055,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.952077850877193,
                "precision_with_null_verb": 0.5088268935168001,
                "recall_with_null_verb": 0.5187788571320844,
                "f1_with_null_verb": 0.5102174113872375,
                "num_predictions": 1824,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7396504459317588,
                "mAP_present_only": 0.07635709587926623,
                "mAP_freq_weighted": 0.30216543562060383,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9495497387084733,
                "precision": 0.508597751062319,
                "recall": 0.5188525877377494,
                "f1": 0.509777368803801,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.1219469648197334,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.5051164697586303,
                "mAP_present_only_with_null_verb": 0.10920204242882735,
                "mAP_freq_weighted_with_null_verb": 0.47373708084881305,
                "mAP_sample_wise_with_null_verb": 0.058040969698002785,
                "mAP_standard_all_actions": 0.5051164697586303,
                "mAP_present_only_all_actions": 0.10920204242882735,
                "mAP_freq_weighted_all_actions": 0.47373708084881305,
                "mAP_sample_wise_all_actions": 0.058040969698002785,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8580427631578947,
                "precision_with_null_verb": 0.5060203738657396,
                "recall_with_null_verb": 0.5445677122861979,
                "f1_with_null_verb": 0.4851375292482624,
                "num_predictions": 1824,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5259461627276029,
                "mAP_present_only": 0.1219469648197334,
                "mAP_freq_weighted": 0.48839096946986343,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8592525195968646,
                "precision": 0.5063455004854992,
                "recall": 0.5451818484752752,
                "f1": 0.4863262536955901,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.12486669876523451,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.7961076155508251,
                "mAP_present_only_with_null_verb": 0.11351137196010891,
                "mAP_freq_weighted_with_null_verb": 0.39861226123658866,
                "mAP_sample_wise_with_null_verb": 0.0517324335102104,
                "mAP_standard_all_actions": 0.7961076155508251,
                "mAP_present_only_all_actions": 0.11351137196010891,
                "mAP_freq_weighted_all_actions": 0.39861226123658866,
                "mAP_sample_wise_all_actions": 0.0517324335102104,
                "exact_match_with_null_verb": 0.06578947368421052,
                "hamming_accuracy_with_null_verb": 0.9841611842105263,
                "precision_with_null_verb": 0.49208059210526317,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49600868721868085,
                "num_predictions": 1824,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8138014252691987,
                "mAP_present_only": 0.12486669876523451,
                "mAP_freq_weighted": 0.4105461637004417,
                "exact_match": 0.07291666666666667,
                "hamming_accuracy": 0.9836809443822322,
                "precision": 0.4918404721911161,
                "recall": 0.5,
                "f1": 0.49588667329189623,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.07635709587926623,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.1219469648197334,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.12486669876523451,
                "exact_match": 0.07291666666666667,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID79": {
          "video_id": "VID79",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.047718657470800785,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "31",
                  "subset_action_sparsity": 0.6702127659574468
                },
                "mAP_standard_with_null_verb": 0.6056109582913712,
                "mAP_present_only_with_null_verb": 0.04336377303158675,
                "mAP_freq_weighted_with_null_verb": 0.23168072854570282,
                "mAP_sample_wise_with_null_verb": 0.05593640425432918,
                "mAP_standard_all_actions": 0.6056109582913712,
                "mAP_present_only_all_actions": 0.04336377303158675,
                "mAP_freq_weighted_all_actions": 0.23168072854570282,
                "mAP_sample_wise_all_actions": 0.05593640425432918,
                "exact_match_with_null_verb": 0.0005858230814294083,
                "hamming_accuracy_with_null_verb": 0.950178676039836,
                "precision_with_null_verb": 0.5072662827118622,
                "recall_with_null_verb": 0.5151380870251057,
                "f1_with_null_verb": 0.5080585037683324,
                "num_predictions": 3414,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "36",
                "action_sparsity_with_null_verb": 0.64,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6327582806552641,
                "mAP_present_only": 0.047718657470800785,
                "mAP_freq_weighted": 0.2431383627328123,
                "exact_match": 0.002050380785002929,
                "hamming_accuracy": 0.9479178351967493,
                "precision": 0.5072089761769829,
                "recall": 0.5158063955560487,
                "f1": 0.5078140588476869,
                "num_actions_total": 94,
                "num_actions_present": "31",
                "action_sparsity": 0.6702127659574468
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08724713515959996,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "31",
                  "subset_action_sparsity": 0.6702127659574468
                },
                "mAP_standard_with_null_verb": 0.4289279476564831,
                "mAP_present_only_with_null_verb": 0.08035541015689751,
                "mAP_freq_weighted_with_null_verb": 0.37672072424731784,
                "mAP_sample_wise_with_null_verb": 0.051174926884649086,
                "mAP_standard_all_actions": 0.4289279476564831,
                "mAP_present_only_all_actions": 0.08035541015689751,
                "mAP_freq_weighted_all_actions": 0.37672072424731784,
                "mAP_sample_wise_all_actions": 0.051174926884649086,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8493966022261277,
                "precision_with_null_verb": 0.5038877218503147,
                "recall_with_null_verb": 0.5286885572043157,
                "f1_with_null_verb": 0.4799941146123368,
                "num_predictions": 3414,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "36",
                "action_sparsity_with_null_verb": 0.64,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.4543049062760383,
                "mAP_present_only": 0.08724713515959996,
                "mAP_freq_weighted": 0.39415911914994023,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8501882112453103,
                "precision": 0.5045976905615166,
                "recall": 0.5335273220476293,
                "f1": 0.4814125489759673,
                "num_actions_total": 94,
                "num_actions_present": "31",
                "action_sparsity": 0.6702127659574468
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.061298381561645895,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "31",
                  "subset_action_sparsity": 0.6702127659574468
                },
                "mAP_standard_with_null_verb": 0.6604987316623614,
                "mAP_present_only_with_null_verb": 0.05694092128433722,
                "mAP_freq_weighted_with_null_verb": 0.3045836700991395,
                "mAP_sample_wise_with_null_verb": 0.08601377960199752,
                "mAP_standard_all_actions": 0.6604987316623614,
                "mAP_present_only_all_actions": 0.05694092128433722,
                "mAP_freq_weighted_all_actions": 0.3045836700991395,
                "mAP_sample_wise_all_actions": 0.08601377960199752,
                "exact_match_with_null_verb": 0.04979496192149971,
                "hamming_accuracy_with_null_verb": 0.9833567662565905,
                "precision_with_null_verb": 0.49167838312829526,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4958042763595119,
                "num_predictions": 3414,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "36",
                "action_sparsity_with_null_verb": 0.64,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.690428189663947,
                "mAP_present_only": 0.061298381561645895,
                "mAP_freq_weighted": 0.3184290102565793,
                "exact_match": 0.06854130052724078,
                "hamming_accuracy": 0.9832136758528711,
                "precision": 0.49160683792643556,
                "recall": 0.5,
                "f1": 0.4957678982472955,
                "num_actions_total": 94,
                "num_actions_present": "31",
                "action_sparsity": 0.6702127659574468
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.047718657470800785,
                "exact_match": 0.002050380785002929,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08724713515959996,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.061298381561645895,
                "exact_match": 0.06854130052724078,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        }
      },
      "aggregate_results": {
        "single_step_comparison": {
          "WorldModelRL_world_model_ppo": {
            "mean_mAP": 0.0590667243413399,
            "std_mAP": 0.015803668720615976,
            "mean_exact_match": 0.0025720209003503762,
            "std_exact_match": 0.0029927225811680965,
            "num_videos": 10,
            "evaluation_type": "single_step_fair_comparison"
          },
          "DirectVideoRL_a2c": {
            "mean_mAP": 0.08406212161493201,
            "std_mAP": 0.02307743187973928,
            "mean_exact_match": 0.10624791079624359,
            "std_exact_match": 0.06941742420543016,
            "num_videos": 10,
            "evaluation_type": "single_step_fair_comparison"
          },
          "DirectVideoRL_ppo": {
            "mean_mAP": 0.09894608833274096,
            "std_mAP": 0.028586903659321268,
            "mean_exact_match": 0.0,
            "std_exact_match": 0.0,
            "num_videos": 10,
            "evaluation_type": "single_step_fair_comparison"
          }
        },
        "planning_analysis": {},
        "method_rankings": {
          "single_step_ranking": [
            [
              "DirectVideoRL_ppo",
              0.09894608833274096
            ],
            [
              "DirectVideoRL_a2c",
              0.08406212161493201
            ],
            [
              "WorldModelRL_world_model_ppo",
              0.0590667243413399
            ]
          ],
          "planning_ranking": []
        }
      },
      "statistical_tests": {
        "WorldModelRL_world_model_ppo_vs_DirectVideoRL_a2c": {
          "t_statistic": -2.6809457451727607,
          "p_value": 0.015256155021883488,
          "cohens_d": -1.2638099442699204,
          "significant": "True",
          "mean_diff": -0.02499539727359211,
          "method1_mean": 0.0590667243413399,
          "method2_mean": 0.08406212161493201
        },
        "WorldModelRL_world_model_ppo_vs_DirectVideoRL_ppo": {
          "t_statistic": -3.6626378847063097,
          "p_value": 0.001780821378377363,
          "cohens_d": -1.7265840568710558,
          "significant": "True",
          "mean_diff": -0.03987936399140106,
          "method1_mean": 0.0590667243413399,
          "method2_mean": 0.09894608833274096
        },
        "DirectVideoRL_a2c_vs_DirectVideoRL_ppo": {
          "t_statistic": -1.2153703126230435,
          "p_value": 0.23992619774260884,
          "cohens_d": -0.5729310598057122,
          "significant": "False",
          "mean_diff": -0.01488396671780895,
          "method1_mean": 0.08406212161493201,
          "method2_mean": 0.09894608833274096
        }
      },
      "evaluation_design": {
        "data_handling": "uses_dataloader_batches_like_training",
        "temporal_structure": "maintained_properly",
        "model_interfaces": "consistent_with_training",
        "primary_evaluation": "single_step_action_prediction_with_proper_context",
        "secondary_evaluation": "multi_step_planning_analysis",
        "fairness_approach": "respects_training_paradigms_and_data_structure"
      },
      "timestamp": "2025-06-14 12:36:36.616353"
    },
    "file_paths": {
      "evaluation": "results/fixed_rl_2025-06-14_11-41-53/integrated_evaluation/evaluation_results.json",
      "fair_comparison": "results/fixed_rl_2025-06-14_11-41-53/integrated_evaluation/fair_single_step_comparison.csv",
      "planning_analysis": "results/fixed_rl_2025-06-14_11-41-53/integrated_evaluation/planning_capability_analysis.csv"
    }
  }
}