{
    "VID02": {
        "video_id": "VID02",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.24822976369628708,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6926205911221598,
                    "mAP_present_only_with_null_verb": 0.213589973888705,
                    "mAP_freq_weighted_with_null_verb": 0.5743368635728457,
                    "mAP_sample_wise_with_null_verb": 0.6065914048322866,
                    "mAP_standard_all_actions": 0.6926205911221598,
                    "mAP_present_only_all_actions": 0.213589973888705,
                    "mAP_freq_weighted_all_actions": 0.5743368635728457,
                    "mAP_sample_wise_all_actions": 0.6065914048322866,
                    "exact_match_with_null_verb": 0.3370905248326876,
                    "hamming_accuracy_with_null_verb": 0.9881331454737584,
                    "precision_with_null_verb": 0.8031537445388199,
                    "recall_with_null_verb": 0.7305014108402186,
                    "f1_with_null_verb": 0.7616971774664453,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7335152487605962,
                    "mAP_present_only": 0.24822976369628708,
                    "mAP_freq_weighted": 0.6043901545151827,
                    "exact_match": 0.37090524832687566,
                    "hamming_accuracy": 0.9885410655534987,
                    "precision": 0.8113527295660194,
                    "recall": 0.7429476370520174,
                    "f1": 0.7727730918313338,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.24822976369628708,
                    "exact_match": 0.37090524832687566,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID06": {
        "video_id": "VID06",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.23971396673113773,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.7462125105822455,
                    "mAP_present_only_with_null_verb": 0.22070836860748472,
                    "mAP_freq_weighted_with_null_verb": 0.6156014782231056,
                    "mAP_sample_wise_with_null_verb": 0.689158472210406,
                    "mAP_standard_all_actions": 0.7462125105822455,
                    "mAP_present_only_all_actions": 0.22070836860748472,
                    "mAP_freq_weighted_all_actions": 0.6156014782231056,
                    "mAP_sample_wise_all_actions": 0.689158472210406,
                    "exact_match_with_null_verb": 0.2289828146771946,
                    "hamming_accuracy_with_null_verb": 0.9867905248490478,
                    "precision_with_null_verb": 0.8182881206522639,
                    "recall_with_null_verb": 0.7657292826299283,
                    "f1_with_null_verb": 0.7895366349122528,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7790698205852084,
                    "mAP_present_only": 0.23971396673113773,
                    "mAP_freq_weighted": 0.649998902552289,
                    "exact_match": 0.2805387830933581,
                    "hamming_accuracy": 0.9874593590339061,
                    "precision": 0.8227020822606739,
                    "recall": 0.784631351754157,
                    "f1": 0.8024200649496633,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.23971396673113773,
                    "exact_match": 0.2805387830933581,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    }
}