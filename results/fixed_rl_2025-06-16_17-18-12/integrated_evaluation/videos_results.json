{
    "VID01": {
        "video_id": "VID01",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.12165462634416493,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 19,
                        "subset_action_sparsity": 0.7978723404255319
                    },
                    "mAP_standard_with_null_verb": 0.7275486768972118,
                    "mAP_present_only_with_null_verb": 0.1252212586236894,
                    "mAP_freq_weighted_with_null_verb": 0.4048668752997102,
                    "mAP_sample_wise_with_null_verb": 0.07292943212200677,
                    "mAP_standard_all_actions": 0.7275486768972118,
                    "mAP_present_only_all_actions": 0.1252212586236894,
                    "mAP_freq_weighted_all_actions": 0.4048668752997102,
                    "mAP_sample_wise_all_actions": 0.07292943212200677,
                    "exact_match_with_null_verb": 0.0946335833814195,
                    "hamming_accuracy_with_null_verb": 0.9647259088286209,
                    "precision_with_null_verb": 0.5215442918487937,
                    "recall_with_null_verb": 0.5334733974845305,
                    "f1_with_null_verb": 0.5257195929131566,
                    "num_predictions": 1733,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 22,
                    "action_sparsity_with_null_verb": 0.78,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7479940202185016,
                    "mAP_present_only": 0.12165462634416493,
                    "mAP_freq_weighted": 0.4364166777345019,
                    "exact_match": 0.1425274091171379,
                    "hamming_accuracy": 0.9644080490110619,
                    "precision": 0.5220929683925163,
                    "recall": 0.5389926346502936,
                    "f1": 0.5273845350460606,
                    "num_actions_total": 94,
                    "num_actions_present": 19,
                    "action_sparsity": 0.7978723404255319
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.09548576177355439,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 19,
                        "subset_action_sparsity": 0.7978723404255319
                    },
                    "mAP_standard_with_null_verb": 0.32148304410342227,
                    "mAP_present_only_with_null_verb": 0.09765020047010128,
                    "mAP_freq_weighted_with_null_verb": 0.28572974683602226,
                    "mAP_sample_wise_with_null_verb": 0.0951498096027441,
                    "mAP_standard_all_actions": 0.32148304410342227,
                    "mAP_present_only_all_actions": 0.09765020047010128,
                    "mAP_freq_weighted_all_actions": 0.28572974683602226,
                    "mAP_sample_wise_all_actions": 0.0951498096027441,
                    "exact_match_with_null_verb": 0.001154068090017311,
                    "hamming_accuracy_with_null_verb": 0.8210098095787651,
                    "precision_with_null_verb": 0.5079884090755251,
                    "recall_with_null_verb": 0.5789015204418301,
                    "f1_with_null_verb": 0.47636604074963557,
                    "num_predictions": 1733,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 22,
                    "action_sparsity_with_null_verb": 0.78,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.3278109518478461,
                    "mAP_present_only": 0.09548576177355439,
                    "mAP_freq_weighted": 0.30240216449410895,
                    "exact_match": 0.001154068090017311,
                    "hamming_accuracy": 0.8279579133466747,
                    "precision": 0.5062531287332194,
                    "recall": 0.5637622280262677,
                    "f1": 0.47509327613971153,
                    "num_actions_total": 94,
                    "num_actions_present": 19,
                    "action_sparsity": 0.7978723404255319
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.07866318833934587,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 19,
                        "subset_action_sparsity": 0.7978723404255319
                    },
                    "mAP_standard_with_null_verb": 0.7566670762082949,
                    "mAP_present_only_with_null_verb": 0.07575943731043124,
                    "mAP_freq_weighted_with_null_verb": 0.23366826336754343,
                    "mAP_sample_wise_with_null_verb": 0.06424443165327026,
                    "mAP_standard_all_actions": 0.7566670762082949,
                    "mAP_present_only_all_actions": 0.07575943731043124,
                    "mAP_freq_weighted_all_actions": 0.23366826336754343,
                    "mAP_sample_wise_all_actions": 0.06424443165327026,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9562781304096942,
                    "precision_with_null_verb": 0.4923966912682283,
                    "recall_with_null_verb": 0.48530522838685497,
                    "f1_with_null_verb": 0.48882524194523674,
                    "num_predictions": 1733,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 22,
                    "action_sparsity_with_null_verb": 0.78,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7712191550898678,
                    "mAP_present_only": 0.07866318833934587,
                    "mAP_freq_weighted": 0.25144740157915946,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9554087733729482,
                    "precision": 0.4928871829951103,
                    "recall": 0.4843827805995419,
                    "f1": 0.4885979782758837,
                    "num_actions_total": 94,
                    "num_actions_present": 19,
                    "action_sparsity": 0.7978723404255319
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.07713512107355439,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 19,
                        "subset_action_sparsity": 0.7978723404255319
                    },
                    "mAP_standard_with_null_verb": 0.7963416483727721,
                    "mAP_present_only_with_null_verb": 0.07428021987623752,
                    "mAP_freq_weighted_with_null_verb": 0.2652681344494968,
                    "mAP_sample_wise_with_null_verb": 0.024546523588140867,
                    "mAP_standard_all_actions": 0.7963416483727721,
                    "mAP_present_only_all_actions": 0.07428021987623752,
                    "mAP_freq_weighted_all_actions": 0.2652681344494968,
                    "mAP_sample_wise_all_actions": 0.024546523588140867,
                    "exact_match_with_null_verb": 0.1102135025966532,
                    "hamming_accuracy_with_null_verb": 0.9852336987882285,
                    "precision_with_null_verb": 0.49261684939411426,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49628096651271214,
                    "num_predictions": 1733,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 22,
                    "action_sparsity_with_null_verb": 0.78,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8134634819191225,
                    "mAP_present_only": 0.07713512107355439,
                    "mAP_freq_weighted": 0.2875369353731672,
                    "exact_match": 0.18118869013271782,
                    "hamming_accuracy": 0.9862125695203251,
                    "precision": 0.49310628476016255,
                    "recall": 0.5,
                    "f1": 0.49652921578202364,
                    "num_actions_total": 94,
                    "num_actions_present": 19,
                    "action_sparsity": 0.7978723404255319
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.12165462634416493,
                    "exact_match": 0.1425274091171379,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.09548576177355439,
                    "exact_match": 0.001154068090017311,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.07866318833934587,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.07713512107355439,
                    "exact_match": 0.18118869013271782,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID04": {
        "video_id": "VID04",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.10706299302922337,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.743297090128373,
                    "mAP_present_only_with_null_verb": 0.11648545064186511,
                    "mAP_freq_weighted_with_null_verb": 0.37834066928104004,
                    "mAP_sample_wise_with_null_verb": 0.07051354444892399,
                    "mAP_standard_all_actions": 0.743297090128373,
                    "mAP_present_only_all_actions": 0.11648545064186511,
                    "mAP_freq_weighted_all_actions": 0.37834066928104004,
                    "mAP_sample_wise_all_actions": 0.07051354444892399,
                    "exact_match_with_null_verb": 0.1583442838370565,
                    "hamming_accuracy_with_null_verb": 0.9613863337713535,
                    "precision_with_null_verb": 0.5101184840772981,
                    "recall_with_null_verb": 0.5201434781240877,
                    "f1_with_null_verb": 0.5122649777426662,
                    "num_predictions": 1522,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.764043307249966,
                    "mAP_present_only": 0.10706299302922337,
                    "mAP_freq_weighted": 0.3874331625704969,
                    "exact_match": 0.16885676741130093,
                    "hamming_accuracy": 0.9615567422484413,
                    "precision": 0.5110817468960857,
                    "recall": 0.5257806134669486,
                    "f1": 0.5137533462710732,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.10284344580231014,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.3414023583730581,
                    "mAP_present_only_with_null_verb": 0.1070117918652905,
                    "mAP_freq_weighted_with_null_verb": 0.3427797034938753,
                    "mAP_sample_wise_with_null_verb": 0.1671758716920746,
                    "mAP_standard_all_actions": 0.3414023583730581,
                    "mAP_present_only_all_actions": 0.1070117918652905,
                    "mAP_freq_weighted_all_actions": 0.3427797034938753,
                    "mAP_sample_wise_all_actions": 0.1671758716920746,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8464323258869908,
                    "precision_with_null_verb": 0.5107425315166824,
                    "recall_with_null_verb": 0.6035324203140262,
                    "f1_with_null_verb": 0.487255554291845,
                    "num_predictions": 1522,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.34838658062382205,
                    "mAP_present_only": 0.10284344580231014,
                    "mAP_freq_weighted": 0.36254113479453154,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8528182402773506,
                    "precision": 0.5074492158826975,
                    "recall": 0.5767386382651509,
                    "f1": 0.4829716451352678,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.07097608288299097,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.7739345256867943,
                    "mAP_present_only_with_null_verb": 0.06967262843397114,
                    "mAP_freq_weighted_with_null_verb": 0.21721560191318745,
                    "mAP_sample_wise_with_null_verb": 0.07812114197109662,
                    "mAP_standard_all_actions": 0.7739345256867943,
                    "mAP_present_only_all_actions": 0.06967262843397114,
                    "mAP_freq_weighted_all_actions": 0.21721560191318745,
                    "mAP_sample_wise_all_actions": 0.07812114197109662,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9562746386333771,
                    "precision_with_null_verb": 0.4930987518803106,
                    "recall_with_null_verb": 0.4846233759314878,
                    "f1_with_null_verb": 0.4888243295437371,
                    "num_predictions": 1522,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7894318447767111,
                    "mAP_present_only": 0.07097608288299097,
                    "mAP_freq_weighted": 0.23700294912222636,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.955734336119887,
                    "precision": 0.49380642831347055,
                    "recall": 0.4836650088784815,
                    "f1": 0.4886831091875355,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.08200438487020689,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.8157744252464576,
                    "mAP_present_only_with_null_verb": 0.07887212623228804,
                    "mAP_freq_weighted_with_null_verb": 0.26403075867890985,
                    "mAP_sample_wise_with_null_verb": 0.024039882193280723,
                    "mAP_standard_all_actions": 0.8157744252464576,
                    "mAP_present_only_all_actions": 0.07887212623228804,
                    "mAP_freq_weighted_all_actions": 0.26403075867890985,
                    "mAP_sample_wise_all_actions": 0.024039882193280723,
                    "exact_match_with_null_verb": 0.19250985545335086,
                    "hamming_accuracy_with_null_verb": 0.9866162943495401,
                    "precision_with_null_verb": 0.49330814717477006,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4966315322972718,
                    "num_predictions": 1522,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.833979516412697,
                    "mAP_present_only": 0.08200438487020689,
                    "mAP_freq_weighted": 0.2928210951434318,
                    "exact_match": 0.22864651773981604,
                    "hamming_accuracy": 0.9880126932647413,
                    "precision": 0.49400634663237064,
                    "recall": 0.5,
                    "f1": 0.49698510306904203,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.10706299302922337,
                    "exact_match": 0.16885676741130093,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.10284344580231014,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.07097608288299097,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.08200438487020689,
                    "exact_match": 0.22864651773981604,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID05": {
        "video_id": "VID05",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.08196582160044834,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 23,
                        "subset_action_sparsity": 0.7553191489361702
                    },
                    "mAP_standard_with_null_verb": 0.6697086570776777,
                    "mAP_present_only_with_null_verb": 0.07580252722183675,
                    "mAP_freq_weighted_with_null_verb": 0.2982920466875931,
                    "mAP_sample_wise_with_null_verb": 0.052981658751436185,
                    "mAP_standard_all_actions": 0.6697086570776777,
                    "mAP_present_only_all_actions": 0.07580252722183675,
                    "mAP_freq_weighted_all_actions": 0.2982920466875931,
                    "mAP_sample_wise_all_actions": 0.052981658751436185,
                    "exact_match_with_null_verb": 0.1612627986348123,
                    "hamming_accuracy_with_null_verb": 0.95830204778157,
                    "precision_with_null_verb": 0.5126124267009212,
                    "recall_with_null_verb": 0.5314918637798253,
                    "f1_with_null_verb": 0.5157943538405977,
                    "num_predictions": 2344,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6796299350724502,
                    "mAP_present_only": 0.08196582160044834,
                    "mAP_freq_weighted": 0.32253657631260235,
                    "exact_match": 0.18344709897610922,
                    "hamming_accuracy": 0.9573878077118583,
                    "precision": 0.5131614634526472,
                    "recall": 0.5352151911879012,
                    "f1": 0.5165798490818079,
                    "num_actions_total": 94,
                    "num_actions_present": 23,
                    "action_sparsity": 0.7553191489361702
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.13811684979274205,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 23,
                        "subset_action_sparsity": 0.7553191489361702
                    },
                    "mAP_standard_with_null_verb": 0.27382668672191274,
                    "mAP_present_only_with_null_verb": 0.13010264123812604,
                    "mAP_freq_weighted_with_null_verb": 0.3613588814971762,
                    "mAP_sample_wise_with_null_verb": 0.06880234411389421,
                    "mAP_standard_all_actions": 0.27382668672191274,
                    "mAP_present_only_all_actions": 0.13010264123812604,
                    "mAP_freq_weighted_all_actions": 0.3613588814971762,
                    "mAP_sample_wise_all_actions": 0.06880234411389421,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8012713310580205,
                    "precision_with_null_verb": 0.5049970889306316,
                    "recall_with_null_verb": 0.5637963623969566,
                    "f1_with_null_verb": 0.4638298535539723,
                    "num_predictions": 2344,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.2784753994173731,
                    "mAP_present_only": 0.13811684979274205,
                    "mAP_freq_weighted": 0.38639169235668724,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8075121632415947,
                    "precision": 0.5041583655159119,
                    "recall": 0.5530904337445797,
                    "f1": 0.46423921394679346,
                    "num_actions_total": 94,
                    "num_actions_present": 23,
                    "action_sparsity": 0.7553191489361702
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.051884628808136615,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 23,
                        "subset_action_sparsity": 0.7553191489361702
                    },
                    "mAP_standard_with_null_verb": 0.7129219872122373,
                    "mAP_present_only_with_null_verb": 0.049699950816297335,
                    "mAP_freq_weighted_with_null_verb": 0.156108876603954,
                    "mAP_sample_wise_with_null_verb": 0.06636137389353935,
                    "mAP_standard_all_actions": 0.7129219872122373,
                    "mAP_present_only_all_actions": 0.049699950816297335,
                    "mAP_freq_weighted_all_actions": 0.156108876603954,
                    "mAP_sample_wise_all_actions": 0.06636137389353935,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.952721843003413,
                    "precision_with_null_verb": 0.4935684353837721,
                    "recall_with_null_verb": 0.48234912955203246,
                    "f1_with_null_verb": 0.48789429299262865,
                    "num_predictions": 2344,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7254611325807141,
                    "mAP_present_only": 0.051884628808136615,
                    "mAP_freq_weighted": 0.166274735592838,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9508341805242901,
                    "precision": 0.49372890782602136,
                    "recall": 0.4812288962903411,
                    "f1": 0.4873987702372284,
                    "num_actions_total": 94,
                    "num_actions_present": 23,
                    "action_sparsity": 0.7553191489361702
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.05127195115351537,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 23,
                        "subset_action_sparsity": 0.7553191489361702
                    },
                    "mAP_standard_with_null_verb": 0.752612335724826,
                    "mAP_present_only_with_null_verb": 0.04850898355702265,
                    "mAP_freq_weighted_with_null_verb": 0.1887246428752687,
                    "mAP_sample_wise_with_null_verb": 0.031071926592853637,
                    "mAP_standard_all_actions": 0.752612335724826,
                    "mAP_present_only_all_actions": 0.04850898355702265,
                    "mAP_freq_weighted_all_actions": 0.1887246428752687,
                    "mAP_sample_wise_all_actions": 0.031071926592853637,
                    "exact_match_with_null_verb": 0.23378839590443687,
                    "hamming_accuracy_with_null_verb": 0.9875853242320819,
                    "precision_with_null_verb": 0.49379266211604095,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49687694520165704,
                    "num_predictions": 2344,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7678644135801154,
                    "mAP_present_only": 0.05127195115351537,
                    "mAP_freq_weighted": 0.20292610669971428,
                    "exact_match": 0.2598122866894198,
                    "hamming_accuracy": 0.9879229903420231,
                    "precision": 0.49396149517101157,
                    "recall": 0.5,
                    "f1": 0.49696240505375433,
                    "num_actions_total": 94,
                    "num_actions_present": 23,
                    "action_sparsity": 0.7553191489361702
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.08196582160044834,
                    "exact_match": 0.18344709897610922,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.13811684979274205,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.051884628808136615,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.05127195115351537,
                    "exact_match": 0.2598122866894198,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID08": {
        "video_id": "VID08",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.10054135549392812,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 22,
                        "subset_action_sparsity": 0.7659574468085106
                    },
                    "mAP_standard_with_null_verb": 0.6831230558706787,
                    "mAP_present_only_with_null_verb": 0.08893483027184143,
                    "mAP_freq_weighted_with_null_verb": 0.3983800258726906,
                    "mAP_sample_wise_with_null_verb": 0.04774167109842616,
                    "mAP_standard_all_actions": 0.6831230558706787,
                    "mAP_present_only_all_actions": 0.08893483027184143,
                    "mAP_freq_weighted_all_actions": 0.3983800258726906,
                    "mAP_sample_wise_all_actions": 0.04774167109842616,
                    "exact_match_with_null_verb": 0.10072416063199473,
                    "hamming_accuracy_with_null_verb": 0.9560829493087558,
                    "precision_with_null_verb": 0.5182354685809984,
                    "recall_with_null_verb": 0.5478099124147273,
                    "f1_with_null_verb": 0.523747570417372,
                    "num_predictions": 1519,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7150203172432599,
                    "mAP_present_only": 0.10054135549392812,
                    "mAP_freq_weighted": 0.427360053900683,
                    "exact_match": 0.13495720868992758,
                    "hamming_accuracy": 0.95543680753015,
                    "precision": 0.5191594240231061,
                    "recall": 0.5525087121719333,
                    "f1": 0.5251470585695057,
                    "num_actions_total": 94,
                    "num_actions_present": 22,
                    "action_sparsity": 0.7659574468085106
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.09927042005406984,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 22,
                        "subset_action_sparsity": 0.7659574468085106
                    },
                    "mAP_standard_with_null_verb": 0.2538143945868325,
                    "mAP_present_only_with_null_verb": 0.0915938253339713,
                    "mAP_freq_weighted_with_null_verb": 0.31119225831547576,
                    "mAP_sample_wise_with_null_verb": 0.0657110537077817,
                    "mAP_standard_all_actions": 0.2538143945868325,
                    "mAP_present_only_all_actions": 0.0915938253339713,
                    "mAP_freq_weighted_all_actions": 0.31119225831547576,
                    "mAP_sample_wise_all_actions": 0.0657110537077817,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8104608294930875,
                    "precision_with_null_verb": 0.5029266146733125,
                    "recall_with_null_verb": 0.5345690520918208,
                    "f1_with_null_verb": 0.46392620900760445,
                    "num_predictions": 1519,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.2679143536296759,
                    "mAP_present_only": 0.09927042005406984,
                    "mAP_freq_weighted": 0.3301892536167066,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8155561469611867,
                    "precision": 0.502197374632613,
                    "recall": 0.5257254932231603,
                    "f1": 0.46421198083584575,
                    "num_actions_total": 94,
                    "num_actions_present": 22,
                    "action_sparsity": 0.7659574468085106
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.08248612582593223,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 22,
                        "subset_action_sparsity": 0.7659574468085106
                    },
                    "mAP_standard_with_null_verb": 0.7191203106595755,
                    "mAP_present_only_with_null_verb": 0.07353965638298257,
                    "mAP_freq_weighted_with_null_verb": 0.17715317886151802,
                    "mAP_sample_wise_with_null_verb": 0.06937115337208286,
                    "mAP_standard_all_actions": 0.7191203106595755,
                    "mAP_present_only_all_actions": 0.07353965638298257,
                    "mAP_freq_weighted_all_actions": 0.17715317886151802,
                    "mAP_sample_wise_all_actions": 0.06937115337208286,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9523633969716919,
                    "precision_with_null_verb": 0.49335324971182637,
                    "recall_with_null_verb": 0.4823709078298911,
                    "f1_with_null_verb": 0.48780027245383795,
                    "num_predictions": 1519,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7427095188103245,
                    "mAP_present_only": 0.08248612582593223,
                    "mAP_freq_weighted": 0.18890653465473606,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.950338268457692,
                    "precision": 0.4934399522905621,
                    "recall": 0.4812493793533926,
                    "f1": 0.48726843123947416,
                    "num_actions_total": 94,
                    "num_actions_present": 22,
                    "action_sparsity": 0.7659574468085106
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.05740446477312788,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 22,
                        "subset_action_sparsity": 0.7659574468085106
                    },
                    "mAP_standard_with_null_verb": 0.7534812114544284,
                    "mAP_present_only_with_null_verb": 0.051850813286262704,
                    "mAP_freq_weighted_with_null_verb": 0.18897835618981546,
                    "mAP_sample_wise_with_null_verb": 0.025525080903597657,
                    "mAP_standard_all_actions": 0.7534812114544284,
                    "mAP_present_only_all_actions": 0.051850813286262704,
                    "mAP_freq_weighted_all_actions": 0.18897835618981546,
                    "mAP_sample_wise_all_actions": 0.025525080903597657,
                    "exact_match_with_null_verb": 0.17577353522053982,
                    "hamming_accuracy_with_null_verb": 0.9871691902567479,
                    "precision_with_null_verb": 0.49358459512837394,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4967715859811629,
                    "num_predictions": 1519,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7793925343086046,
                    "mAP_present_only": 0.05740446477312788,
                    "mAP_freq_weighted": 0.20179875184383184,
                    "exact_match": 0.21593153390388414,
                    "hamming_accuracy": 0.9873657081226451,
                    "precision": 0.49368285406132256,
                    "recall": 0.5,
                    "f1": 0.4968213470158721,
                    "num_actions_total": 94,
                    "num_actions_present": 22,
                    "action_sparsity": 0.7659574468085106
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.10054135549392812,
                    "exact_match": 0.13495720868992758,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.09927042005406984,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.08248612582593223,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.05740446477312788,
                    "exact_match": 0.21593153390388414,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID10": {
        "video_id": "VID10",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.12351120357171501,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 15,
                        "subset_action_sparsity": 0.8404255319148937
                    },
                    "mAP_standard_with_null_verb": 0.7598607877583096,
                    "mAP_present_only_with_null_verb": 0.09930393879154793,
                    "mAP_freq_weighted_with_null_verb": 0.3441177127860486,
                    "mAP_sample_wise_with_null_verb": 0.07696487349272764,
                    "mAP_standard_all_actions": 0.7598607877583096,
                    "mAP_present_only_all_actions": 0.09930393879154793,
                    "mAP_freq_weighted_all_actions": 0.3441177127860486,
                    "mAP_sample_wise_all_actions": 0.07696487349272764,
                    "exact_match_with_null_verb": 0.1017724413950829,
                    "hamming_accuracy_with_null_verb": 0.969685534591195,
                    "precision_with_null_verb": 0.5013894070377425,
                    "recall_with_null_verb": 0.5013653046140267,
                    "f1_with_null_verb": 0.5013766292424829,
                    "num_predictions": 1749,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7963049792933588,
                    "mAP_present_only": 0.12351120357171501,
                    "mAP_freq_weighted": 0.3723752276739048,
                    "exact_match": 0.11149228130360206,
                    "hamming_accuracy": 0.9693320195126699,
                    "precision": 0.5016893086218269,
                    "recall": 0.5018322532141136,
                    "f1": 0.5017443248259298,
                    "num_actions_total": 94,
                    "num_actions_present": 15,
                    "action_sparsity": 0.8404255319148937
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.1242220022489972,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 15,
                        "subset_action_sparsity": 0.8404255319148937
                    },
                    "mAP_standard_with_null_verb": 0.3910039278322727,
                    "mAP_present_only_with_null_verb": 0.10501963916136343,
                    "mAP_freq_weighted_with_null_verb": 0.3353288186907937,
                    "mAP_sample_wise_with_null_verb": 0.12066438605161212,
                    "mAP_standard_all_actions": 0.3910039278322727,
                    "mAP_present_only_all_actions": 0.10501963916136343,
                    "mAP_freq_weighted_all_actions": 0.3353288186907937,
                    "mAP_sample_wise_all_actions": 0.12066438605161212,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8746769582618639,
                    "precision_with_null_verb": 0.5056707774047591,
                    "recall_with_null_verb": 0.5378341089969612,
                    "f1_with_null_verb": 0.4890347052779433,
                    "num_predictions": 1749,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.41343968120994634,
                    "mAP_present_only": 0.1242220022489972,
                    "mAP_freq_weighted": 0.35869349164679676,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8775774606766177,
                    "precision": 0.5044904225159618,
                    "recall": 0.5303947279625686,
                    "f1": 0.4875690706248297,
                    "num_actions_total": 94,
                    "num_actions_present": 15,
                    "action_sparsity": 0.8404255319148937
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.11298462837670716,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 15,
                        "subset_action_sparsity": 0.8404255319148937
                    },
                    "mAP_standard_with_null_verb": 0.7783682507918607,
                    "mAP_present_only_with_null_verb": 0.09184125395930318,
                    "mAP_freq_weighted_with_null_verb": 0.3185005381562721,
                    "mAP_sample_wise_with_null_verb": 0.09021005294258219,
                    "mAP_standard_all_actions": 0.7783682507918607,
                    "mAP_present_only_all_actions": 0.09184125395930318,
                    "mAP_freq_weighted_all_actions": 0.3185005381562721,
                    "mAP_sample_wise_all_actions": 0.09021005294258219,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9519496855345913,
                    "precision_with_null_verb": 0.4919513059921995,
                    "recall_with_null_verb": 0.4835052504414088,
                    "f1_with_null_verb": 0.487691712849594,
                    "num_predictions": 1749,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8159018024005383,
                    "mAP_present_only": 0.11298462837670716,
                    "mAP_freq_weighted": 0.34340375059324557,
                    "exact_match": 0.002287021154945683,
                    "hamming_accuracy": 0.950464094984368,
                    "precision": 0.49223819664075197,
                    "recall": 0.48246285707228515,
                    "f1": 0.4873015081018374,
                    "num_actions_total": 94,
                    "num_actions_present": 15,
                    "action_sparsity": 0.8404255319148937
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.14324212584383922,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 15,
                        "subset_action_sparsity": 0.8404255319148937
                    },
                    "mAP_standard_with_null_verb": 0.8228880703316668,
                    "mAP_present_only_with_null_verb": 0.11444035165833441,
                    "mAP_freq_weighted_with_null_verb": 0.26295715336240866,
                    "mAP_sample_wise_with_null_verb": 0.023185797294734436,
                    "mAP_standard_all_actions": 0.8228880703316668,
                    "mAP_present_only_all_actions": 0.11444035165833441,
                    "mAP_freq_weighted_all_actions": 0.26295715336240866,
                    "mAP_sample_wise_all_actions": 0.023185797294734436,
                    "exact_match_with_null_verb": 0.13607775871926817,
                    "hamming_accuracy_with_null_verb": 0.9844253859348199,
                    "precision_with_null_verb": 0.49221269296740994,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4960757874356049,
                    "num_predictions": 1749,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8632833179538041,
                    "mAP_present_only": 0.14324212584383922,
                    "mAP_freq_weighted": 0.28205250713361724,
                    "exact_match": 0.14922813036020582,
                    "hamming_accuracy": 0.9850127124314197,
                    "precision": 0.4925063562157099,
                    "recall": 0.5,
                    "f1": 0.4962248887690441,
                    "num_actions_total": 94,
                    "num_actions_present": 15,
                    "action_sparsity": 0.8404255319148937
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.12351120357171501,
                    "exact_match": 0.11149228130360206,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.1242220022489972,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.11298462837670716,
                    "exact_match": 0.002287021154945683,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.14324212584383922,
                    "exact_match": 0.14922813036020582,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID103": {
        "video_id": "VID103",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.08077520920009187,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.7022514938738972,
                    "mAP_present_only_with_null_verb": 0.0794696209782045,
                    "mAP_freq_weighted_with_null_verb": 0.20715140547921942,
                    "mAP_sample_wise_with_null_verb": 0.06575207354361717,
                    "mAP_standard_all_actions": 0.7022514938738972,
                    "mAP_present_only_all_actions": 0.0794696209782045,
                    "mAP_freq_weighted_all_actions": 0.20715140547921942,
                    "mAP_sample_wise_all_actions": 0.06575207354361717,
                    "exact_match_with_null_verb": 0.13789995493465526,
                    "hamming_accuracy_with_null_verb": 0.9733123028391167,
                    "precision_with_null_verb": 0.49561683593308414,
                    "recall_with_null_verb": 0.4972577808482804,
                    "f1_with_null_verb": 0.49625843454599927,
                    "num_predictions": 2219,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 28,
                    "action_sparsity_with_null_verb": 0.72,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7129721301064075,
                    "mAP_present_only": 0.08077520920009187,
                    "mAP_freq_weighted": 0.22083416346602697,
                    "exact_match": 0.18521856692203695,
                    "hamming_accuracy": 0.9737470395903848,
                    "precision": 0.496160598626593,
                    "recall": 0.49727090972620447,
                    "f1": 0.4966144824660141,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.08350451693323617,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.36426923355642293,
                    "mAP_present_only_with_null_verb": 0.08667583413008219,
                    "mAP_freq_weighted_with_null_verb": 0.23993754122701708,
                    "mAP_sample_wise_with_null_verb": 0.08222879459238693,
                    "mAP_standard_all_actions": 0.36426923355642293,
                    "mAP_present_only_all_actions": 0.08667583413008219,
                    "mAP_freq_weighted_all_actions": 0.23993754122701708,
                    "mAP_sample_wise_all_actions": 0.08222879459238693,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8283235691753041,
                    "precision_with_null_verb": 0.5063483803130614,
                    "recall_with_null_verb": 0.5534629599282592,
                    "f1_with_null_verb": 0.47750080088115693,
                    "num_predictions": 2219,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 28,
                    "action_sparsity_with_null_verb": 0.72,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.37327247790777557,
                    "mAP_present_only": 0.08350451693323617,
                    "mAP_freq_weighted": 0.24296659312066976,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.832467183799488,
                    "precision": 0.5029165632642859,
                    "recall": 0.5255342943703004,
                    "f1": 0.4726969253038981,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.06935255690963711,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.6989531662663915,
                    "mAP_present_only_with_null_verb": 0.06768987952282655,
                    "mAP_freq_weighted_with_null_verb": 0.16182755142866706,
                    "mAP_sample_wise_with_null_verb": 0.06462190547161564,
                    "mAP_standard_all_actions": 0.6989531662663915,
                    "mAP_present_only_all_actions": 0.06768987952282655,
                    "mAP_freq_weighted_all_actions": 0.16182755142866706,
                    "mAP_sample_wise_all_actions": 0.06462190547161564,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.95,
                    "precision_with_null_verb": 0.49143505891019634,
                    "recall_with_null_verb": 0.4829969847772493,
                    "f1_with_null_verb": 0.48717948717948717,
                    "num_predictions": 2219,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 28,
                    "action_sparsity_with_null_verb": 0.72,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7099341906674567,
                    "mAP_present_only": 0.06935255690963711,
                    "mAP_freq_weighted": 0.17154798984646422,
                    "exact_match": 0.0009013068949977468,
                    "hamming_accuracy": 0.9489467174211117,
                    "precision": 0.4919767355156215,
                    "recall": 0.48193155367699336,
                    "f1": 0.48690234008899863,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.061281739414169284,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.7367879114237226,
                    "mAP_present_only_with_null_verb": 0.059956826513294884,
                    "mAP_freq_weighted_with_null_verb": 0.1644083554640799,
                    "mAP_sample_wise_with_null_verb": 0.03967005200978078,
                    "mAP_standard_all_actions": 0.7367879114237226,
                    "mAP_present_only_all_actions": 0.059956826513294884,
                    "mAP_freq_weighted_all_actions": 0.1644083554640799,
                    "mAP_sample_wise_all_actions": 0.03967005200978078,
                    "exact_match_with_null_verb": 0.23073456511942317,
                    "hamming_accuracy_with_null_verb": 0.9834429923388914,
                    "precision_with_null_verb": 0.4917214961694457,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4958261952259126,
                    "num_predictions": 2219,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 28,
                    "action_sparsity_with_null_verb": 0.72,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7503408881420662,
                    "mAP_present_only": 0.061281739414169284,
                    "mAP_freq_weighted": 0.17635931817538075,
                    "exact_match": 0.28841820639927895,
                    "hamming_accuracy": 0.9845243688454642,
                    "precision": 0.4922621844227321,
                    "recall": 0.5,
                    "f1": 0.496100921863827,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.08077520920009187,
                    "exact_match": 0.18521856692203695,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.08350451693323617,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.06935255690963711,
                    "exact_match": 0.0009013068949977468,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.061281739414169284,
                    "exact_match": 0.28841820639927895,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID110": {
        "video_id": "VID110",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.05623166662953872,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.6643323897804175,
                    "mAP_present_only_with_null_verb": 0.05308292511265758,
                    "mAP_freq_weighted_with_null_verb": 0.16365558466696709,
                    "mAP_sample_wise_with_null_verb": 0.05576463439180801,
                    "mAP_standard_all_actions": 0.6643323897804175,
                    "mAP_present_only_all_actions": 0.05308292511265758,
                    "mAP_freq_weighted_all_actions": 0.16365558466696709,
                    "mAP_sample_wise_all_actions": 0.05576463439180801,
                    "exact_match_with_null_verb": 0.17049632352941177,
                    "hamming_accuracy_with_null_verb": 0.9742417279411765,
                    "precision_with_null_verb": 0.497721614399413,
                    "recall_with_null_verb": 0.4971286521347011,
                    "f1_with_null_verb": 0.49737024020494514,
                    "num_predictions": 2176,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 27,
                    "action_sparsity_with_null_verb": 0.73,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7040517553108544,
                    "mAP_present_only": 0.05623166662953872,
                    "mAP_freq_weighted": 0.17733310181519102,
                    "exact_match": 0.22886029411764705,
                    "hamming_accuracy": 0.9745775969962454,
                    "precision": 0.4983331282196219,
                    "recall": 0.49752703782127544,
                    "f1": 0.4977571412108735,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.11657030947428115,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.29025275379826193,
                    "mAP_present_only_with_null_verb": 0.11204723628985873,
                    "mAP_freq_weighted_with_null_verb": 0.36627920444317325,
                    "mAP_sample_wise_with_null_verb": 0.16807603322906525,
                    "mAP_standard_all_actions": 0.29025275379826193,
                    "mAP_present_only_all_actions": 0.11204723628985873,
                    "mAP_freq_weighted_all_actions": 0.36627920444317325,
                    "mAP_sample_wise_all_actions": 0.16807603322906525,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8146047794117647,
                    "precision_with_null_verb": 0.5093093893604923,
                    "recall_with_null_verb": 0.6231760795014636,
                    "f1_with_null_verb": 0.4737361977904877,
                    "num_predictions": 2176,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 27,
                    "action_sparsity_with_null_verb": 0.73,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.30263804786127557,
                    "mAP_present_only": 0.11657030947428115,
                    "mAP_freq_weighted": 0.3814777225102446,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8224391817897372,
                    "precision": 0.5078373276328717,
                    "recall": 0.611300354880076,
                    "f1": 0.47299585515218134,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.0630573355779682,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.704830333226726,
                    "mAP_present_only_with_null_verb": 0.054927160098984434,
                    "mAP_freq_weighted_with_null_verb": 0.16344262585648928,
                    "mAP_sample_wise_with_null_verb": 0.0843648694835739,
                    "mAP_standard_all_actions": 0.704830333226726,
                    "mAP_present_only_all_actions": 0.054927160098984434,
                    "mAP_freq_weighted_all_actions": 0.16344262585648928,
                    "mAP_sample_wise_all_actions": 0.0843648694835739,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9549540441176471,
                    "precision_with_null_verb": 0.49406780064005934,
                    "recall_with_null_verb": 0.48301527161153857,
                    "f1_with_null_verb": 0.4884790243489626,
                    "num_predictions": 2176,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 27,
                    "action_sparsity_with_null_verb": 0.73,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.748129830288695,
                    "mAP_present_only": 0.0630573355779682,
                    "mAP_freq_weighted": 0.18071102212577125,
                    "exact_match": 0.00045955882352941176,
                    "hamming_accuracy": 0.953985450563204,
                    "precision": 0.4946637801223908,
                    "recall": 0.48195258818705883,
                    "f1": 0.48822546262472605,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.060573055441569004,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.7450998060854722,
                    "mAP_present_only_with_null_verb": 0.05592520772397099,
                    "mAP_freq_weighted_with_null_verb": 0.14981614881629893,
                    "mAP_sample_wise_with_null_verb": 0.02630247454735723,
                    "mAP_standard_all_actions": 0.7450998060854722,
                    "mAP_present_only_all_actions": 0.05592520772397099,
                    "mAP_freq_weighted_all_actions": 0.14981614881629893,
                    "mAP_sample_wise_all_actions": 0.02630247454735723,
                    "exact_match_with_null_verb": 0.2426470588235294,
                    "hamming_accuracy_with_null_verb": 0.9885340073529412,
                    "precision_with_null_verb": 0.4942670036764706,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4971169734576675,
                    "num_predictions": 2176,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 27,
                    "action_sparsity_with_null_verb": 0.73,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7901280230241804,
                    "mAP_present_only": 0.060573055441569004,
                    "mAP_freq_weighted": 0.16361255848581144,
                    "exact_match": 0.3102022058823529,
                    "hamming_accuracy": 0.9897088157071339,
                    "precision": 0.49485440785356694,
                    "recall": 0.5,
                    "f1": 0.4974138968949563,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.05623166662953872,
                    "exact_match": 0.22886029411764705,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.11657030947428115,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.0630573355779682,
                    "exact_match": 0.00045955882352941176,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.060573055441569004,
                    "exact_match": 0.3102022058823529,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID12": {
        "video_id": "VID12",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.13332417897611318,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.7542853230484188,
                    "mAP_present_only_with_null_verb": 0.12142661524209364,
                    "mAP_freq_weighted_with_null_verb": 0.35696856045440734,
                    "mAP_sample_wise_with_null_verb": 0.0529837756264274,
                    "mAP_standard_all_actions": 0.7542853230484188,
                    "mAP_present_only_all_actions": 0.12142661524209364,
                    "mAP_freq_weighted_all_actions": 0.35696856045440734,
                    "mAP_sample_wise_all_actions": 0.0529837756264274,
                    "exact_match_with_null_verb": 0.16238532110091744,
                    "hamming_accuracy_with_null_verb": 0.9623761467889909,
                    "precision_with_null_verb": 0.5285427283035226,
                    "recall_with_null_verb": 0.5470669260806246,
                    "f1_with_null_verb": 0.5348303096540334,
                    "num_predictions": 1090,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7794309685382331,
                    "mAP_present_only": 0.13332417897611318,
                    "mAP_freq_weighted": 0.38185843734108815,
                    "exact_match": 0.18807339449541285,
                    "hamming_accuracy": 0.9615752488776108,
                    "precision": 0.5289598124979284,
                    "recall": 0.5524067209225252,
                    "f1": 0.5363009334131794,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.12160987485692498,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.2951070086920258,
                    "mAP_present_only_with_null_verb": 0.12553504346012895,
                    "mAP_freq_weighted_with_null_verb": 0.34094025847735177,
                    "mAP_sample_wise_with_null_verb": 0.058190907076200026,
                    "mAP_standard_all_actions": 0.2951070086920258,
                    "mAP_present_only_all_actions": 0.12553504346012895,
                    "mAP_freq_weighted_all_actions": 0.34094025847735177,
                    "mAP_sample_wise_all_actions": 0.058190907076200026,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8025688073394496,
                    "precision_with_null_verb": 0.5016271448385424,
                    "recall_with_null_verb": 0.5163443998524566,
                    "f1_with_null_verb": 0.46177943375549063,
                    "num_predictions": 1090,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.29858901992093323,
                    "mAP_present_only": 0.12160987485692498,
                    "mAP_freq_weighted": 0.34201790650976727,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8081788014835057,
                    "precision": 0.4995185144446905,
                    "recall": 0.4951293728809976,
                    "f1": 0.45988670975125395,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.1015349177063391,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.778735796792899,
                    "mAP_present_only_with_null_verb": 0.09367898396449423,
                    "mAP_freq_weighted_with_null_verb": 0.24154626384489208,
                    "mAP_sample_wise_with_null_verb": 0.053823024330065146,
                    "mAP_standard_all_actions": 0.778735796792899,
                    "mAP_present_only_all_actions": 0.09367898396449423,
                    "mAP_freq_weighted_all_actions": 0.24154626384489208,
                    "mAP_sample_wise_all_actions": 0.053823024330065146,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9578165137614679,
                    "precision_with_null_verb": 0.4920444905269111,
                    "recall_with_null_verb": 0.4864414045027583,
                    "f1_with_null_verb": 0.48922690508992417,
                    "num_predictions": 1090,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7949584425639123,
                    "mAP_present_only": 0.1015349177063391,
                    "mAP_freq_weighted": 0.2567015873466203,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9566562560999414,
                    "precision": 0.4923103967855349,
                    "recall": 0.4855839253336504,
                    "f1": 0.48892402695544174,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.14692567390742287,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 17,
                        "subset_action_sparsity": 0.8191489361702128
                    },
                    "mAP_standard_with_null_verb": 0.8265217145279287,
                    "mAP_present_only_with_null_verb": 0.1326085726396437,
                    "mAP_freq_weighted_with_null_verb": 0.3155491315921744,
                    "mAP_sample_wise_with_null_verb": 0.02684683285997532,
                    "mAP_standard_all_actions": 0.8265217145279287,
                    "mAP_present_only_all_actions": 0.1326085726396437,
                    "mAP_freq_weighted_all_actions": 0.3155491315921744,
                    "mAP_sample_wise_all_actions": 0.02684683285997532,
                    "exact_match_with_null_verb": 0.20825688073394497,
                    "hamming_accuracy_with_null_verb": 0.9845137614678899,
                    "precision_with_null_verb": 0.49225688073394497,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49609822848478125,
                    "num_predictions": 1090,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 20,
                    "action_sparsity_with_null_verb": 0.8,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8457206006002785,
                    "mAP_present_only": 0.14692567390742287,
                    "mAP_freq_weighted": 0.3380718878204779,
                    "exact_match": 0.23577981651376148,
                    "hamming_accuracy": 0.985057583447199,
                    "precision": 0.4925287917235995,
                    "recall": 0.5,
                    "f1": 0.4962362762981282,
                    "num_actions_total": 94,
                    "num_actions_present": 17,
                    "action_sparsity": 0.8191489361702128
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.13332417897611318,
                    "exact_match": 0.18807339449541285,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.12160987485692498,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.1015349177063391,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.14692567390742287,
                    "exact_match": 0.23577981651376148,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID13": {
        "video_id": "VID13",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.18808687345598876,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.7402582310378699,
                    "mAP_present_only_with_null_verb": 0.1592538475677366,
                    "mAP_freq_weighted_with_null_verb": 0.5222443647140181,
                    "mAP_sample_wise_with_null_verb": 0.06282880273801969,
                    "mAP_standard_all_actions": 0.7402582310378699,
                    "mAP_present_only_all_actions": 0.1592538475677366,
                    "mAP_freq_weighted_all_actions": 0.5222443647140181,
                    "mAP_sample_wise_all_actions": 0.06282880273801969,
                    "exact_match_with_null_verb": 0.011213047910295617,
                    "hamming_accuracy_with_null_verb": 0.9418042813455657,
                    "precision_with_null_verb": 0.5234573937924788,
                    "recall_with_null_verb": 0.5535161869993469,
                    "f1_with_null_verb": 0.5297801398578674,
                    "num_predictions": 981,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 19,
                    "action_sparsity_with_null_verb": 0.81,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7660573401627214,
                    "mAP_present_only": 0.18808687345598876,
                    "mAP_freq_weighted": 0.5283489180549855,
                    "exact_match": 0.013251783893985729,
                    "hamming_accuracy": 0.9389463638926844,
                    "precision": 0.5234380858051774,
                    "recall": 0.5533330002535705,
                    "f1": 0.5295874701271426,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.16250384150886663,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.33624700381177985,
                    "mAP_present_only_with_null_verb": 0.13814212532515707,
                    "mAP_freq_weighted_with_null_verb": 0.46230383268223657,
                    "mAP_sample_wise_with_null_verb": 0.11673877010182318,
                    "mAP_standard_all_actions": 0.33624700381177985,
                    "mAP_present_only_all_actions": 0.13814212532515707,
                    "mAP_freq_weighted_all_actions": 0.46230383268223657,
                    "mAP_sample_wise_all_actions": 0.11673877010182318,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8405912334352701,
                    "precision_with_null_verb": 0.5114601372304027,
                    "recall_with_null_verb": 0.5787535529216477,
                    "f1_with_null_verb": 0.4906694009102597,
                    "num_predictions": 981,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 19,
                    "action_sparsity_with_null_verb": 0.81,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.3361708666398071,
                    "mAP_present_only": 0.16250384150886663,
                    "mAP_freq_weighted": 0.46764566228483945,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.8420521829657102,
                    "precision": 0.5123834897516354,
                    "recall": 0.5802323070178832,
                    "f1": 0.493131712280587,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.16264859625441178,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.7963021244175821,
                    "mAP_present_only_with_null_verb": 0.13843223377674743,
                    "mAP_freq_weighted_with_null_verb": 0.398374053714416,
                    "mAP_sample_wise_with_null_verb": 0.07580198241547756,
                    "mAP_standard_all_actions": 0.7963021244175821,
                    "mAP_present_only_all_actions": 0.13843223377674743,
                    "mAP_freq_weighted_all_actions": 0.398374053714416,
                    "mAP_sample_wise_all_actions": 0.07580198241547756,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9546483180428135,
                    "precision_with_null_verb": 0.49018078658389774,
                    "recall_with_null_verb": 0.48663015463917525,
                    "f1_with_null_verb": 0.4883990174757889,
                    "num_predictions": 981,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 19,
                    "action_sparsity_with_null_verb": 0.81,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.814918910000751,
                    "mAP_present_only": 0.16264859625441178,
                    "mAP_freq_weighted": 0.40293467000101474,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9519921053202334,
                    "precision": 0.4896587499023884,
                    "recall": 0.4857625055334219,
                    "f1": 0.48770284609529946,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.18400558730619476,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.839726547571082,
                    "mAP_present_only_with_null_verb": 0.15645551353201068,
                    "mAP_freq_weighted_with_null_verb": 0.44501548917918193,
                    "mAP_sample_wise_with_null_verb": 0.02740933317947077,
                    "mAP_standard_all_actions": 0.839726547571082,
                    "mAP_present_only_all_actions": 0.15645551353201068,
                    "mAP_freq_weighted_all_actions": 0.44501548917918193,
                    "mAP_sample_wise_all_actions": 0.02740933317947077,
                    "exact_match_with_null_verb": 0.05402650356778797,
                    "hamming_accuracy_with_null_verb": 0.9808766564729867,
                    "precision_with_null_verb": 0.49043832823649336,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49517301002449515,
                    "num_predictions": 981,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 19,
                    "action_sparsity_with_null_verb": 0.81,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8611073340095651,
                    "mAP_present_only": 0.18400558730619476,
                    "mAP_freq_weighted": 0.4501570712027121,
                    "exact_match": 0.0581039755351682,
                    "hamming_accuracy": 0.9798945930119071,
                    "precision": 0.48994729650595353,
                    "recall": 0.5,
                    "f1": 0.49492260672384897,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.18808687345598876,
                    "exact_match": 0.013251783893985729,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.16250384150886663,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.16264859625441178,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.18400558730619476,
                    "exact_match": 0.0581039755351682,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID15": {
        "video_id": "VID15",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.07062957300108333,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6305768283762659,
                    "mAP_present_only_with_null_verb": 0.06637686572988966,
                    "mAP_freq_weighted_with_null_verb": 0.37204227284302493,
                    "mAP_sample_wise_with_null_verb": 0.06298251016014264,
                    "mAP_standard_all_actions": 0.6305768283762659,
                    "mAP_present_only_all_actions": 0.06637686572988966,
                    "mAP_freq_weighted_all_actions": 0.37204227284302493,
                    "mAP_sample_wise_all_actions": 0.06298251016014264,
                    "exact_match_with_null_verb": 0.08746355685131195,
                    "hamming_accuracy_with_null_verb": 0.949961127308066,
                    "precision_with_null_verb": 0.5121712187759087,
                    "recall_with_null_verb": 0.5313675866925707,
                    "f1_with_null_verb": 0.5146780384459042,
                    "num_predictions": 2058,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6486981706811739,
                    "mAP_present_only": 0.07062957300108333,
                    "mAP_freq_weighted": 0.3991733382771862,
                    "exact_match": 0.11564625850340136,
                    "hamming_accuracy": 0.9488296838492236,
                    "precision": 0.5127590745431574,
                    "recall": 0.5348253862103608,
                    "f1": 0.5154247496719656,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "WorldModelRL_direct_video_ppo": {
                "metrics": {
                    "mAP": 0.10907452917134859,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.273095987451503,
                    "mAP_present_only_with_null_verb": 0.10676124984355803,
                    "mAP_freq_weighted_with_null_verb": 0.39097579956670564,
                    "mAP_sample_wise_with_null_verb": 0.0758726993607729,
                    "mAP_standard_all_actions": 0.273095987451503,
                    "mAP_present_only_all_actions": 0.10676124984355803,
                    "mAP_freq_weighted_all_actions": 0.39097579956670564,
                    "mAP_sample_wise_all_actions": 0.0758726993607729,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.8251068999028183,
                    "precision_with_null_verb": 0.505919197430268,
                    "recall_with_null_verb": 0.5578859041122628,
                    "f1_with_null_verb": 0.47431932778888014,
                    "num_predictions": 2058,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_direct_video_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.2878094342212528,
                    "mAP_present_only": 0.10907452917134859,
                    "mAP_freq_weighted": 0.40857346955993074,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.828091723011393,
                    "precision": 0.5042886196525915,
                    "recall": 0.541937013528285,
                    "f1": 0.47252651892837033,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.06728404257490182,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6798846108692458,
                    "mAP_present_only_with_null_verb": 0.06414390602982488,
                    "mAP_freq_weighted_with_null_verb": 0.22748390876602304,
                    "mAP_sample_wise_with_null_verb": 0.08043220474647464,
                    "mAP_standard_all_actions": 0.6798846108692458,
                    "mAP_present_only_all_actions": 0.06414390602982488,
                    "mAP_freq_weighted_all_actions": 0.22748390876602304,
                    "mAP_sample_wise_all_actions": 0.08043220474647464,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9542176870748299,
                    "precision_with_null_verb": 0.4928143277865557,
                    "recall_with_null_verb": 0.48480831964408594,
                    "f1_with_null_verb": 0.488710226343402,
                    "num_predictions": 2058,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6902548211925239,
                    "mAP_present_only": 0.06728404257490182,
                    "mAP_freq_weighted": 0.2408997285649908,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9524946756818229,
                    "precision": 0.49293963725198753,
                    "recall": 0.48385677266515564,
                    "f1": 0.48826933039920617,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.05126041814764916,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.7051212607537952,
                    "mAP_present_only_with_null_verb": 0.048778260496114,
                    "mAP_freq_weighted_with_null_verb": 0.23724257248277875,
                    "mAP_sample_wise_with_null_verb": 0.03978261545229905,
                    "mAP_standard_all_actions": 0.7051212607537952,
                    "mAP_present_only_all_actions": 0.048778260496114,
                    "mAP_freq_weighted_all_actions": 0.23724257248277875,
                    "mAP_sample_wise_all_actions": 0.03978261545229905,
                    "exact_match_with_null_verb": 0.1360544217687075,
                    "hamming_accuracy_with_null_verb": 0.9854567541302235,
                    "precision_with_null_verb": 0.49272837706511174,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4963375566253148,
                    "num_predictions": 2058,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7173967202992997,
                    "mAP_present_only": 0.05126041814764916,
                    "mAP_freq_weighted": 0.25339977327368945,
                    "exact_match": 0.17784256559766765,
                    "hamming_accuracy": 0.9857277257407522,
                    "precision": 0.4928638628703761,
                    "recall": 0.5,
                    "f1": 0.4964062862007117,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "WorldModelRL_direct_video_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.07062957300108333,
                    "exact_match": 0.11564625850340136,
                    "task": "single_step_action_prediction"
                },
                "WorldModelRL_direct_video_ppo": {
                    "mAP": 0.10907452917134859,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.06728404257490182,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.05126041814764916,
                    "exact_match": 0.17784256559766765,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    }
}