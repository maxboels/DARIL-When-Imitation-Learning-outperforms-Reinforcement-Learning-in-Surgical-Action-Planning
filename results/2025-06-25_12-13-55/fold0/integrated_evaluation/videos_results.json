{
    "VID02": {
        "video_id": "VID02",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.4131367254324308,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6980818665261936,
                    "mAP_present_only_with_null_verb": 0.37671137213586364,
                    "mAP_freq_weighted_with_null_verb": 0.6824366100030327,
                    "mAP_sample_wise_with_null_verb": 0.713833566682863,
                    "mAP_standard_all_actions": 0.6980818665261936,
                    "mAP_present_only_all_actions": 0.37671137213586364,
                    "mAP_freq_weighted_all_actions": 0.6824366100030327,
                    "mAP_sample_wise_all_actions": 0.713833566682863,
                    "exact_match_with_null_verb": 0.3754843254667136,
                    "hamming_accuracy_with_null_verb": 0.9899084184572032,
                    "precision_with_null_verb": 0.8463197300752954,
                    "recall_with_null_verb": 0.7582579329285377,
                    "f1_with_null_verb": 0.7956561780309357,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7294449820437029,
                    "mAP_present_only": 0.4131367254324308,
                    "mAP_freq_weighted": 0.7041875106651216,
                    "exact_match": 0.40894681225783724,
                    "hamming_accuracy": 0.9905233338079785,
                    "precision": 0.8624573650812246,
                    "recall": 0.7673422552203435,
                    "f1": 0.807479151798683,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.4131367254324308,
                    "exact_match": 0.40894681225783724,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID06": {
        "video_id": "VID06",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.5349787979669166,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.8286863275440551,
                    "mAP_present_only_with_null_verb": 0.49562109181351716,
                    "mAP_freq_weighted_with_null_verb": 0.7630840525508162,
                    "mAP_sample_wise_with_null_verb": 0.831260476043804,
                    "mAP_standard_all_actions": 0.8286863275440551,
                    "mAP_present_only_all_actions": 0.49562109181351716,
                    "mAP_freq_weighted_all_actions": 0.7630840525508162,
                    "mAP_sample_wise_all_actions": 0.831260476043804,
                    "exact_match_with_null_verb": 0.3660009289363679,
                    "hamming_accuracy_with_null_verb": 0.9907013469577334,
                    "precision_with_null_verb": 0.8987708816346373,
                    "recall_with_null_verb": 0.8063869280623164,
                    "f1_with_null_verb": 0.8462934425074607,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8501005185865939,
                    "mAP_present_only": 0.5349787979669166,
                    "mAP_freq_weighted": 0.8012983865788673,
                    "exact_match": 0.4407803065490014,
                    "hamming_accuracy": 0.9915654554258778,
                    "precision": 0.905550905346751,
                    "recall": 0.826197485396795,
                    "f1": 0.8614074273605463,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.5349787979669166,
                    "exact_match": 0.4407803065490014,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID111": {
        "video_id": "VID111",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.4751673164879733,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.8068976858170086,
                    "mAP_present_only_with_null_verb": 0.4375782269552023,
                    "mAP_freq_weighted_with_null_verb": 0.627352425447587,
                    "mAP_sample_wise_with_null_verb": 0.6985564684144517,
                    "mAP_standard_all_actions": 0.8068976858170086,
                    "mAP_present_only_all_actions": 0.4375782269552023,
                    "mAP_freq_weighted_all_actions": 0.627352425447587,
                    "mAP_sample_wise_all_actions": 0.6985564684144517,
                    "exact_match_with_null_verb": 0.45174825174825173,
                    "hamming_accuracy_with_null_verb": 0.9895664335664336,
                    "precision_with_null_verb": 0.7917956956706316,
                    "recall_with_null_verb": 0.7412429788744682,
                    "f1_with_null_verb": 0.7640426853654192,
                    "num_predictions": 2145,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8391402437468013,
                    "mAP_present_only": 0.4751673164879733,
                    "mAP_freq_weighted": 0.6667387762254551,
                    "exact_match": 0.5156177156177156,
                    "hamming_accuracy": 0.9912314635718891,
                    "precision": 0.802154605430187,
                    "recall": 0.7805731676326582,
                    "f1": 0.7909523759248178,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.4751673164879733,
                    "exact_match": 0.5156177156177156,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID14": {
        "video_id": "VID14",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.32882127530481264,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 36,
                        "subset_action_sparsity": 0.6170212765957447
                    },
                    "mAP_standard_with_null_verb": 0.7066026505781057,
                    "mAP_present_only_with_null_verb": 0.3087869526295258,
                    "mAP_freq_weighted_with_null_verb": 0.6818124824540821,
                    "mAP_sample_wise_with_null_verb": 0.7685148385265889,
                    "mAP_standard_all_actions": 0.7066026505781057,
                    "mAP_present_only_all_actions": 0.3087869526295258,
                    "mAP_freq_weighted_all_actions": 0.6818124824540821,
                    "mAP_sample_wise_all_actions": 0.7685148385265889,
                    "exact_match_with_null_verb": 0.41100702576112413,
                    "hamming_accuracy_with_null_verb": 0.9904918032786886,
                    "precision_with_null_verb": 0.8768145761117316,
                    "recall_with_null_verb": 0.7801863995679539,
                    "f1_with_null_verb": 0.8211454662486258,
                    "num_predictions": 1708,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 41,
                    "action_sparsity_with_null_verb": 0.5900000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7323145309678005,
                    "mAP_present_only": 0.32882127530481264,
                    "mAP_freq_weighted": 0.7075050878600736,
                    "exact_match": 0.46662763466042156,
                    "hamming_accuracy": 0.9910994568737854,
                    "precision": 0.8844036628286038,
                    "recall": 0.7973097428851195,
                    "f1": 0.8350988828185593,
                    "num_actions_total": 94,
                    "num_actions_present": 36,
                    "action_sparsity": 0.6170212765957447
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.32882127530481264,
                    "exact_match": 0.46662763466042156,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID23": {
        "video_id": "VID23",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.44190359656189926,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.7637337905026778,
                    "mAP_present_only_with_null_verb": 0.39914125968605696,
                    "mAP_freq_weighted_with_null_verb": 0.6686950634006166,
                    "mAP_sample_wise_with_null_verb": 0.7497734590497523,
                    "mAP_standard_all_actions": 0.7637337905026778,
                    "mAP_present_only_all_actions": 0.39914125968605696,
                    "mAP_freq_weighted_all_actions": 0.6686950634006166,
                    "mAP_sample_wise_all_actions": 0.7497734590497523,
                    "exact_match_with_null_verb": 0.3889908256880734,
                    "hamming_accuracy_with_null_verb": 0.9898776758409786,
                    "precision_with_null_verb": 0.8711943433225884,
                    "recall_with_null_verb": 0.7693066915141831,
                    "f1_with_null_verb": 0.8118547082959707,
                    "num_predictions": 1635,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7924414203256317,
                    "mAP_present_only": 0.44190359656189926,
                    "mAP_freq_weighted": 0.693940258365958,
                    "exact_match": 0.46788990825688076,
                    "hamming_accuracy": 0.9908387012818011,
                    "precision": 0.8803066294971209,
                    "recall": 0.7894933731383997,
                    "f1": 0.8285287907182545,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.44190359656189926,
                    "exact_match": 0.46788990825688076,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID25": {
        "video_id": "VID25",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.6515556967246449,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.8842820692907545,
                    "mAP_present_only_with_null_verb": 0.5549310357336703,
                    "mAP_freq_weighted_with_null_verb": 0.7784982955073393,
                    "mAP_sample_wise_with_null_verb": 0.8089756232836075,
                    "mAP_standard_all_actions": 0.8842820692907545,
                    "mAP_present_only_all_actions": 0.5549310357336703,
                    "mAP_freq_weighted_all_actions": 0.7784982955073393,
                    "mAP_sample_wise_all_actions": 0.8089756232836075,
                    "exact_match_with_null_verb": 0.4279004227336778,
                    "hamming_accuracy_with_null_verb": 0.9914185063410051,
                    "precision_with_null_verb": 0.901262614496714,
                    "recall_with_null_verb": 0.814501111692918,
                    "f1_with_null_verb": 0.8524301673647598,
                    "num_predictions": 2129,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.9221560599065696,
                    "mAP_present_only": 0.6515556967246449,
                    "mAP_freq_weighted": 0.8181614090124388,
                    "exact_match": 0.47111319868482854,
                    "hamming_accuracy": 0.9920749927545647,
                    "precision": 0.906576592432329,
                    "recall": 0.8318458632571795,
                    "f1": 0.8652912620699941,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.6515556967246449,
                    "exact_match": 0.47111319868482854,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID50": {
        "video_id": "VID50",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.5815507423680483,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.8954344389197573,
                    "mAP_present_only_with_null_verb": 0.5301913273319848,
                    "mAP_freq_weighted_with_null_verb": 0.779175854427179,
                    "mAP_sample_wise_with_null_verb": 0.8467649119203622,
                    "mAP_standard_all_actions": 0.8954344389197573,
                    "mAP_present_only_all_actions": 0.5301913273319848,
                    "mAP_freq_weighted_all_actions": 0.779175854427179,
                    "mAP_sample_wise_all_actions": 0.8467649119203622,
                    "exact_match_with_null_verb": 0.48171846435100546,
                    "hamming_accuracy_with_null_verb": 0.99172760511883,
                    "precision_with_null_verb": 0.8928686592490748,
                    "recall_with_null_verb": 0.8362861420900454,
                    "f1_with_null_verb": 0.8622990323626287,
                    "num_predictions": 1094,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 18,
                    "action_sparsity_with_null_verb": 0.8200000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.9074979987009443,
                    "mAP_present_only": 0.5815507423680483,
                    "mAP_freq_weighted": 0.814248068073398,
                    "exact_match": 0.5237659963436929,
                    "hamming_accuracy": 0.9925026255396943,
                    "precision": 0.903680867913419,
                    "recall": 0.8543736298297402,
                    "f1": 0.8773632457385043,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.5815507423680483,
                    "exact_match": 0.5237659963436929,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID51": {
        "video_id": "VID51",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.48224605009561,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 24,
                        "subset_action_sparsity": 0.7446808510638299
                    },
                    "mAP_standard_with_null_verb": 0.7799836936764223,
                    "mAP_present_only_with_null_verb": 0.44821963336697335,
                    "mAP_freq_weighted_with_null_verb": 0.7177120481161889,
                    "mAP_sample_wise_with_null_verb": 0.7697526286287806,
                    "mAP_standard_all_actions": 0.7799836936764223,
                    "mAP_present_only_all_actions": 0.44821963336697335,
                    "mAP_freq_weighted_all_actions": 0.7177120481161889,
                    "mAP_sample_wise_all_actions": 0.7697526286287806,
                    "exact_match_with_null_verb": 0.3811141304347826,
                    "hamming_accuracy_with_null_verb": 0.9899898097826086,
                    "precision_with_null_verb": 0.8500028894997693,
                    "recall_with_null_verb": 0.7882336381234011,
                    "f1_with_null_verb": 0.8160192814149803,
                    "num_predictions": 2944,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.803977714918028,
                    "mAP_present_only": 0.48224605009561,
                    "mAP_freq_weighted": 0.755954416621518,
                    "exact_match": 0.4391983695652174,
                    "hamming_accuracy": 0.9912046137835338,
                    "precision": 0.8668759059438923,
                    "recall": 0.8038969815104935,
                    "f1": 0.8323283600787521,
                    "num_actions_total": 94,
                    "num_actions_present": 24,
                    "action_sparsity": 0.7446808510638299
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.48224605009561,
                    "exact_match": 0.4391983695652174,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID66": {
        "video_id": "VID66",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.6193666075509946,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 20,
                        "subset_action_sparsity": 0.7872340425531915
                    },
                    "mAP_standard_with_null_verb": 0.8505257682489868,
                    "mAP_present_only_with_null_verb": 0.5675033402129861,
                    "mAP_freq_weighted_with_null_verb": 0.8429977820233758,
                    "mAP_sample_wise_with_null_verb": 0.8596656412339877,
                    "mAP_standard_all_actions": 0.8505257682489868,
                    "mAP_present_only_all_actions": 0.5675033402129861,
                    "mAP_freq_weighted_all_actions": 0.8429977820233758,
                    "mAP_sample_wise_all_actions": 0.8596656412339877,
                    "exact_match_with_null_verb": 0.506578947368421,
                    "hamming_accuracy_with_null_verb": 0.9929276315789474,
                    "precision_with_null_verb": 0.912542227417324,
                    "recall_with_null_verb": 0.8468967440003865,
                    "f1_with_null_verb": 0.8767825887849064,
                    "num_predictions": 1824,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 23,
                    "action_sparsity_with_null_verb": 0.77,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8870992782023392,
                    "mAP_present_only": 0.6193666075509946,
                    "mAP_freq_weighted": 0.864133688871406,
                    "exact_match": 0.5279605263157895,
                    "hamming_accuracy": 0.9932635778275476,
                    "precision": 0.9209000793675668,
                    "recall": 0.8575699281934416,
                    "f1": 0.886566382335627,
                    "num_actions_total": 94,
                    "num_actions_present": 20,
                    "action_sparsity": 0.7872340425531915
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.6193666075509946,
                    "exact_match": 0.5279605263157895,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID79": {
        "video_id": "VID79",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.3940519430992508,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 31,
                        "subset_action_sparsity": 0.6702127659574468
                    },
                    "mAP_standard_with_null_verb": 0.7441330977827875,
                    "mAP_present_only_with_null_verb": 0.37259193828552095,
                    "mAP_freq_weighted_with_null_verb": 0.7818074581789812,
                    "mAP_sample_wise_with_null_verb": 0.789822778295273,
                    "mAP_standard_all_actions": 0.7441330977827875,
                    "mAP_present_only_all_actions": 0.37259193828552095,
                    "mAP_freq_weighted_all_actions": 0.7818074581789812,
                    "mAP_sample_wise_all_actions": 0.789822778295273,
                    "exact_match_with_null_verb": 0.434680726420621,
                    "hamming_accuracy_with_null_verb": 0.9904012888107792,
                    "precision_with_null_verb": 0.874052323022936,
                    "recall_with_null_verb": 0.8120688286599567,
                    "f1_with_null_verb": 0.8401513186827245,
                    "num_predictions": 3414,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 36,
                    "action_sparsity_with_null_verb": 0.64,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7682511727242209,
                    "mAP_present_only": 0.3940519430992508,
                    "mAP_freq_weighted": 0.8103920903706449,
                    "exact_match": 0.47656707674282367,
                    "hamming_accuracy": 0.9911565643345922,
                    "precision": 0.8909461347190013,
                    "recall": 0.8222543653435108,
                    "f1": 0.8531654486449811,
                    "num_actions_total": 94,
                    "num_actions_present": 31,
                    "action_sparsity": 0.6702127659574468
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.3940519430992508,
                    "exact_match": 0.47656707674282367,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    }
}