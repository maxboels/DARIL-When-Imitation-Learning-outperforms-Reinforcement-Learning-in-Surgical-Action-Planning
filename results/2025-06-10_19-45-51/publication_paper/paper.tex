
\documentclass[conference]{IEEEtran}
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{subcaption}
\usepackage{url}
\usepackage{array}
\usepackage{threeparttable}

\def\BibTeX{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}

\begin{document}

\title{Learning Paradigms for Surgical Action Prediction: A Comprehensive Empirical Comparison of Supervised Learning and Reinforcement Learning Approaches}

\author{
\IEEEauthorblockN{Authors}
\IEEEauthorblockA{Institution\\
Email: authors@institution.edu}
}

\maketitle

\begin{abstract}
Accurate surgical action prediction is fundamental for intelligent surgical assistance systems, yet the choice of learning paradigm remains largely unexplored. This paper presents the first systematic empirical comparison of three distinct learning paradigms for surgical action prediction: (1) supervised imitation learning with autoregressive modeling, (2) model-based reinforcement learning with world model simulation, and (3) model-free reinforcement learning on video episodes. Using the CholecT50 dataset, we implement and evaluate each paradigm under identical conditions with unified evaluation protocols. Our comprehensive analysis reveals that while supervised imitation learning achieves the highest single-step prediction accuracy (mAP = 0.737 ± 0.020), all paradigms demonstrate comparable performance (mAP ≥ 0.70), with significant differences emerging in training efficiency, planning stability, and computational requirements. We provide the first empirical foundation for paradigm selection in surgical AI, establishing that the choice should be guided by application-specific constraints rather than pure predictive performance. Our open-source implementation enables reproducible research and provides benchmarks for future surgical AI systems.
\end{abstract}

\begin{IEEEkeywords}
Surgical robotics, imitation learning, reinforcement learning, action prediction, learning paradigms, world models, surgical AI
\end{IEEEkeywords}

\section{Introduction}

The development of intelligent surgical assistance systems requires accurate prediction of upcoming surgical actions to enable proactive guidance, risk assessment, and adaptive decision support~\cite{maier2017surgical}. This capability forms the foundation for advanced surgical AI applications, including real-time anomaly detection, skill assessment, and autonomous surgical assistance~\cite{vardazaryan2018systematic}.

Current approaches to surgical action prediction have predominantly relied on supervised learning paradigms, particularly imitation learning from expert demonstrations~\cite{hussein2017imitation}. While effective, these approaches are fundamentally constrained by the quality and coverage of expert demonstrations and cannot discover strategies that exceed expert performance or adapt to novel scenarios.

Reinforcement Learning (RL) offers alternative paradigms that may overcome these limitations through exploration, optimization, and interaction with the surgical environment~\cite{sutton2018reinforcement}. Recent advances in world models~\cite{ha2018world} and offline RL~\cite{levine2020offline} have made RL approaches increasingly viable for surgical domains, enabling safe exploration through simulation and learning from pre-collected datasets.

However, a fundamental question remains unanswered: \textbf{Which learning paradigm produces the most effective surgical action prediction models for real-world deployment?} This question is critical for practitioners designing surgical AI systems and researchers developing new approaches.

\subsection{Research Questions and Contributions}

This paper addresses the paradigm selection question through the first comprehensive empirical comparison of learning approaches for surgical action prediction. Our key contributions include:

\begin{itemize}
\item \textbf{Paradigm comparison framework}: We compare three distinct learning paradigms—supervised imitation learning, model-based RL with world model simulation, and model-free RL on video episodes—using identical evaluation protocols.

\item \textbf{Comprehensive empirical evaluation}: We provide detailed analysis of prediction accuracy, training efficiency, planning stability, computational requirements, and statistical significance across paradigms.

\item \textbf{Fair evaluation methodology}: We develop evaluation protocols that respect each paradigm's training environment while enabling meaningful cross-paradigm comparisons.

\item \textbf{Practical deployment guidance}: We establish evidence-based criteria for paradigm selection based on application requirements, computational constraints, and performance objectives.

\item \textbf{Open-source implementation}: We release our complete implementation to enable reproducible research and benchmarking for future surgical AI development.
\end{itemize}

\section{Related Work}

\subsection{Surgical Action Recognition and Prediction}

Surgical action prediction has evolved from rule-based approaches~\cite{padoy2012statistical} to deep learning methods using CNNs~\cite{twinanda2016endonet} and transformers~\cite{gao2022trans}. The CholecT50 dataset~\cite{nwoye2022cholect50} has emerged as the standard benchmark, enabling systematic evaluation across different approaches.

Most existing work focuses on architectural improvements within the supervised learning paradigm, with limited exploration of alternative learning frameworks. Our work fills this gap by systematically comparing learning paradigms rather than architectural variants.

\subsection{Learning Paradigms in Healthcare}

\textbf{Supervised Imitation Learning} has been successfully applied to surgical tasks~\cite{murali2015learning, thananjeyan2017multilateral}, offering the advantage of direct learning from expert demonstrations. However, IL is fundamentally limited by demonstration quality and cannot exceed expert performance.

\textbf{Reinforcement Learning} has shown promise in healthcare applications~\cite{gottesman2019guidelines, popova2018deep}, with emerging work in surgical domains~\cite{richter2019open}. World models~\cite{ha2018world} enable safe exploration through simulation, while offline RL~\cite{levine2020offline} allows learning from existing datasets without environment interaction.

\textbf{Model-Based vs. Model-Free RL} represents a fundamental dichotomy in reinforcement learning~\cite{moerland2023model}. Model-based approaches learn environment dynamics for planning and simulation, while model-free methods directly optimize policies through trial and error.

\section{Methodology}

\subsection{Problem Formulation}

We formulate surgical action prediction as a sequential decision problem where different learning paradigms learn policies $\pi: \mathcal{S} \rightarrow \mathcal{A}$ mapping surgical states (frame embeddings) to action predictions. Our goal is to compare how different learning paradigms affect policy quality on identical evaluation tasks.

\subsection{Learning Paradigms}

\subsubsection{Paradigm 1: Supervised Imitation Learning}

Our supervised approach uses autoregressive modeling for causal frame generation followed by action prediction:

\begin{equation}
\mathcal{L}_{IL} = \mathbb{E}_{(s_t, a_t) \sim \mathcal{D}_{expert}}[\ell(f(s_{1:t}), a_t)]
\end{equation}

where $f$ is an autoregressive model (GPT-2 based) that processes frame sequences causally without action conditioning.

\textbf{Implementation}: We employ a 6-layer transformer with autoregressive attention, trained on frame-to-action sequences using binary cross-entropy loss. The model first generates next frame representations, then predicts actions from these representations.

\subsubsection{Paradigm 2: Model-Based RL with World Model Simulation}

This paradigm learns a world model for action-conditioned simulation, then trains RL policies in the simulated environment:

\begin{align}
\text{World Model:} \quad &M(s_t, a_t) \rightarrow (s_{t+1}, r_t) \\
\text{Policy Learning:} \quad &\pi^* = \arg\max_\pi \mathbb{E}_M\left[\sum_t \gamma^t r_t\right]
\end{align}

\textbf{Implementation}: We train a conditional transformer world model that takes current states and actions as input and predicts next states and multiple reward types. RL policies (PPO and A2C) are then trained in this simulated environment.

\subsubsection{Paradigm 3: Model-Free RL on Video Episodes}

This paradigm directly applies RL to video sequences without explicit world modeling:

\begin{equation}
\pi^* = \arg\max_\pi \mathbb{E}_{episodes}\left[\sum_t \gamma^t r_t\right]
\end{equation}

where episodes are extracted from surgical videos with action-based and progression-based rewards.

\textbf{Implementation}: We create an environment that steps through actual video frames, calculating rewards based on expert action matching, surgical progression, and safety considerations. RL policies are trained directly on these video episodes.

\subsection{Fair Evaluation Protocol}

To ensure meaningful comparison across paradigms, we establish:

\begin{itemize}
\item \textbf{Identical Primary Task}: All paradigms evaluated on single-step action prediction using the same test data and metrics.
\item \textbf{Unified Data}: All methods use identical CholecT50 training/test splits and preprocessing.
\item \textbf{Consistent Metrics}: Mean Average Precision (mAP) as primary metric, with exact match accuracy and planning stability as secondary metrics.
\item \textbf{Paradigm-Specific Evaluation}: Secondary analysis respects each paradigm's training environment and capabilities.
\end{itemize}

\section{Experimental Setup}

\subsection{Dataset and Preprocessing}

We use the CholecT50 dataset containing 50 cholecystectomy videos with frame-level annotations. Each frame is represented using 1024-dimensional Swin Transformer features~\cite{liu2021swin}. We extract multiple reward signals for RL training including phase progression, completion rewards, action probability rewards, and safety penalties.

\subsection{Implementation Details}

\textbf{Hardware}: All experiments conducted on NVIDIA RTX 3090 GPUs with consistent computational budgets across paradigms.

\textbf{Supervised IL}: 6-layer transformer, 768 hidden dimensions, trained for convergence using Adam optimizer (lr=1e-4), with autoregressive masking and binary cross-entropy loss.

\textbf{Model-Based RL}: Conditional world model with 6-layer transformer, trained using MSE loss for state prediction and multiple reward heads. RL policies trained using Stable-Baselines3 PPO and A2C for 10,000 timesteps.

\textbf{Model-Free RL}: Direct video environment with action accuracy, phase progression, and safety rewards. PPO and A2C policies trained for 10,000 timesteps with identical hyperparameters.

\section{Results}

\subsection{Primary Performance Comparison}

Table~\ref{tab:main_results} presents our core findings. All paradigms achieve high prediction accuracy (mAP ≥ 0.70), with supervised imitation learning achieving the highest performance.

\input{tables/main_results_real.tex}

The performance differences, while statistically significant in some cases, are relatively small in absolute terms. This suggests that paradigm selection should consider factors beyond pure predictive accuracy.

\subsection{Training Efficiency and Computational Requirements}

Figure~\ref{fig:training_dynamics} shows substantial differences in training efficiency. Supervised IL converges rapidly (2.1 minutes) while RL approaches require significantly more computational resources (12-14 minutes) but offer continued improvement potential through exploration.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.48\textwidth]{figures/training_dynamics.png}
\caption{Training dynamics and efficiency comparison across learning paradigms. (A) Training time requirements, (B) Performance vs efficiency trade-offs, (C) Learning convergence patterns, (D) Computational requirements across multiple dimensions.}
\label{fig:training_dynamics}
\end{figure}

\subsection{Planning Stability Analysis}

Our secondary evaluation reveals interesting differences in planning capabilities. Model-based and model-free RL approaches demonstrate superior planning stability (≥0.999) compared to supervised IL (0.998), suggesting better long-term consistency despite similar single-step performance.

\subsection{Statistical Significance}

Pairwise statistical tests reveal significant differences between supervised IL and RL approaches (p < 0.05), while differences between RL paradigms are not statistically significant. This supports our finding that the choice between model-based and model-free RL should be based on computational and deployment constraints.

\section{Discussion}

\subsection{Paradigm Selection Guidelines}

Based on our comprehensive analysis, we provide evidence-based selection criteria:

\textbf{Choose Supervised Imitation Learning when:}
\begin{itemize}
\item Training time and computational resources are limited
\item High-quality expert demonstrations are abundant  
\item Fastest deployment is critical
\item Single-step prediction accuracy is the primary objective
\end{itemize}

\textbf{Choose Model-Based RL when:}
\begin{itemize}
\item Planning and simulation capabilities are important
\item Exploration beyond expert demonstrations is desired
\item Computational resources are sufficient for world model training
\item Understanding of environment dynamics is valuable
\end{itemize}

\textbf{Choose Model-Free RL when:}
\begin{itemize}
\item Direct learning from video data is preferred
\item Model complexity should be minimized
\item Robust performance across metrics is desired
\item Moderate computational efficiency is acceptable
\end{itemize}

\subsection{Implications for Surgical AI}

Our findings have several important implications:

\textbf{Performance Ceiling}: The similar accuracy across paradigms suggests that surgical action prediction with current datasets may have reached a performance ceiling. Future work should focus on more challenging evaluation scenarios and metrics.

\textbf{Beyond Accuracy}: Paradigm selection should consider training efficiency, computational requirements, deployment constraints, and long-term planning capabilities rather than focusing solely on prediction accuracy.

\textbf{Methodology Matters}: Our fair evaluation approach demonstrates the importance of respecting each paradigm's training environment while enabling meaningful comparisons.

\subsection{Limitations and Future Work}

\textbf{Dataset Scope}: Our evaluation focuses on cholecystectomy procedures. Future work should validate findings across surgical specialties and institutions.

\textbf{Evaluation Metrics}: Current metrics may not fully capture the unique advantages of each paradigm. Novel evaluation protocols could better differentiate paradigm capabilities.

\textbf{Hybrid Approaches}: Future research should explore combinations of paradigms to leverage complementary strengths.

\textbf{Real-World Deployment}: Clinical validation studies are needed to assess paradigm performance in actual surgical settings.

\section{Conclusion}

This paper presents the first systematic empirical comparison of learning paradigms for surgical action prediction. Our comprehensive evaluation reveals that while supervised imitation learning achieves the highest single-step prediction accuracy (mAP = 0.737), all paradigms demonstrate comparable performance with significant differences in training efficiency, computational requirements, and planning capabilities.

The key insight is that paradigm selection should be guided by application-specific requirements and deployment constraints rather than pure performance metrics. Supervised IL excels in efficiency and simplicity, model-based RL provides superior planning and simulation capabilities, and model-free RL offers a balanced approach with robust performance.

Our findings establish the first empirical foundation for learning paradigm selection in surgical AI, enabling more informed decisions in system design and deployment. The open-source implementation facilitates reproducible research and provides benchmarks for future surgical AI development.

Future work should focus on developing evaluation protocols that better capture paradigm-specific advantages, exploring hybrid approaches that combine multiple paradigms, and conducting clinical validation studies to assess real-world deployment effectiveness.

\section*{Acknowledgments}

The authors thank the contributors to the CholecT50 dataset and the open-source communities that enabled this research.

\begin{thebibliography}{00}
\bibitem{maier2017surgical} Maier-Hein, L., et al. "Surgical data science for next-generation interventions." Nature Biomedical Engineering 1.9 (2017): 691-696.
\bibitem{vardazaryan2018systematic} Vardazaryan, A., et al. "Systematic evaluation of surgical workflow modeling." Medical Image Analysis 50 (2018): 59-78.
\bibitem{hussein2017imitation} Hussein, A., et al. "Imitation learning: A survey of learning methods." ACM Computing Surveys 50.2 (2017): 1-35.
\bibitem{sutton2018reinforcement} Sutton, R.S., Barto, A.G. "Reinforcement learning: An introduction." MIT press (2018).
\bibitem{ha2018world} Ha, D., Schmidhuber, J. "World models." arXiv preprint arXiv:1803.10122 (2018).
\bibitem{levine2020offline} Levine, S., et al. "Offline reinforcement learning: Tutorial, review, and perspectives on open problems." arXiv preprint arXiv:2005.01643 (2020).
\bibitem{nwoye2022cholect50} Nwoye, C.I., et al. "CholecT50: An endoscopic image dataset for phase, instrument, action triplet recognition." Medical Image Analysis 78 (2022): 102433.
\bibitem{liu2021swin} Liu, Z., et al. "Swin transformer: Hierarchical vision transformer using shifted windows." ICCV 2021.
\bibitem{gao2022trans} Gao, X., et al. "Trans-SVNet: Accurate phase recognition from surgical videos via hybrid embedding aggregation transformer." MICCAI 2022.
\bibitem{padoy2012statistical} Padoy, N., et al. "Statistical modeling and recognition of surgical workflow." Medical image analysis 16.3 (2012): 632-641.
\bibitem{twinanda2016endonet} Twinanda, A.P., et al. "EndoNet: a deep architecture for recognition tasks on laparoscopic videos." IEEE TMI 36.1 (2016): 86-97.
\bibitem{murali2015learning} Murali, A., et al. "Learning by observation for surgical subtasks: Multilateral cutting of 3D viscoelastic and 2D Orthotropic Tissue Phantoms." ICRA 2015.
\bibitem{thananjeyan2017multilateral} Thananjeyan, B., et al. "Multilateral surgical pattern cutting in 2D orthotropic gauze with deep reinforcement learning policies for tensioning." ICRA 2017.
\bibitem{gottesman2019guidelines} Gottesman, O., et al. "Guidelines for reinforcement learning in healthcare." Nature medicine 25.1 (2019): 16-18.
\bibitem{popova2018deep} Popova, M., et al. "Deep reinforcement learning for de novo drug design." Science advances 4.7 (2018): eaap7885.
\bibitem{richter2019open} Richter, F., et al. "Open-sourced reinforcement learning environments for surgical robotics." arXiv preprint arXiv:1903.02090 (2019).
\bibitem{moerland2023model} Moerland, T.M., et al. "Model-based reinforcement learning: A survey." Foundations and Trends in Machine Learning 16.1 (2023): 1-118.
\end{thebibliography}

\end{document}
