{
  "experiment_name": "fixed_rl_2025-06-13_19-22-25",
  "config": {
    "debug": false,
    "training_mode": "rl",
    "preprocess": {
      "extract_rewards": false,
      "rewards": {
        "grounded": {
          "phase_completion": true,
          "phase_transition": true,
          "phase_progression": true,
          "global_progression": true
        },
        "imitation": {
          "action_distribution": true
        },
        "expert_knowledge": {
          "risk_score": true,
          "frame_risk_agg": "max"
        }
      }
    },
    "experiment": {
      "max_videos": 50,
      "train": {
        "max_videos": 40
      },
      "test": {
        "max_videos": 10
      },
      "dual_world_model": {
        "train": true,
        "best_model_path": null
      },
      "autoregressive_il": {
        "enabled": false,
        "il_model_path": null
      },
      "world_model": {
        "enabled": true,
        "wm_model_path": null
      },
      "rl_experiments": {
        "enabled": true,
        "eval_episodes": 10
      }
    },
    "training": {
      "epochs": 5,
      "batch_size": 16,
      "learning_rate": 0.0001,
      "log_every_n_steps": 50,
      "scheduler": {
        "type": "cosine",
        "warmup_steps": 100
      },
      "weight_decay": 0.01,
      "gradient_clip_val": 1.0,
      "dropout": 0.1,
      "num_workers": 4,
      "pin_memory": true,
      "log_dir": "logs",
      "checkpoint_dir": "checkpoints",
      "eval_epoch_interval": 1,
      "save_model": true
    },
    "evaluation": {
      "prediction_horizon": 15,
      "supervised": {
        "action_prediction": true
      },
      "rl": {
        "rollout_horizon": 15,
        "use_best_actions": true
      },
      "comparison": {
        "statistical_tests": true,
        "effect_size_threshold": 0.2
      },
      "world_model": {
        "use_memory": false,
        "overall_horizon": 1
      }
    },
    "rl_training": {
      "outcome_based_rewards": true,
      "rl_horizon": 30,
      "reward_mode": "dense",
      "normalize_rewards": true,
      "early_termination": true,
      "timesteps": 25000,
      "reward_weights": {
        "expert_matching": 10.0,
        "action_sparsity": 1.0,
        "world_model_rewards": 0.5,
        "completion_bonus": 5.0,
        "consistency_bonus": 1.0,
        "phase_completion": 1.0,
        "risk_penalty": -0.5
      },
      "ppo": {
        "learning_rate": "5e-5",
        "n_steps": 512,
        "batch_size": 64,
        "n_epochs": 10,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "clip_range": 0.1,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      },
      "a2c": {
        "learning_rate": "1e-4",
        "n_steps": 32,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    "data": {
      "context_length": 20,
      "train_shift": 1,
      "padding_value": 0.0,
      "max_horizon": 15,
      "paths": {
        "data_dir": "/nfs/home/mboels/datasets/CholecT50",
        "class_labels_file_path": "./data/labels.json",
        "fold": 0,
        "metadata_file": "embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv",
        "video_global_outcome_file": "embeddings_f0_swin_bas_129_with_enhanced_global_metrics.csv"
      },
      "frame_risk_agg": "max"
    },
    "models": {
      "autoregressive_il": {
        "hidden_dim": 768,
        "embedding_dim": 1024,
        "n_layer": 6,
        "num_action_classes": 100,
        "num_phase_classes": 7,
        "dropout": 0.1,
        "max_length": 1024
      },
      "conditional_world_model": {
        "hidden_dim": 768,
        "embedding_dim": 1024,
        "action_embedding_dim": 128,
        "n_layer": 6,
        "num_action_classes": 100,
        "num_phase_classes": 7,
        "dropout": 0.1,
        "max_sequence_length": 512
      }
    },
    "fair_evaluation": {
      "enabled": true,
      "include_traditional_metrics": true,
      "include_clinical_metrics": true,
      "clinical_outcome_weights": {
        "phase_progression": 2.0,
        "innovation": 0.5
      }
    },
    "supervised_learning": {
      "data_augmentation": false
    },
    "research_comparison": {
      "methods": [
        "autoregressive_il",
        "conditional_world_model",
        "direct_video_rl"
      ]
    },
    "advanced": {
      "mixed_precision": false
    },
    "hardware": {
      "persistent_workers": true
    },
    "rl_debugging": {
      "enabled": true,
      "save_training_curves": true,
      "monitor_expert_matching": true,
      "log_action_distributions": true,
      "convergence_analysis": true,
      "episode_log_frequency": 10,
      "eval_frequency": 1000,
      "reward_improvement_threshold": 0.1,
      "expert_matching_threshold": 0.5,
      "debug_dir": "rl_debug",
      "plot_dir": "rl_plots"
    }
  },
  "timestamp": "2025-06-13_19-22-25",
  "results_dir": "results/fixed_rl_2025-06-13_19-22-25",
  "method_1_autoregressive_il": {
    "status": "skipped",
    "reason": "Autoregressive IL disabled in config"
  },
  "method_2_conditional_world_model": {
    "status": "success",
    "world_model_path": "results/fixed_rl_2025-06-13_19-22-25/logs/checkpoints/world_model_best_epoch_1.pt",
    "world_model_evaluation": {
      "overall_metrics": {
        "state_loss": 0.141403464526245,
        "reward_risk_penalty_loss": 0.21600984014215813,
        "reward_phase_completion_loss": 0.010971411174959997,
        "reward_phase_initiation_loss": 0.016254484854290614,
        "reward_phase_progression_loss": 0.06397056947004552,
        "reward_action_probability_loss": 0.01593664938634771,
        "total_reward_loss": 0.06462859200208933,
        "phase_loss": 0.9628522894482241,
        "total_loss": 1.0348937208622524
      },
      "model_type": "ConditionalWorldModel",
      "evaluation_summary": {
        "best_metric": "state_loss",
        "best_value": 0.141403464526245,
        "strength": "Action-conditioned state-reward prediction",
        "architecture": "ConditionalWorldModel with action conditioning"
      }
    },
    "world_model_pretrained": null,
    "rl_models": {
      "world_model_ppo": {
        "algorithm": "PPO_WorldModel",
        "mean_reward": 262.60562719999996,
        "std_reward": 164.5987931069288,
        "model_path": "results/fixed_rl_2025-06-13_19-22-25/logs/rl_training/ppo_world_model_final.zip",
        "status": "success",
        "training_timesteps": 25000,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": -720.0,
          "std_reward": 0.0,
          "episodes": 836,
          "last_reward": -720.0,
          "reward_trend": "stable",
          "avg_expert_matching": 0.6809560578562395,
          "std_expert_matching": 0.01004707689393925,
          "last_expert_matching": 0.6824137931034485
        },
        "monitoring_data": [
          {
            "timestep": 1250,
            "mean_reward": -423.9960446,
            "std_reward": 174.39809832482823
          },
          {
            "timestep": 2500,
            "mean_reward": -511.68563720000003,
            "std_reward": 9.79556907019876
          },
          {
            "timestep": 3750,
            "mean_reward": -506.9095968,
            "std_reward": 2.583274841883181
          },
          {
            "timestep": 5000,
            "mean_reward": -504.027733,
            "std_reward": 3.9802533348676055
          },
          {
            "timestep": 6250,
            "mean_reward": -504.34562919999996,
            "std_reward": 9.170297241805025
          },
          {
            "timestep": 7500,
            "mean_reward": -506.3755356,
            "std_reward": 2.3000406496517076
          },
          {
            "timestep": 8750,
            "mean_reward": -501.8099314,
            "std_reward": 8.51557373844125
          },
          {
            "timestep": 10000,
            "mean_reward": 70.4483854,
            "std_reward": 269.88959553747395
          },
          {
            "timestep": 11250,
            "mean_reward": -465.82647899999995,
            "std_reward": 41.9091041614792
          },
          {
            "timestep": 12500,
            "mean_reward": -291.773996,
            "std_reward": 254.26218964405524
          },
          {
            "timestep": 13750,
            "mean_reward": -271.8940022,
            "std_reward": 246.20201797363663
          },
          {
            "timestep": 15000,
            "mean_reward": 17.98559480000001,
            "std_reward": 267.68809206271936
          },
          {
            "timestep": 16250,
            "mean_reward": 1.1606839999999976,
            "std_reward": 257.6740449652197
          },
          {
            "timestep": 17500,
            "mean_reward": -49.5654612,
            "std_reward": 295.1819097371344
          },
          {
            "timestep": 18750,
            "mean_reward": -119.40853179999996,
            "std_reward": 297.0992313080288
          },
          {
            "timestep": 20000,
            "mean_reward": 87.11759660000003,
            "std_reward": 167.28609153791402
          },
          {
            "timestep": 21250,
            "mean_reward": 86.7275108,
            "std_reward": 182.279353807406
          },
          {
            "timestep": 22500,
            "mean_reward": 221.45173519999997,
            "std_reward": 75.76790721686834
          },
          {
            "timestep": 23750,
            "mean_reward": 354.11516819999997,
            "std_reward": 170.9569693698982
          },
          {
            "timestep": 25000,
            "mean_reward": 246.1956784,
            "std_reward": 124.86750600249258
          }
        ],
        "expert_matching_enabled": true,
        "reward_design": "expert_demonstration_matching",
        "optimal_threshold": 0.3,
        "threshold_map": 0.375
      },
      "direct_video_ppo": {
        "algorithm": "PPO_DirectVideo",
        "status": "failed",
        "error": "No module named 'rl_environment'"
      }
    },
    "model_type": "ConditionalWorldModel",
    "approach": "FIXED: Action-conditioned world model + improved RL (TRAINED)",
    "method_description": "World model-based RL with fixed rewards and debugging (trained WM)",
    "improvements": [
      "Expert demonstration matching rewards",
      "Proper action space handling",
      "Enhanced monitoring and debugging",
      "Optimized hyperparameters",
      "Trained world model from scratch"
    ]
  },
  "method_3_direct_video_rl": {
    "status": "success",
    "rl_models": {
      "ppo": {
        "algorithm": "PPO_DirectVideo",
        "approach": "Direct RL on video sequences (no world model)",
        "mean_reward": 48.1357454,
        "std_reward": 4.636147126680024,
        "model_path": "results/fixed_rl_2025-06-13_19-22-25/logs/direct_video_rl/ppo_direct_video.zip",
        "status": "success",
        "training_timesteps": 25000,
        "uses_world_model": false,
        "uses_real_frames": true,
        "simulation_based": false,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": 36.23437069055315,
          "episodes": 100,
          "last_length": 30,
          "last_reward": 42.176366237482114,
          "using_real_frames": true
        }
      },
      "a2c": {
        "algorithm": "A2C_DirectVideo",
        "approach": "Direct RL on video sequences (no world model)",
        "mean_reward": 43.0037172,
        "std_reward": 2.0584571092401625,
        "model_path": "results/fixed_rl_2025-06-13_19-22-25/logs/direct_video_rl/a2c_direct_video.zip",
        "status": "success",
        "training_timesteps": 25000,
        "uses_world_model": false,
        "uses_real_frames": true,
        "simulation_based": false,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": 34.34069573399902,
          "episodes": 100,
          "last_length": 30,
          "last_reward": 41.867619047619044,
          "using_real_frames": true
        }
      }
    },
    "model_type": "DirectVideoRL",
    "approach": "FIXED: Direct RL on video sequences with improved rewards",
    "method_description": "Model-free RL on offline video episodes with fixed reward design",
    "improvements": [
      "Expert demonstration matching rewards",
      "Proper continuous action space [0,1]",
      "Better episode termination",
      "Meaningful reward functions"
    ]
  },
  "comprehensive_evaluation": {
    "evaluator": "<evaluation.integrated_evaluation.IntegratedEvaluationFramework object at 0x7fac794e01c0>",
    "results": {
      "status": "success",
      "evaluation_type": "comprehensive_evaluation_with_proper_batches",
      "num_models": 3,
      "num_videos": 10,
      "horizon": 15,
      "video_results": {
        "VID02": {
          "video_id": "VID02",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.049638947521280126,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.6446525853671661,
                "mAP_present_only_with_null_verb": 0.04309583931519388,
                "mAP_freq_weighted_with_null_verb": 0.20537965690440121,
                "mAP_sample_wise_with_null_verb": 0.11984630742819222,
                "mAP_standard_all_actions": 0.6446525853671661,
                "mAP_present_only_all_actions": 0.04309583931519388,
                "mAP_freq_weighted_all_actions": 0.20537965690440121,
                "mAP_sample_wise_all_actions": 0.11984630742819222,
                "exact_match_with_null_verb": 0.0003522367030644593,
                "hamming_accuracy_with_null_verb": 0.9575132088763649,
                "precision_with_null_verb": 0.5232938130224276,
                "recall_with_null_verb": 0.5509294578170989,
                "f1_with_null_verb": 0.5301464591057772,
                "num_predictions": 2839,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6849988354318706,
                "mAP_present_only": 0.049638947521280126,
                "mAP_freq_weighted": 0.2169444491955405,
                "exact_match": 0.0007044734061289186,
                "hamming_accuracy": 0.9557043609901599,
                "precision": 0.5233391416404671,
                "recall": 0.5540849696067891,
                "f1": 0.5304380177127525,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.06295342555927916,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.49840599628250537,
                "mAP_present_only_with_null_verb": 0.05413528318383914,
                "mAP_freq_weighted_with_null_verb": 0.2328372968303212,
                "mAP_sample_wise_with_null_verb": 0.03281471996997284,
                "mAP_standard_all_actions": 0.49840599628250537,
                "mAP_present_only_all_actions": 0.05413528318383914,
                "mAP_freq_weighted_all_actions": 0.2328372968303212,
                "mAP_sample_wise_all_actions": 0.03281471996997284,
                "exact_match_with_null_verb": 0.0003522367030644593,
                "hamming_accuracy_with_null_verb": 0.9650968650933427,
                "precision_with_null_verb": 0.49441289890062823,
                "recall_with_null_verb": 0.4919915848606229,
                "f1_with_null_verb": 0.49312884847094773,
                "num_predictions": 2839,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5293903820814874,
                "mAP_present_only": 0.06295342555927916,
                "mAP_freq_weighted": 0.24565174022987446,
                "exact_match": 0.0007044734061289186,
                "hamming_accuracy": 0.9637645859719859,
                "precision": 0.4943950446212934,
                "recall": 0.49147651662742203,
                "f1": 0.49283302037920035,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08025964468724027,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.6832822615440733,
                "mAP_present_only_with_null_verb": 0.06847723983550957,
                "mAP_freq_weighted_with_null_verb": 0.28179087078495235,
                "mAP_sample_wise_with_null_verb": 0.09225772632115983,
                "mAP_standard_all_actions": 0.6832822615440733,
                "mAP_present_only_all_actions": 0.06847723983550957,
                "mAP_freq_weighted_all_actions": 0.28179087078495235,
                "mAP_sample_wise_all_actions": 0.09225772632115983,
                "exact_match_with_null_verb": 0.07713983797111659,
                "hamming_accuracy_with_null_verb": 0.9856533990841846,
                "precision_with_null_verb": 0.4928266995420923,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49638743576234595,
                "num_predictions": 2839,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7260347877791781,
                "mAP_present_only": 0.08025964468724027,
                "mAP_freq_weighted": 0.2976219209803726,
                "exact_match": 0.11165903487143361,
                "hamming_accuracy": 0.9856332391537326,
                "precision": 0.4928166195768663,
                "recall": 0.5,
                "f1": 0.49638232263567705,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.049638947521280126,
                "exact_match": 0.0007044734061289186,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.06295342555927916,
                "exact_match": 0.0007044734061289186,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08025964468724027,
                "exact_match": 0.11165903487143361,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID06": {
          "video_id": "VID06",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.06440253217433559,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.6884522828122751,
                "mAP_present_only_with_null_verb": 0.061507609374250156,
                "mAP_freq_weighted_with_null_verb": 0.3431129524625567,
                "mAP_sample_wise_with_null_verb": 0.08899499165439656,
                "mAP_standard_all_actions": 0.6884522828122751,
                "mAP_present_only_all_actions": 0.061507609374250156,
                "mAP_freq_weighted_all_actions": 0.3431129524625567,
                "mAP_sample_wise_all_actions": 0.08899499165439656,
                "exact_match_with_null_verb": 0.00046446818392940084,
                "hamming_accuracy_with_null_verb": 0.9498931723176962,
                "precision_with_null_verb": 0.5102620568252829,
                "recall_with_null_verb": 0.5204534611161944,
                "f1_with_null_verb": 0.5120591819210882,
                "num_predictions": 2153,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "30",
                "action_sparsity_with_null_verb": 0.7,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7093028280482204,
                "mAP_present_only": 0.06440253217433559,
                "mAP_freq_weighted": 0.36385489524131104,
                "exact_match": 0.00046446818392940084,
                "hamming_accuracy": 0.9482068563409789,
                "precision": 0.5104923361547326,
                "recall": 0.522422196651474,
                "f1": 0.5123038703785934,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08078579158680335,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.5825093331108051,
                "mAP_present_only_with_null_verb": 0.07503111036935065,
                "mAP_freq_weighted_with_null_verb": 0.3621431181163669,
                "mAP_sample_wise_with_null_verb": 0.034940228067077916,
                "mAP_standard_all_actions": 0.5825093331108051,
                "mAP_present_only_all_actions": 0.07503111036935065,
                "mAP_freq_weighted_all_actions": 0.3621431181163669,
                "mAP_sample_wise_all_actions": 0.034940228067077916,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9605434277751974,
                "precision_with_null_verb": 0.4949188754098516,
                "recall_with_null_verb": 0.4934969838004242,
                "f1_with_null_verb": 0.49413779226985555,
                "num_predictions": 2153,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "30",
                "action_sparsity_with_null_verb": 0.7,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5968130912899668,
                "mAP_present_only": 0.08078579158680335,
                "mAP_freq_weighted": 0.38564323379481963,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9593837396606417,
                "precision": 0.49503843554170146,
                "recall": 0.4931587926086573,
                "f1": 0.49397510207906387,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08943954846571397,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.7243199896190731,
                "mAP_present_only_with_null_verb": 0.08106663206357709,
                "mAP_freq_weighted_with_null_verb": 0.4549785029697269,
                "mAP_sample_wise_with_null_verb": 0.07725869627805673,
                "mAP_standard_all_actions": 0.7243199896190731,
                "mAP_present_only_all_actions": 0.08106663206357709,
                "mAP_freq_weighted_all_actions": 0.4549785029697269,
                "mAP_sample_wise_all_actions": 0.07725869627805673,
                "exact_match_with_null_verb": 0.030190431955411056,
                "hamming_accuracy_with_null_verb": 0.9825963771481654,
                "precision_with_null_verb": 0.4912981885740827,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4956109011767517,
                "num_predictions": 2153,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "30",
                "action_sparsity_with_null_verb": 0.7,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7481428538309421,
                "mAP_present_only": 0.08943954846571397,
                "mAP_freq_weighted": 0.4872373683517888,
                "exact_match": 0.04412447747329308,
                "hamming_accuracy": 0.9828443240999694,
                "precision": 0.4914221620499847,
                "recall": 0.5,
                "f1": 0.4956739730669936,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.06440253217433559,
                "exact_match": 0.00046446818392940084,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08078579158680335,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08943954846571397,
                "exact_match": 0.04412447747329308,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID111": {
          "video_id": "VID111",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.04859372193724501,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.7068900418159523,
                "mAP_present_only_with_null_verb": 0.058241523503283966,
                "mAP_freq_weighted_with_null_verb": 0.1882628137959698,
                "mAP_sample_wise_with_null_verb": 0.19007872381040064,
                "mAP_standard_all_actions": 0.7068900418159523,
                "mAP_present_only_all_actions": 0.058241523503283966,
                "mAP_freq_weighted_all_actions": 0.1882628137959698,
                "mAP_sample_wise_all_actions": 0.19007872381040064,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9605547785547786,
                "precision_with_null_verb": 0.5304216873271954,
                "recall_with_null_verb": 0.5771776502087242,
                "f1_with_null_verb": 0.5412480820778793,
                "num_predictions": 2145,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7256898196641609,
                "mAP_present_only": 0.04859372193724501,
                "mAP_freq_weighted": 0.1696749359137733,
                "exact_match": 0.0009324009324009324,
                "hamming_accuracy": 0.9600753856073005,
                "precision": 0.5311355126495587,
                "recall": 0.5933175303887682,
                "f1": 0.5434609716516628,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.06439263096406607,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.567753828299711,
                "mAP_present_only_with_null_verb": 0.06122009758521021,
                "mAP_freq_weighted_with_null_verb": 0.18575035907236073,
                "mAP_sample_wise_with_null_verb": 0.03619482810369531,
                "mAP_standard_all_actions": 0.567753828299711,
                "mAP_present_only_all_actions": 0.06122009758521021,
                "mAP_freq_weighted_all_actions": 0.18575035907236073,
                "mAP_sample_wise_all_actions": 0.03619482810369531,
                "exact_match_with_null_verb": 0.0013986013986013986,
                "hamming_accuracy_with_null_verb": 0.9653473193473193,
                "precision_with_null_verb": 0.493733756169675,
                "recall_with_null_verb": 0.4886606064038665,
                "f1_with_null_verb": 0.4911840822455268,
                "num_predictions": 2145,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5809554869585283,
                "mAP_present_only": 0.06439263096406607,
                "mAP_freq_weighted": 0.2059805334724591,
                "exact_match": 0.009324009324009324,
                "hamming_accuracy": 0.9651738332589397,
                "precision": 0.49436809348405947,
                "recall": 0.48795213952951916,
                "f1": 0.4911391638358764,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.0731439196351103,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.7299260779167825,
                "mAP_present_only_with_null_verb": 0.0687106135061469,
                "mAP_freq_weighted_with_null_verb": 0.13529819539265286,
                "mAP_sample_wise_with_null_verb": 0.09749638341573791,
                "mAP_standard_all_actions": 0.7299260779167825,
                "mAP_present_only_all_actions": 0.0687106135061469,
                "mAP_freq_weighted_all_actions": 0.13529819539265286,
                "mAP_sample_wise_all_actions": 0.09749638341573791,
                "exact_match_with_null_verb": 0.24615384615384617,
                "hamming_accuracy_with_null_verb": 0.9877482517482518,
                "precision_with_null_verb": 0.4938741258741259,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4969181841209085,
                "num_predictions": 2145,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7534957233072102,
                "mAP_present_only": 0.0731439196351103,
                "mAP_freq_weighted": 0.14516105280478644,
                "exact_match": 0.29976689976689974,
                "hamming_accuracy": 0.9890046124088677,
                "precision": 0.49450230620443386,
                "recall": 0.5,
                "f1": 0.49723595724149283,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.04859372193724501,
                "exact_match": 0.0009324009324009324,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.06439263096406607,
                "exact_match": 0.009324009324009324,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.0731439196351103,
                "exact_match": 0.29976689976689974,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID14": {
          "video_id": "VID14",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.03925575319786554,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "36",
                  "subset_action_sparsity": 0.6170212765957447
                },
                "mAP_standard_with_null_verb": 0.575134972978324,
                "mAP_present_only_with_null_verb": 0.03691456823981475,
                "mAP_freq_weighted_with_null_verb": 0.15975665447342727,
                "mAP_sample_wise_with_null_verb": 0.20581957422809574,
                "mAP_standard_all_actions": 0.575134972978324,
                "mAP_present_only_all_actions": 0.03691456823981475,
                "mAP_freq_weighted_all_actions": 0.15975665447342727,
                "mAP_sample_wise_all_actions": 0.20581957422809574,
                "exact_match_with_null_verb": 0.00117096018735363,
                "hamming_accuracy_with_null_verb": 0.9612470725995316,
                "precision_with_null_verb": 0.5498534082669935,
                "recall_with_null_verb": 0.5952076585537837,
                "f1_with_null_verb": 0.5640984005216677,
                "num_predictions": 1708,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "41",
                "action_sparsity_with_null_verb": 0.5900000000000001,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6001405012247145,
                "mAP_present_only": 0.03925575319786554,
                "mAP_freq_weighted": 0.1683128270190394,
                "exact_match": 0.005269320843091335,
                "hamming_accuracy": 0.9597949573969804,
                "precision": 0.549977052611713,
                "recall": 0.6014915896381963,
                "f1": 0.5653130638706596,
                "num_actions_total": 94,
                "num_actions_present": "36",
                "action_sparsity": 0.6170212765957447
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0724088098699824,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "36",
                  "subset_action_sparsity": 0.6170212765957447
                },
                "mAP_standard_with_null_verb": 0.5169093947802537,
                "mAP_present_only_with_null_verb": 0.06563267019574068,
                "mAP_freq_weighted_with_null_verb": 0.1989966256287845,
                "mAP_sample_wise_with_null_verb": 0.03629377362985108,
                "mAP_standard_all_actions": 0.5169093947802537,
                "mAP_present_only_all_actions": 0.06563267019574068,
                "mAP_freq_weighted_all_actions": 0.1989966256287845,
                "mAP_sample_wise_all_actions": 0.03629377362985108,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9594320843091335,
                "precision_with_null_verb": 0.49372448563311977,
                "recall_with_null_verb": 0.4898647306429204,
                "f1_with_null_verb": 0.4916595378946718,
                "num_predictions": 1708,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "41",
                "action_sparsity_with_null_verb": 0.5900000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5383693314395678,
                "mAP_present_only": 0.0724088098699824,
                "mAP_freq_weighted": 0.20995981254894927,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9578578902785391,
                "precision": 0.49372804442696855,
                "recall": 0.4892231232241515,
                "f1": 0.49129738254001615,
                "num_actions_total": 94,
                "num_actions_present": "36",
                "action_sparsity": 0.6170212765957447
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.06531688037304252,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "36",
                  "subset_action_sparsity": 0.6170212765957447
                },
                "mAP_standard_with_null_verb": 0.6145535010548414,
                "mAP_present_only_with_null_verb": 0.059886587938637686,
                "mAP_freq_weighted_with_null_verb": 0.2605610016381722,
                "mAP_sample_wise_with_null_verb": 0.10428458989029209,
                "mAP_standard_all_actions": 0.6145535010548414,
                "mAP_present_only_all_actions": 0.059886587938637686,
                "mAP_freq_weighted_all_actions": 0.2605610016381722,
                "mAP_sample_wise_all_actions": 0.10428458989029209,
                "exact_match_with_null_verb": 0.13056206088992975,
                "hamming_accuracy_with_null_verb": 0.984519906323185,
                "precision_with_null_verb": 0.4922599531615925,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4960997887631433,
                "num_predictions": 1708,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "41",
                "action_sparsity_with_null_verb": 0.5900000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.642036252057761,
                "mAP_present_only": 0.06531688037304252,
                "mAP_freq_weighted": 0.2755098183372358,
                "exact_match": 0.14227166276346603,
                "hamming_accuracy": 0.984547062633913,
                "precision": 0.4922735313169565,
                "recall": 0.5,
                "f1": 0.4961066840749098,
                "num_actions_total": 94,
                "num_actions_present": "36",
                "action_sparsity": 0.6170212765957447
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.03925575319786554,
                "exact_match": 0.005269320843091335,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0724088098699824,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.06531688037304252,
                "exact_match": 0.14227166276346603,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID23": {
          "video_id": "VID23",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.05247306374763834,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.6667405824091187,
                "mAP_present_only_with_null_verb": 0.05400187873909284,
                "mAP_freq_weighted_with_null_verb": 0.15505818405455551,
                "mAP_sample_wise_with_null_verb": 0.08943199814362278,
                "mAP_standard_all_actions": 0.6667405824091187,
                "mAP_present_only_all_actions": 0.05400187873909284,
                "mAP_freq_weighted_all_actions": 0.15505818405455551,
                "mAP_sample_wise_all_actions": 0.08943199814362278,
                "exact_match_with_null_verb": 0.0006116207951070336,
                "hamming_accuracy_with_null_verb": 0.9556758409785933,
                "precision_with_null_verb": 0.5164853265318009,
                "recall_with_null_verb": 0.5322704533276864,
                "f1_with_null_verb": 0.5204374044972079,
                "num_predictions": 1635,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.695364889972751,
                "mAP_present_only": 0.05247306374763834,
                "mAP_freq_weighted": 0.16246422922165799,
                "exact_match": 0.007339449541284404,
                "hamming_accuracy": 0.9543301450972738,
                "precision": 0.516772150361433,
                "recall": 0.5357805875875664,
                "f1": 0.5210485199493043,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0714991050277633,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.5599889429437572,
                "mAP_present_only_with_null_verb": 0.06448046110889408,
                "mAP_freq_weighted_with_null_verb": 0.16111406359397676,
                "mAP_sample_wise_with_null_verb": 0.03123532597506034,
                "mAP_standard_all_actions": 0.5599889429437572,
                "mAP_present_only_all_actions": 0.06448046110889408,
                "mAP_freq_weighted_all_actions": 0.16111406359397676,
                "mAP_sample_wise_all_actions": 0.03123532597506034,
                "exact_match_with_null_verb": 0.003669724770642202,
                "hamming_accuracy_with_null_verb": 0.9643975535168196,
                "precision_with_null_verb": 0.49208053045022293,
                "recall_with_null_verb": 0.4901471987498066,
                "f1_with_null_verb": 0.49110975138412405,
                "num_predictions": 1635,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.583606135433211,
                "mAP_present_only": 0.0714991050277633,
                "mAP_freq_weighted": 0.16940554557849777,
                "exact_match": 0.003669724770642202,
                "hamming_accuracy": 0.9635695230659119,
                "precision": 0.4922918434329459,
                "recall": 0.48952879037392943,
                "f1": 0.49090188345049224,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10510213714624586,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.7191300602040341,
                "mAP_present_only_with_null_verb": 0.09396793614204633,
                "mAP_freq_weighted_with_null_verb": 0.3026254315073626,
                "mAP_sample_wise_with_null_verb": 0.08977909347502354,
                "mAP_standard_all_actions": 0.7191300602040341,
                "mAP_present_only_all_actions": 0.09396793614204633,
                "mAP_freq_weighted_all_actions": 0.3026254315073626,
                "mAP_sample_wise_all_actions": 0.08977909347502354,
                "exact_match_with_null_verb": 0.0581039755351682,
                "hamming_accuracy_with_null_verb": 0.9841651376146789,
                "precision_with_null_verb": 0.49208256880733947,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4960096914099707,
                "num_predictions": 1635,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7524750592106637,
                "mAP_present_only": 0.10510213714624586,
                "mAP_freq_weighted": 0.3232070189418857,
                "exact_match": 0.08440366972477065,
                "hamming_accuracy": 0.9845988678508686,
                "precision": 0.4922994339254343,
                "recall": 0.5,
                "f1": 0.4961198375151223,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.05247306374763834,
                "exact_match": 0.007339449541284404,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0714991050277633,
                "exact_match": 0.003669724770642202,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10510213714624586,
                "exact_match": 0.08440366972477065,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID25": {
          "video_id": "VID25",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.07070140480880081,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7062006612064827,
                "mAP_present_only_with_null_verb": 0.062310235409548574,
                "mAP_freq_weighted_with_null_verb": 0.24573105537221318,
                "mAP_sample_wise_with_null_verb": 0.13992189701758867,
                "mAP_standard_all_actions": 0.7062006612064827,
                "mAP_present_only_all_actions": 0.062310235409548574,
                "mAP_freq_weighted_all_actions": 0.24573105537221318,
                "mAP_sample_wise_all_actions": 0.13992189701758867,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9545796148426491,
                "precision_with_null_verb": 0.5270815935720251,
                "recall_with_null_verb": 0.5537459408360075,
                "f1_with_null_verb": 0.5344481811391778,
                "num_predictions": 2129,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7391992500104769,
                "mAP_present_only": 0.07070140480880081,
                "mAP_freq_weighted": 0.2593294634970879,
                "exact_match": 0.0009394081728511038,
                "hamming_accuracy": 0.9529046700578635,
                "precision": 0.5272554008071102,
                "recall": 0.5576878315204876,
                "f1": 0.5350916452453282,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.12140258679673256,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.6068344372272503,
                "mAP_present_only_with_null_verb": 0.10320937395096268,
                "mAP_freq_weighted_with_null_verb": 0.3141389472728241,
                "mAP_sample_wise_with_null_verb": 0.03006366539990433,
                "mAP_standard_all_actions": 0.6068344372272503,
                "mAP_present_only_all_actions": 0.10320937395096268,
                "mAP_freq_weighted_all_actions": 0.3141389472728241,
                "mAP_sample_wise_all_actions": 0.03006366539990433,
                "exact_match_with_null_verb": 0.008454673555659934,
                "hamming_accuracy_with_null_verb": 0.9628980742132457,
                "precision_with_null_verb": 0.49154074493235955,
                "recall_with_null_verb": 0.4895616414802361,
                "f1_with_null_verb": 0.4905491970586335,
                "num_predictions": 2129,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6335048332205466,
                "mAP_present_only": 0.12140258679673256,
                "mAP_freq_weighted": 0.33300086685571856,
                "exact_match": 0.008454673555659934,
                "hamming_accuracy": 0.9616941326964012,
                "precision": 0.49158386758958905,
                "recall": 0.4888965660896912,
                "f1": 0.490236534160668,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.1301065958933673,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7682768576939297,
                "mAP_present_only_with_null_verb": 0.10875714497665229,
                "mAP_freq_weighted_with_null_verb": 0.40594182476073637,
                "mAP_sample_wise_with_null_verb": 0.10478601559032125,
                "mAP_standard_all_actions": 0.7682768576939297,
                "mAP_present_only_all_actions": 0.10875714497665229,
                "mAP_freq_weighted_all_actions": 0.40594182476073637,
                "mAP_sample_wise_all_actions": 0.10478601559032125,
                "exact_match_with_null_verb": 0.04744011272898074,
                "hamming_accuracy_with_null_verb": 0.9834288398309066,
                "precision_with_null_verb": 0.4917144199154533,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.495822597756896,
                "num_predictions": 2129,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8056621118485183,
                "mAP_present_only": 0.1301065958933673,
                "mAP_freq_weighted": 0.4321584954106256,
                "exact_match": 0.07045561296383279,
                "hamming_accuracy": 0.9835353727151894,
                "precision": 0.4917676863575947,
                "recall": 0.5,
                "f1": 0.4958496764133143,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.07070140480880081,
                "exact_match": 0.0009394081728511038,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.12140258679673256,
                "exact_match": 0.008454673555659934,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.1301065958933673,
                "exact_match": 0.07045561296383279,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID50": {
          "video_id": "VID50",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09749479288079936,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.7876443853806294,
                "mAP_present_only_with_null_verb": 0.09802436322571959,
                "mAP_freq_weighted_with_null_verb": 0.37958168361145245,
                "mAP_sample_wise_with_null_verb": 0.04121947136049738,
                "mAP_standard_all_actions": 0.7876443853806294,
                "mAP_present_only_all_actions": 0.09802436322571959,
                "mAP_freq_weighted_all_actions": 0.37958168361145245,
                "mAP_sample_wise_all_actions": 0.04121947136049738,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9486106032906764,
                "precision_with_null_verb": 0.4974330520192917,
                "recall_with_null_verb": 0.49453066914498145,
                "f1_with_null_verb": 0.4946864182675623,
                "num_predictions": 1094,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.8038289009158808,
                "mAP_present_only": 0.09749479288079936,
                "mAP_freq_weighted": 0.39169240362972996,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9464000933525225,
                "precision": 0.4973951303406709,
                "recall": 0.4941608690934811,
                "f1": 0.49425766672489335,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.11465360831861707,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.699172390659199,
                "mAP_present_only_with_null_verb": 0.10651328143999468,
                "mAP_freq_weighted_with_null_verb": 0.36385668655043557,
                "mAP_sample_wise_with_null_verb": 0.05142309721598556,
                "mAP_standard_all_actions": 0.699172390659199,
                "mAP_present_only_all_actions": 0.10651328143999468,
                "mAP_freq_weighted_all_actions": 0.36385668655043557,
                "mAP_sample_wise_all_actions": 0.05142309721598556,
                "exact_match_with_null_verb": 0.0018281535648994515,
                "hamming_accuracy_with_null_verb": 0.966471663619744,
                "precision_with_null_verb": 0.4948555871273575,
                "recall_with_null_verb": 0.49459727385377944,
                "f1_with_null_verb": 0.494724321100433,
                "num_predictions": 1094,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7003665716287009,
                "mAP_present_only": 0.11465360831861707,
                "mAP_freq_weighted": 0.3812175065876256,
                "exact_match": 0.005484460694698354,
                "hamming_accuracy": 0.9652845307090901,
                "precision": 0.4948002693893116,
                "recall": 0.494230665150263,
                "f1": 0.49450567534865336,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10858291026673246,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.8383772934905117,
                "mAP_present_only_with_null_verb": 0.10209607494728788,
                "mAP_freq_weighted_with_null_verb": 0.47335992064796145,
                "mAP_sample_wise_with_null_verb": 0.08796204330847988,
                "mAP_standard_all_actions": 0.8383772934905117,
                "mAP_present_only_all_actions": 0.10209607494728788,
                "mAP_freq_weighted_all_actions": 0.47335992064796145,
                "mAP_sample_wise_all_actions": 0.08796204330847988,
                "exact_match_with_null_verb": 0.05758683729433273,
                "hamming_accuracy_with_null_verb": 0.9835466179159049,
                "precision_with_null_verb": 0.49177330895795246,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49585253456221196,
                "num_predictions": 1094,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.848269431534763,
                "mAP_present_only": 0.10858291026673246,
                "mAP_freq_weighted": 0.49618673236857835,
                "exact_match": 0.07221206581352833,
                "hamming_accuracy": 0.983449375705006,
                "precision": 0.491724687852503,
                "recall": 0.5,
                "f1": 0.49582781781634555,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09749479288079936,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.11465360831861707,
                "exact_match": 0.005484460694698354,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10858291026673246,
                "exact_match": 0.07221206581352833,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID51": {
          "video_id": "VID51",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.059055726872932424,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.6767117287706257,
                "mAP_present_only_with_null_verb": 0.05762665093319211,
                "mAP_freq_weighted_with_null_verb": 0.2479860669889828,
                "mAP_sample_wise_with_null_verb": 0.0981011517754135,
                "mAP_standard_all_actions": 0.6767117287706257,
                "mAP_present_only_all_actions": 0.05762665093319211,
                "mAP_freq_weighted_all_actions": 0.2479860669889828,
                "mAP_sample_wise_all_actions": 0.0981011517754135,
                "exact_match_with_null_verb": 0.00033967391304347825,
                "hamming_accuracy_with_null_verb": 0.9571942934782609,
                "precision_with_null_verb": 0.5204543652198657,
                "recall_with_null_verb": 0.5412720343884735,
                "f1_with_null_verb": 0.5258728769000166,
                "num_predictions": 2944,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.717205717499472,
                "mAP_present_only": 0.059055726872932424,
                "mAP_freq_weighted": 0.26709400611934947,
                "exact_match": 0.00033967391304347825,
                "hamming_accuracy": 0.9560628179925994,
                "precision": 0.5208380224095049,
                "recall": 0.5463632205580198,
                "f1": 0.5268151630847618,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.07900823623710312,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.5503576658183185,
                "mAP_present_only_with_null_verb": 0.0701988476493739,
                "mAP_freq_weighted_with_null_verb": 0.2720152661753746,
                "mAP_sample_wise_with_null_verb": 0.027269220474898416,
                "mAP_standard_all_actions": 0.5503576658183185,
                "mAP_present_only_all_actions": 0.0701988476493739,
                "mAP_freq_weighted_all_actions": 0.2720152661753746,
                "mAP_sample_wise_all_actions": 0.027269220474898416,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9611073369565217,
                "precision_with_null_verb": 0.4926812645165623,
                "recall_with_null_verb": 0.4886065486303719,
                "f1_with_null_verb": 0.4906072672524614,
                "num_predictions": 2944,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5733638049967071,
                "mAP_present_only": 0.07900823623710312,
                "mAP_freq_weighted": 0.2971707322172723,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9601822675763182,
                "precision": 0.49297286700392784,
                "recall": 0.4879160465969188,
                "f1": 0.49038705668904065,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10407948449592874,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.736381446421918,
                "mAP_present_only_with_null_verb": 0.09097050490316577,
                "mAP_freq_weighted_with_null_verb": 0.35981864522021045,
                "mAP_sample_wise_with_null_verb": 0.06437829798576751,
                "mAP_standard_all_actions": 0.736381446421918,
                "mAP_present_only_all_actions": 0.09097050490316577,
                "mAP_freq_weighted_all_actions": 0.35981864522021045,
                "mAP_sample_wise_all_actions": 0.06437829798576751,
                "exact_match_with_null_verb": 0.08186141304347826,
                "hamming_accuracy_with_null_verb": 0.9848539402173913,
                "precision_with_null_verb": 0.4924269701086956,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49618459084678296,
                "num_predictions": 2944,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7712543364670457,
                "mAP_present_only": 0.10407948449592874,
                "mAP_freq_weighted": 0.39426846649544833,
                "exact_match": 0.09612771739130435,
                "hamming_accuracy": 0.9854446114708603,
                "precision": 0.49272230573543013,
                "recall": 0.5,
                "f1": 0.4963344763069576,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.059055726872932424,
                "exact_match": 0.00033967391304347825,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.07900823623710312,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10407948449592874,
                "exact_match": 0.09612771739130435,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID66": {
          "video_id": "VID66",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08201972353420502,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.7274135540866843,
                "mAP_present_only_with_null_verb": 0.07571110472471387,
                "mAP_freq_weighted_with_null_verb": 0.2968029879138433,
                "mAP_sample_wise_with_null_verb": 0.12517502189199717,
                "mAP_standard_all_actions": 0.7274135540866843,
                "mAP_present_only_all_actions": 0.07571110472471387,
                "mAP_freq_weighted_all_actions": 0.2968029879138433,
                "mAP_sample_wise_all_actions": 0.12517502189199717,
                "exact_match_with_null_verb": 0.0010964912280701754,
                "hamming_accuracy_with_null_verb": 0.9559978070175439,
                "precision_with_null_verb": 0.5343846562780765,
                "recall_with_null_verb": 0.5718558551122843,
                "f1_with_null_verb": 0.5447058245322574,
                "num_predictions": 1824,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7514935581987671,
                "mAP_present_only": 0.08201972353420502,
                "mAP_freq_weighted": 0.3052980295638841,
                "exact_match": 0.0010964912280701754,
                "hamming_accuracy": 0.9537315696155282,
                "precision": 0.5342258166798335,
                "recall": 0.5736985384967795,
                "f1": 0.5446909930650007,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.11853586452217979,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.6341422788243272,
                "mAP_present_only_with_null_verb": 0.10496642967098774,
                "mAP_freq_weighted_with_null_verb": 0.2896857035368473,
                "mAP_sample_wise_with_null_verb": 0.038238282215524055,
                "mAP_standard_all_actions": 0.6341422788243272,
                "mAP_present_only_all_actions": 0.10496642967098774,
                "mAP_freq_weighted_all_actions": 0.2896857035368473,
                "mAP_sample_wise_all_actions": 0.038238282215524055,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9655646929824562,
                "precision_with_null_verb": 0.4947515757075106,
                "recall_with_null_verb": 0.49378752452874847,
                "f1_with_null_verb": 0.49424622899987486,
                "num_predictions": 1824,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6422416733025915,
                "mAP_present_only": 0.11853586452217979,
                "mAP_freq_weighted": 0.29861589195751964,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9638974430757745,
                "precision": 0.4945001257788127,
                "recall": 0.4932831026673152,
                "f1": 0.4938581425751124,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.12212666632386635,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.7948773752061062,
                "mAP_present_only_with_null_verb": 0.1081625008961139,
                "mAP_freq_weighted_with_null_verb": 0.38428772098465597,
                "mAP_sample_wise_with_null_verb": 0.10559264312359565,
                "mAP_standard_all_actions": 0.7948773752061062,
                "mAP_present_only_all_actions": 0.1081625008961139,
                "mAP_freq_weighted_all_actions": 0.38428772098465597,
                "mAP_sample_wise_all_actions": 0.10559264312359565,
                "exact_match_with_null_verb": 0.06578947368421052,
                "hamming_accuracy_with_null_verb": 0.9841611842105263,
                "precision_with_null_verb": 0.49208059210526317,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49600868721868085,
                "num_predictions": 1824,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8132184396433757,
                "mAP_present_only": 0.12212666632386635,
                "mAP_freq_weighted": 0.3962732326883961,
                "exact_match": 0.07291666666666667,
                "hamming_accuracy": 0.9836809443822322,
                "precision": 0.4918404721911161,
                "recall": 0.5,
                "f1": 0.49588667329189623,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08201972353420502,
                "exact_match": 0.0010964912280701754,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.11853586452217979,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.12212666632386635,
                "exact_match": 0.07291666666666667,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID79": {
          "video_id": "VID79",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.049850177977441915,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "31",
                  "subset_action_sparsity": 0.6702127659574468
                },
                "mAP_standard_with_null_verb": 0.6461918601930372,
                "mAP_present_only_with_null_verb": 0.04497738942510337,
                "mAP_freq_weighted_with_null_verb": 0.24041146278309142,
                "mAP_sample_wise_with_null_verb": 0.17636940241105095,
                "mAP_standard_all_actions": 0.6461918601930372,
                "mAP_present_only_all_actions": 0.04497738942510337,
                "mAP_freq_weighted_all_actions": 0.24041146278309142,
                "mAP_sample_wise_all_actions": 0.17636940241105095,
                "exact_match_with_null_verb": 0.00029291154071470416,
                "hamming_accuracy_with_null_verb": 0.9567252489748096,
                "precision_with_null_verb": 0.5453456133081424,
                "recall_with_null_verb": 0.5900952641822577,
                "f1_with_null_verb": 0.5586751026267511,
                "num_predictions": 3414,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "36",
                "action_sparsity_with_null_verb": 0.64,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6760144203968159,
                "mAP_present_only": 0.049850177977441915,
                "mAP_freq_weighted": 0.2523006681423955,
                "exact_match": 0.002050380785002929,
                "hamming_accuracy": 0.9549040870508171,
                "precision": 0.5454043673730575,
                "recall": 0.5948987570484897,
                "f1": 0.5594437866844274,
                "num_actions_total": 94,
                "num_actions_present": "31",
                "action_sparsity": 0.6702127659574468
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0631067678270965,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "31",
                  "subset_action_sparsity": 0.6702127659574468
                },
                "mAP_standard_with_null_verb": 0.5403867401332549,
                "mAP_present_only_with_null_verb": 0.056629833703486114,
                "mAP_freq_weighted_with_null_verb": 0.260277991311172,
                "mAP_sample_wise_with_null_verb": 0.032779705956136655,
                "mAP_standard_all_actions": 0.5403867401332549,
                "mAP_present_only_all_actions": 0.056629833703486114,
                "mAP_freq_weighted_all_actions": 0.260277991311172,
                "mAP_sample_wise_all_actions": 0.032779705956136655,
                "exact_match_with_null_verb": 0.00029291154071470416,
                "hamming_accuracy_with_null_verb": 0.9660281195079086,
                "precision_with_null_verb": 0.4922174664520561,
                "recall_with_null_verb": 0.49188109636417143,
                "f1_with_null_verb": 0.49204889391819917,
                "num_predictions": 3414,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "36",
                "action_sparsity_with_null_verb": 0.64,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5633649979004254,
                "mAP_present_only": 0.0631067678270965,
                "mAP_freq_weighted": 0.2730285304963846,
                "exact_match": 0.00029291154071470416,
                "hamming_accuracy": 0.964778945269167,
                "precision": 0.4921357798922931,
                "recall": 0.4913551185937784,
                "f1": 0.49174344001296866,
                "num_actions_total": 94,
                "num_actions_present": "31",
                "action_sparsity": 0.6702127659574468
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.0801893405474723,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "31",
                  "subset_action_sparsity": 0.6702127659574468
                },
                "mAP_standard_with_null_verb": 0.665774410794667,
                "mAP_present_only_with_null_verb": 0.07159558554074191,
                "mAP_freq_weighted_with_null_verb": 0.31493721371411226,
                "mAP_sample_wise_with_null_verb": 0.1047379910043325,
                "mAP_standard_all_actions": 0.665774410794667,
                "mAP_present_only_all_actions": 0.07159558554074191,
                "mAP_freq_weighted_all_actions": 0.31493721371411226,
                "mAP_sample_wise_all_actions": 0.1047379910043325,
                "exact_match_with_null_verb": 0.04979496192149971,
                "hamming_accuracy_with_null_verb": 0.9833567662565905,
                "precision_with_null_verb": 0.49167838312829526,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4958042763595119,
                "num_predictions": 3414,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "36",
                "action_sparsity_with_null_verb": 0.64,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.6966581867762941,
                "mAP_present_only": 0.0801893405474723,
                "mAP_freq_weighted": 0.33060379456694255,
                "exact_match": 0.06854130052724078,
                "hamming_accuracy": 0.9832136758528711,
                "precision": 0.49160683792643556,
                "recall": 0.5,
                "f1": 0.4957678982472955,
                "num_actions_total": 94,
                "num_actions_present": "31",
                "action_sparsity": 0.6702127659574468
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.049850177977441915,
                "exact_match": 0.002050380785002929,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0631067678270965,
                "exact_match": 0.00029291154071470416,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.0801893405474723,
                "exact_match": 0.06854130052724078,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        }
      },
      "aggregate_results": {
        "single_step_comparison": {
          "WorldModelRL_world_model_ppo": {
            "mean_mAP": 0.061348584465254416,
            "std_mAP": 0.016836825540464142,
            "mean_exact_match": 0.0019136067005802678,
            "std_exact_match": 0.002302172041859016,
            "num_videos": 10,
            "evaluation_type": "single_step_fair_comparison"
          },
          "DirectVideoRL_ppo": {
            "mean_mAP": 0.08487468267096232,
            "std_mAP": 0.022613013320519536,
            "mean_exact_match": 0.002793025329185344,
            "std_exact_match": 0.003528980289926002,
            "num_videos": 10,
            "evaluation_type": "single_step_fair_comparison"
          },
          "DirectVideoRL_a2c": {
            "mean_mAP": 0.09583471278347201,
            "std_mAP": 0.02038792652733529,
            "mean_exact_match": 0.10624791079624359,
            "std_exact_match": 0.06941742420543016,
            "num_videos": 10,
            "evaluation_type": "single_step_fair_comparison"
          }
        },
        "planning_analysis": {},
        "method_rankings": {
          "single_step_ranking": [
            [
              "DirectVideoRL_a2c",
              0.09583471278347201
            ],
            [
              "DirectVideoRL_ppo",
              0.08487468267096232
            ],
            [
              "WorldModelRL_world_model_ppo",
              0.061348584465254416
            ]
          ],
          "planning_ranking": []
        }
      },
      "statistical_tests": {
        "WorldModelRL_world_model_ppo_vs_DirectVideoRL_ppo": {
          "t_statistic": -2.5034264506628263,
          "p_value": 0.022149667304980195,
          "cohens_d": -1.1801265463103032,
          "significant": "True",
          "mean_diff": -0.023526098205707904,
          "method1_mean": 0.061348584465254416,
          "method2_mean": 0.08487468267096232
        },
        "WorldModelRL_world_model_ppo_vs_DirectVideoRL_a2c": {
          "t_statistic": -3.912746220605421,
          "p_value": 0.001020028904390152,
          "cohens_d": -1.8444862571014191,
          "significant": "True",
          "mean_diff": -0.034486128318217596,
          "method1_mean": 0.061348584465254416,
          "method2_mean": 0.09583471278347201
        },
        "DirectVideoRL_ppo_vs_DirectVideoRL_a2c": {
          "t_statistic": -1.0799145832094172,
          "p_value": 0.2944383630929064,
          "cohens_d": -0.509076616593082,
          "significant": "False",
          "mean_diff": -0.010960030112509692,
          "method1_mean": 0.08487468267096232,
          "method2_mean": 0.09583471278347201
        }
      },
      "evaluation_design": {
        "data_handling": "uses_dataloader_batches_like_training",
        "temporal_structure": "maintained_properly",
        "model_interfaces": "consistent_with_training",
        "primary_evaluation": "single_step_action_prediction_with_proper_context",
        "secondary_evaluation": "multi_step_planning_analysis",
        "fairness_approach": "respects_training_paradigms_and_data_structure"
      },
      "timestamp": "2025-06-13 20:11:31.781332"
    },
    "file_paths": {
      "evaluation": "results/fixed_rl_2025-06-13_19-22-25/integrated_evaluation/evaluation_results.json",
      "fair_comparison": "results/fixed_rl_2025-06-13_19-22-25/integrated_evaluation/fair_single_step_comparison.csv",
      "planning_analysis": "results/fixed_rl_2025-06-13_19-22-25/integrated_evaluation/planning_capability_analysis.csv"
    }
  }
}