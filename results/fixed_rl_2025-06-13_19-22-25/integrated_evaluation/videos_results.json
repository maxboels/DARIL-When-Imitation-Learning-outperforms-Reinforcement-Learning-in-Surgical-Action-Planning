{
    "VID02": {
        "video_id": "VID02",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.049638947521280126,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6446525853671661,
                    "mAP_present_only_with_null_verb": 0.04309583931519388,
                    "mAP_freq_weighted_with_null_verb": 0.20537965690440121,
                    "mAP_sample_wise_with_null_verb": 0.11984630742819222,
                    "mAP_standard_all_actions": 0.6446525853671661,
                    "mAP_present_only_all_actions": 0.04309583931519388,
                    "mAP_freq_weighted_all_actions": 0.20537965690440121,
                    "mAP_sample_wise_all_actions": 0.11984630742819222,
                    "exact_match_with_null_verb": 0.0003522367030644593,
                    "hamming_accuracy_with_null_verb": 0.9575132088763649,
                    "precision_with_null_verb": 0.5232938130224276,
                    "recall_with_null_verb": 0.5509294578170989,
                    "f1_with_null_verb": 0.5301464591057772,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6849988354318706,
                    "mAP_present_only": 0.049638947521280126,
                    "mAP_freq_weighted": 0.2169444491955405,
                    "exact_match": 0.0007044734061289186,
                    "hamming_accuracy": 0.9557043609901599,
                    "precision": 0.5233391416404671,
                    "recall": 0.5540849696067891,
                    "f1": 0.5304380177127525,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.06295342555927916,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.49840599628250537,
                    "mAP_present_only_with_null_verb": 0.05413528318383914,
                    "mAP_freq_weighted_with_null_verb": 0.2328372968303212,
                    "mAP_sample_wise_with_null_verb": 0.03281471996997284,
                    "mAP_standard_all_actions": 0.49840599628250537,
                    "mAP_present_only_all_actions": 0.05413528318383914,
                    "mAP_freq_weighted_all_actions": 0.2328372968303212,
                    "mAP_sample_wise_all_actions": 0.03281471996997284,
                    "exact_match_with_null_verb": 0.0003522367030644593,
                    "hamming_accuracy_with_null_verb": 0.9650968650933427,
                    "precision_with_null_verb": 0.49441289890062823,
                    "recall_with_null_verb": 0.4919915848606229,
                    "f1_with_null_verb": 0.49312884847094773,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.5293903820814874,
                    "mAP_present_only": 0.06295342555927916,
                    "mAP_freq_weighted": 0.24565174022987446,
                    "exact_match": 0.0007044734061289186,
                    "hamming_accuracy": 0.9637645859719859,
                    "precision": 0.4943950446212934,
                    "recall": 0.49147651662742203,
                    "f1": 0.49283302037920035,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.08025964468724027,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6832822615440733,
                    "mAP_present_only_with_null_verb": 0.06847723983550957,
                    "mAP_freq_weighted_with_null_verb": 0.28179087078495235,
                    "mAP_sample_wise_with_null_verb": 0.09225772632115983,
                    "mAP_standard_all_actions": 0.6832822615440733,
                    "mAP_present_only_all_actions": 0.06847723983550957,
                    "mAP_freq_weighted_all_actions": 0.28179087078495235,
                    "mAP_sample_wise_all_actions": 0.09225772632115983,
                    "exact_match_with_null_verb": 0.07713983797111659,
                    "hamming_accuracy_with_null_verb": 0.9856533990841846,
                    "precision_with_null_verb": 0.4928266995420923,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49638743576234595,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7260347877791781,
                    "mAP_present_only": 0.08025964468724027,
                    "mAP_freq_weighted": 0.2976219209803726,
                    "exact_match": 0.11165903487143361,
                    "hamming_accuracy": 0.9856332391537326,
                    "precision": 0.4928166195768663,
                    "recall": 0.5,
                    "f1": 0.49638232263567705,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.049638947521280126,
                    "exact_match": 0.0007044734061289186,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.06295342555927916,
                    "exact_match": 0.0007044734061289186,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.08025964468724027,
                    "exact_match": 0.11165903487143361,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID06": {
        "video_id": "VID06",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.06440253217433559,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.6884522828122751,
                    "mAP_present_only_with_null_verb": 0.061507609374250156,
                    "mAP_freq_weighted_with_null_verb": 0.3431129524625567,
                    "mAP_sample_wise_with_null_verb": 0.08899499165439656,
                    "mAP_standard_all_actions": 0.6884522828122751,
                    "mAP_present_only_all_actions": 0.061507609374250156,
                    "mAP_freq_weighted_all_actions": 0.3431129524625567,
                    "mAP_sample_wise_all_actions": 0.08899499165439656,
                    "exact_match_with_null_verb": 0.00046446818392940084,
                    "hamming_accuracy_with_null_verb": 0.9498931723176962,
                    "precision_with_null_verb": 0.5102620568252829,
                    "recall_with_null_verb": 0.5204534611161944,
                    "f1_with_null_verb": 0.5120591819210882,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7093028280482204,
                    "mAP_present_only": 0.06440253217433559,
                    "mAP_freq_weighted": 0.36385489524131104,
                    "exact_match": 0.00046446818392940084,
                    "hamming_accuracy": 0.9482068563409789,
                    "precision": 0.5104923361547326,
                    "recall": 0.522422196651474,
                    "f1": 0.5123038703785934,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.08078579158680335,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.5825093331108051,
                    "mAP_present_only_with_null_verb": 0.07503111036935065,
                    "mAP_freq_weighted_with_null_verb": 0.3621431181163669,
                    "mAP_sample_wise_with_null_verb": 0.034940228067077916,
                    "mAP_standard_all_actions": 0.5825093331108051,
                    "mAP_present_only_all_actions": 0.07503111036935065,
                    "mAP_freq_weighted_all_actions": 0.3621431181163669,
                    "mAP_sample_wise_all_actions": 0.034940228067077916,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9605434277751974,
                    "precision_with_null_verb": 0.4949188754098516,
                    "recall_with_null_verb": 0.4934969838004242,
                    "f1_with_null_verb": 0.49413779226985555,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.5968130912899668,
                    "mAP_present_only": 0.08078579158680335,
                    "mAP_freq_weighted": 0.38564323379481963,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9593837396606417,
                    "precision": 0.49503843554170146,
                    "recall": 0.4931587926086573,
                    "f1": 0.49397510207906387,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.08943954846571397,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.7243199896190731,
                    "mAP_present_only_with_null_verb": 0.08106663206357709,
                    "mAP_freq_weighted_with_null_verb": 0.4549785029697269,
                    "mAP_sample_wise_with_null_verb": 0.07725869627805673,
                    "mAP_standard_all_actions": 0.7243199896190731,
                    "mAP_present_only_all_actions": 0.08106663206357709,
                    "mAP_freq_weighted_all_actions": 0.4549785029697269,
                    "mAP_sample_wise_all_actions": 0.07725869627805673,
                    "exact_match_with_null_verb": 0.030190431955411056,
                    "hamming_accuracy_with_null_verb": 0.9825963771481654,
                    "precision_with_null_verb": 0.4912981885740827,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4956109011767517,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7481428538309421,
                    "mAP_present_only": 0.08943954846571397,
                    "mAP_freq_weighted": 0.4872373683517888,
                    "exact_match": 0.04412447747329308,
                    "hamming_accuracy": 0.9828443240999694,
                    "precision": 0.4914221620499847,
                    "recall": 0.5,
                    "f1": 0.4956739730669936,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.06440253217433559,
                    "exact_match": 0.00046446818392940084,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.08078579158680335,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.08943954846571397,
                    "exact_match": 0.04412447747329308,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID111": {
        "video_id": "VID111",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.04859372193724501,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.7068900418159523,
                    "mAP_present_only_with_null_verb": 0.058241523503283966,
                    "mAP_freq_weighted_with_null_verb": 0.1882628137959698,
                    "mAP_sample_wise_with_null_verb": 0.19007872381040064,
                    "mAP_standard_all_actions": 0.7068900418159523,
                    "mAP_present_only_all_actions": 0.058241523503283966,
                    "mAP_freq_weighted_all_actions": 0.1882628137959698,
                    "mAP_sample_wise_all_actions": 0.19007872381040064,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9605547785547786,
                    "precision_with_null_verb": 0.5304216873271954,
                    "recall_with_null_verb": 0.5771776502087242,
                    "f1_with_null_verb": 0.5412480820778793,
                    "num_predictions": 2145,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7256898196641609,
                    "mAP_present_only": 0.04859372193724501,
                    "mAP_freq_weighted": 0.1696749359137733,
                    "exact_match": 0.0009324009324009324,
                    "hamming_accuracy": 0.9600753856073005,
                    "precision": 0.5311355126495587,
                    "recall": 0.5933175303887682,
                    "f1": 0.5434609716516628,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.06439263096406607,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.567753828299711,
                    "mAP_present_only_with_null_verb": 0.06122009758521021,
                    "mAP_freq_weighted_with_null_verb": 0.18575035907236073,
                    "mAP_sample_wise_with_null_verb": 0.03619482810369531,
                    "mAP_standard_all_actions": 0.567753828299711,
                    "mAP_present_only_all_actions": 0.06122009758521021,
                    "mAP_freq_weighted_all_actions": 0.18575035907236073,
                    "mAP_sample_wise_all_actions": 0.03619482810369531,
                    "exact_match_with_null_verb": 0.0013986013986013986,
                    "hamming_accuracy_with_null_verb": 0.9653473193473193,
                    "precision_with_null_verb": 0.493733756169675,
                    "recall_with_null_verb": 0.4886606064038665,
                    "f1_with_null_verb": 0.4911840822455268,
                    "num_predictions": 2145,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.5809554869585283,
                    "mAP_present_only": 0.06439263096406607,
                    "mAP_freq_weighted": 0.2059805334724591,
                    "exact_match": 0.009324009324009324,
                    "hamming_accuracy": 0.9651738332589397,
                    "precision": 0.49436809348405947,
                    "recall": 0.48795213952951916,
                    "f1": 0.4911391638358764,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.0731439196351103,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 25,
                        "subset_action_sparsity": 0.7340425531914894
                    },
                    "mAP_standard_with_null_verb": 0.7299260779167825,
                    "mAP_present_only_with_null_verb": 0.0687106135061469,
                    "mAP_freq_weighted_with_null_verb": 0.13529819539265286,
                    "mAP_sample_wise_with_null_verb": 0.09749638341573791,
                    "mAP_standard_all_actions": 0.7299260779167825,
                    "mAP_present_only_all_actions": 0.0687106135061469,
                    "mAP_freq_weighted_all_actions": 0.13529819539265286,
                    "mAP_sample_wise_all_actions": 0.09749638341573791,
                    "exact_match_with_null_verb": 0.24615384615384617,
                    "hamming_accuracy_with_null_verb": 0.9877482517482518,
                    "precision_with_null_verb": 0.4938741258741259,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4969181841209085,
                    "num_predictions": 2145,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7534957233072102,
                    "mAP_present_only": 0.0731439196351103,
                    "mAP_freq_weighted": 0.14516105280478644,
                    "exact_match": 0.29976689976689974,
                    "hamming_accuracy": 0.9890046124088677,
                    "precision": 0.49450230620443386,
                    "recall": 0.5,
                    "f1": 0.49723595724149283,
                    "num_actions_total": 94,
                    "num_actions_present": 25,
                    "action_sparsity": 0.7340425531914894
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.04859372193724501,
                    "exact_match": 0.0009324009324009324,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.06439263096406607,
                    "exact_match": 0.009324009324009324,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.0731439196351103,
                    "exact_match": 0.29976689976689974,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID14": {
        "video_id": "VID14",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.03925575319786554,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 36,
                        "subset_action_sparsity": 0.6170212765957447
                    },
                    "mAP_standard_with_null_verb": 0.575134972978324,
                    "mAP_present_only_with_null_verb": 0.03691456823981475,
                    "mAP_freq_weighted_with_null_verb": 0.15975665447342727,
                    "mAP_sample_wise_with_null_verb": 0.20581957422809574,
                    "mAP_standard_all_actions": 0.575134972978324,
                    "mAP_present_only_all_actions": 0.03691456823981475,
                    "mAP_freq_weighted_all_actions": 0.15975665447342727,
                    "mAP_sample_wise_all_actions": 0.20581957422809574,
                    "exact_match_with_null_verb": 0.00117096018735363,
                    "hamming_accuracy_with_null_verb": 0.9612470725995316,
                    "precision_with_null_verb": 0.5498534082669935,
                    "recall_with_null_verb": 0.5952076585537837,
                    "f1_with_null_verb": 0.5640984005216677,
                    "num_predictions": 1708,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 41,
                    "action_sparsity_with_null_verb": 0.5900000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6001405012247145,
                    "mAP_present_only": 0.03925575319786554,
                    "mAP_freq_weighted": 0.1683128270190394,
                    "exact_match": 0.005269320843091335,
                    "hamming_accuracy": 0.9597949573969804,
                    "precision": 0.549977052611713,
                    "recall": 0.6014915896381963,
                    "f1": 0.5653130638706596,
                    "num_actions_total": 94,
                    "num_actions_present": 36,
                    "action_sparsity": 0.6170212765957447
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.0724088098699824,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 36,
                        "subset_action_sparsity": 0.6170212765957447
                    },
                    "mAP_standard_with_null_verb": 0.5169093947802537,
                    "mAP_present_only_with_null_verb": 0.06563267019574068,
                    "mAP_freq_weighted_with_null_verb": 0.1989966256287845,
                    "mAP_sample_wise_with_null_verb": 0.03629377362985108,
                    "mAP_standard_all_actions": 0.5169093947802537,
                    "mAP_present_only_all_actions": 0.06563267019574068,
                    "mAP_freq_weighted_all_actions": 0.1989966256287845,
                    "mAP_sample_wise_all_actions": 0.03629377362985108,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9594320843091335,
                    "precision_with_null_verb": 0.49372448563311977,
                    "recall_with_null_verb": 0.4898647306429204,
                    "f1_with_null_verb": 0.4916595378946718,
                    "num_predictions": 1708,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 41,
                    "action_sparsity_with_null_verb": 0.5900000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.5383693314395678,
                    "mAP_present_only": 0.0724088098699824,
                    "mAP_freq_weighted": 0.20995981254894927,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9578578902785391,
                    "precision": 0.49372804442696855,
                    "recall": 0.4892231232241515,
                    "f1": 0.49129738254001615,
                    "num_actions_total": 94,
                    "num_actions_present": 36,
                    "action_sparsity": 0.6170212765957447
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.06531688037304252,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 36,
                        "subset_action_sparsity": 0.6170212765957447
                    },
                    "mAP_standard_with_null_verb": 0.6145535010548414,
                    "mAP_present_only_with_null_verb": 0.059886587938637686,
                    "mAP_freq_weighted_with_null_verb": 0.2605610016381722,
                    "mAP_sample_wise_with_null_verb": 0.10428458989029209,
                    "mAP_standard_all_actions": 0.6145535010548414,
                    "mAP_present_only_all_actions": 0.059886587938637686,
                    "mAP_freq_weighted_all_actions": 0.2605610016381722,
                    "mAP_sample_wise_all_actions": 0.10428458989029209,
                    "exact_match_with_null_verb": 0.13056206088992975,
                    "hamming_accuracy_with_null_verb": 0.984519906323185,
                    "precision_with_null_verb": 0.4922599531615925,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4960997887631433,
                    "num_predictions": 1708,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 41,
                    "action_sparsity_with_null_verb": 0.5900000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.642036252057761,
                    "mAP_present_only": 0.06531688037304252,
                    "mAP_freq_weighted": 0.2755098183372358,
                    "exact_match": 0.14227166276346603,
                    "hamming_accuracy": 0.984547062633913,
                    "precision": 0.4922735313169565,
                    "recall": 0.5,
                    "f1": 0.4961066840749098,
                    "num_actions_total": 94,
                    "num_actions_present": 36,
                    "action_sparsity": 0.6170212765957447
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.03925575319786554,
                    "exact_match": 0.005269320843091335,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.0724088098699824,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.06531688037304252,
                    "exact_match": 0.14227166276346603,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID23": {
        "video_id": "VID23",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.05247306374763834,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.6667405824091187,
                    "mAP_present_only_with_null_verb": 0.05400187873909284,
                    "mAP_freq_weighted_with_null_verb": 0.15505818405455551,
                    "mAP_sample_wise_with_null_verb": 0.08943199814362278,
                    "mAP_standard_all_actions": 0.6667405824091187,
                    "mAP_present_only_all_actions": 0.05400187873909284,
                    "mAP_freq_weighted_all_actions": 0.15505818405455551,
                    "mAP_sample_wise_all_actions": 0.08943199814362278,
                    "exact_match_with_null_verb": 0.0006116207951070336,
                    "hamming_accuracy_with_null_verb": 0.9556758409785933,
                    "precision_with_null_verb": 0.5164853265318009,
                    "recall_with_null_verb": 0.5322704533276864,
                    "f1_with_null_verb": 0.5204374044972079,
                    "num_predictions": 1635,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.695364889972751,
                    "mAP_present_only": 0.05247306374763834,
                    "mAP_freq_weighted": 0.16246422922165799,
                    "exact_match": 0.007339449541284404,
                    "hamming_accuracy": 0.9543301450972738,
                    "precision": 0.516772150361433,
                    "recall": 0.5357805875875664,
                    "f1": 0.5210485199493043,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.0714991050277633,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.5599889429437572,
                    "mAP_present_only_with_null_verb": 0.06448046110889408,
                    "mAP_freq_weighted_with_null_verb": 0.16111406359397676,
                    "mAP_sample_wise_with_null_verb": 0.03123532597506034,
                    "mAP_standard_all_actions": 0.5599889429437572,
                    "mAP_present_only_all_actions": 0.06448046110889408,
                    "mAP_freq_weighted_all_actions": 0.16111406359397676,
                    "mAP_sample_wise_all_actions": 0.03123532597506034,
                    "exact_match_with_null_verb": 0.003669724770642202,
                    "hamming_accuracy_with_null_verb": 0.9643975535168196,
                    "precision_with_null_verb": 0.49208053045022293,
                    "recall_with_null_verb": 0.4901471987498066,
                    "f1_with_null_verb": 0.49110975138412405,
                    "num_predictions": 1635,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.583606135433211,
                    "mAP_present_only": 0.0714991050277633,
                    "mAP_freq_weighted": 0.16940554557849777,
                    "exact_match": 0.003669724770642202,
                    "hamming_accuracy": 0.9635695230659119,
                    "precision": 0.4922918434329459,
                    "recall": 0.48952879037392943,
                    "f1": 0.49090188345049224,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.10510213714624586,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.7191300602040341,
                    "mAP_present_only_with_null_verb": 0.09396793614204633,
                    "mAP_freq_weighted_with_null_verb": 0.3026254315073626,
                    "mAP_sample_wise_with_null_verb": 0.08977909347502354,
                    "mAP_standard_all_actions": 0.7191300602040341,
                    "mAP_present_only_all_actions": 0.09396793614204633,
                    "mAP_freq_weighted_all_actions": 0.3026254315073626,
                    "mAP_sample_wise_all_actions": 0.08977909347502354,
                    "exact_match_with_null_verb": 0.0581039755351682,
                    "hamming_accuracy_with_null_verb": 0.9841651376146789,
                    "precision_with_null_verb": 0.49208256880733947,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4960096914099707,
                    "num_predictions": 1635,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 31,
                    "action_sparsity_with_null_verb": 0.69,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7524750592106637,
                    "mAP_present_only": 0.10510213714624586,
                    "mAP_freq_weighted": 0.3232070189418857,
                    "exact_match": 0.08440366972477065,
                    "hamming_accuracy": 0.9845988678508686,
                    "precision": 0.4922994339254343,
                    "recall": 0.5,
                    "f1": 0.4961198375151223,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.05247306374763834,
                    "exact_match": 0.007339449541284404,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.0714991050277633,
                    "exact_match": 0.003669724770642202,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.10510213714624586,
                    "exact_match": 0.08440366972477065,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID25": {
        "video_id": "VID25",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.07070140480880081,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.7062006612064827,
                    "mAP_present_only_with_null_verb": 0.062310235409548574,
                    "mAP_freq_weighted_with_null_verb": 0.24573105537221318,
                    "mAP_sample_wise_with_null_verb": 0.13992189701758867,
                    "mAP_standard_all_actions": 0.7062006612064827,
                    "mAP_present_only_all_actions": 0.062310235409548574,
                    "mAP_freq_weighted_all_actions": 0.24573105537221318,
                    "mAP_sample_wise_all_actions": 0.13992189701758867,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9545796148426491,
                    "precision_with_null_verb": 0.5270815935720251,
                    "recall_with_null_verb": 0.5537459408360075,
                    "f1_with_null_verb": 0.5344481811391778,
                    "num_predictions": 2129,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7391992500104769,
                    "mAP_present_only": 0.07070140480880081,
                    "mAP_freq_weighted": 0.2593294634970879,
                    "exact_match": 0.0009394081728511038,
                    "hamming_accuracy": 0.9529046700578635,
                    "precision": 0.5272554008071102,
                    "recall": 0.5576878315204876,
                    "f1": 0.5350916452453282,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.12140258679673256,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.6068344372272503,
                    "mAP_present_only_with_null_verb": 0.10320937395096268,
                    "mAP_freq_weighted_with_null_verb": 0.3141389472728241,
                    "mAP_sample_wise_with_null_verb": 0.03006366539990433,
                    "mAP_standard_all_actions": 0.6068344372272503,
                    "mAP_present_only_all_actions": 0.10320937395096268,
                    "mAP_freq_weighted_all_actions": 0.3141389472728241,
                    "mAP_sample_wise_all_actions": 0.03006366539990433,
                    "exact_match_with_null_verb": 0.008454673555659934,
                    "hamming_accuracy_with_null_verb": 0.9628980742132457,
                    "precision_with_null_verb": 0.49154074493235955,
                    "recall_with_null_verb": 0.4895616414802361,
                    "f1_with_null_verb": 0.4905491970586335,
                    "num_predictions": 2129,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6335048332205466,
                    "mAP_present_only": 0.12140258679673256,
                    "mAP_freq_weighted": 0.33300086685571856,
                    "exact_match": 0.008454673555659934,
                    "hamming_accuracy": 0.9616941326964012,
                    "precision": 0.49158386758958905,
                    "recall": 0.4888965660896912,
                    "f1": 0.490236534160668,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.1301065958933673,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 21,
                        "subset_action_sparsity": 0.7765957446808511
                    },
                    "mAP_standard_with_null_verb": 0.7682768576939297,
                    "mAP_present_only_with_null_verb": 0.10875714497665229,
                    "mAP_freq_weighted_with_null_verb": 0.40594182476073637,
                    "mAP_sample_wise_with_null_verb": 0.10478601559032125,
                    "mAP_standard_all_actions": 0.7682768576939297,
                    "mAP_present_only_all_actions": 0.10875714497665229,
                    "mAP_freq_weighted_all_actions": 0.40594182476073637,
                    "mAP_sample_wise_all_actions": 0.10478601559032125,
                    "exact_match_with_null_verb": 0.04744011272898074,
                    "hamming_accuracy_with_null_verb": 0.9834288398309066,
                    "precision_with_null_verb": 0.4917144199154533,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.495822597756896,
                    "num_predictions": 2129,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 26,
                    "action_sparsity_with_null_verb": 0.74,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8056621118485183,
                    "mAP_present_only": 0.1301065958933673,
                    "mAP_freq_weighted": 0.4321584954106256,
                    "exact_match": 0.07045561296383279,
                    "hamming_accuracy": 0.9835353727151894,
                    "precision": 0.4917676863575947,
                    "recall": 0.5,
                    "f1": 0.4958496764133143,
                    "num_actions_total": 94,
                    "num_actions_present": 21,
                    "action_sparsity": 0.7765957446808511
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.07070140480880081,
                    "exact_match": 0.0009394081728511038,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.12140258679673256,
                    "exact_match": 0.008454673555659934,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.1301065958933673,
                    "exact_match": 0.07045561296383279,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID50": {
        "video_id": "VID50",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.09749479288079936,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.7876443853806294,
                    "mAP_present_only_with_null_verb": 0.09802436322571959,
                    "mAP_freq_weighted_with_null_verb": 0.37958168361145245,
                    "mAP_sample_wise_with_null_verb": 0.04121947136049738,
                    "mAP_standard_all_actions": 0.7876443853806294,
                    "mAP_present_only_all_actions": 0.09802436322571959,
                    "mAP_freq_weighted_all_actions": 0.37958168361145245,
                    "mAP_sample_wise_all_actions": 0.04121947136049738,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9486106032906764,
                    "precision_with_null_verb": 0.4974330520192917,
                    "recall_with_null_verb": 0.49453066914498145,
                    "f1_with_null_verb": 0.4946864182675623,
                    "num_predictions": 1094,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 18,
                    "action_sparsity_with_null_verb": 0.8200000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8038289009158808,
                    "mAP_present_only": 0.09749479288079936,
                    "mAP_freq_weighted": 0.39169240362972996,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9464000933525225,
                    "precision": 0.4973951303406709,
                    "recall": 0.4941608690934811,
                    "f1": 0.49425766672489335,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.11465360831861707,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.699172390659199,
                    "mAP_present_only_with_null_verb": 0.10651328143999468,
                    "mAP_freq_weighted_with_null_verb": 0.36385668655043557,
                    "mAP_sample_wise_with_null_verb": 0.05142309721598556,
                    "mAP_standard_all_actions": 0.699172390659199,
                    "mAP_present_only_all_actions": 0.10651328143999468,
                    "mAP_freq_weighted_all_actions": 0.36385668655043557,
                    "mAP_sample_wise_all_actions": 0.05142309721598556,
                    "exact_match_with_null_verb": 0.0018281535648994515,
                    "hamming_accuracy_with_null_verb": 0.966471663619744,
                    "precision_with_null_verb": 0.4948555871273575,
                    "recall_with_null_verb": 0.49459727385377944,
                    "f1_with_null_verb": 0.494724321100433,
                    "num_predictions": 1094,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 18,
                    "action_sparsity_with_null_verb": 0.8200000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7003665716287009,
                    "mAP_present_only": 0.11465360831861707,
                    "mAP_freq_weighted": 0.3812175065876256,
                    "exact_match": 0.005484460694698354,
                    "hamming_accuracy": 0.9652845307090901,
                    "precision": 0.4948002693893116,
                    "recall": 0.494230665150263,
                    "f1": 0.49450567534865336,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.10858291026673246,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 16,
                        "subset_action_sparsity": 0.8297872340425532
                    },
                    "mAP_standard_with_null_verb": 0.8383772934905117,
                    "mAP_present_only_with_null_verb": 0.10209607494728788,
                    "mAP_freq_weighted_with_null_verb": 0.47335992064796145,
                    "mAP_sample_wise_with_null_verb": 0.08796204330847988,
                    "mAP_standard_all_actions": 0.8383772934905117,
                    "mAP_present_only_all_actions": 0.10209607494728788,
                    "mAP_freq_weighted_all_actions": 0.47335992064796145,
                    "mAP_sample_wise_all_actions": 0.08796204330847988,
                    "exact_match_with_null_verb": 0.05758683729433273,
                    "hamming_accuracy_with_null_verb": 0.9835466179159049,
                    "precision_with_null_verb": 0.49177330895795246,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49585253456221196,
                    "num_predictions": 1094,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 18,
                    "action_sparsity_with_null_verb": 0.8200000000000001,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.848269431534763,
                    "mAP_present_only": 0.10858291026673246,
                    "mAP_freq_weighted": 0.49618673236857835,
                    "exact_match": 0.07221206581352833,
                    "hamming_accuracy": 0.983449375705006,
                    "precision": 0.491724687852503,
                    "recall": 0.5,
                    "f1": 0.49582781781634555,
                    "num_actions_total": 94,
                    "num_actions_present": 16,
                    "action_sparsity": 0.8297872340425532
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.09749479288079936,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.11465360831861707,
                    "exact_match": 0.005484460694698354,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.10858291026673246,
                    "exact_match": 0.07221206581352833,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID51": {
        "video_id": "VID51",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.059055726872932424,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 24,
                        "subset_action_sparsity": 0.7446808510638299
                    },
                    "mAP_standard_with_null_verb": 0.6767117287706257,
                    "mAP_present_only_with_null_verb": 0.05762665093319211,
                    "mAP_freq_weighted_with_null_verb": 0.2479860669889828,
                    "mAP_sample_wise_with_null_verb": 0.0981011517754135,
                    "mAP_standard_all_actions": 0.6767117287706257,
                    "mAP_present_only_all_actions": 0.05762665093319211,
                    "mAP_freq_weighted_all_actions": 0.2479860669889828,
                    "mAP_sample_wise_all_actions": 0.0981011517754135,
                    "exact_match_with_null_verb": 0.00033967391304347825,
                    "hamming_accuracy_with_null_verb": 0.9571942934782609,
                    "precision_with_null_verb": 0.5204543652198657,
                    "recall_with_null_verb": 0.5412720343884735,
                    "f1_with_null_verb": 0.5258728769000166,
                    "num_predictions": 2944,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.717205717499472,
                    "mAP_present_only": 0.059055726872932424,
                    "mAP_freq_weighted": 0.26709400611934947,
                    "exact_match": 0.00033967391304347825,
                    "hamming_accuracy": 0.9560628179925994,
                    "precision": 0.5208380224095049,
                    "recall": 0.5463632205580198,
                    "f1": 0.5268151630847618,
                    "num_actions_total": 94,
                    "num_actions_present": 24,
                    "action_sparsity": 0.7446808510638299
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.07900823623710312,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 24,
                        "subset_action_sparsity": 0.7446808510638299
                    },
                    "mAP_standard_with_null_verb": 0.5503576658183185,
                    "mAP_present_only_with_null_verb": 0.0701988476493739,
                    "mAP_freq_weighted_with_null_verb": 0.2720152661753746,
                    "mAP_sample_wise_with_null_verb": 0.027269220474898416,
                    "mAP_standard_all_actions": 0.5503576658183185,
                    "mAP_present_only_all_actions": 0.0701988476493739,
                    "mAP_freq_weighted_all_actions": 0.2720152661753746,
                    "mAP_sample_wise_all_actions": 0.027269220474898416,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9611073369565217,
                    "precision_with_null_verb": 0.4926812645165623,
                    "recall_with_null_verb": 0.4886065486303719,
                    "f1_with_null_verb": 0.4906072672524614,
                    "num_predictions": 2944,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.5733638049967071,
                    "mAP_present_only": 0.07900823623710312,
                    "mAP_freq_weighted": 0.2971707322172723,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9601822675763182,
                    "precision": 0.49297286700392784,
                    "recall": 0.4879160465969188,
                    "f1": 0.49038705668904065,
                    "num_actions_total": 94,
                    "num_actions_present": 24,
                    "action_sparsity": 0.7446808510638299
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.10407948449592874,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 24,
                        "subset_action_sparsity": 0.7446808510638299
                    },
                    "mAP_standard_with_null_verb": 0.736381446421918,
                    "mAP_present_only_with_null_verb": 0.09097050490316577,
                    "mAP_freq_weighted_with_null_verb": 0.35981864522021045,
                    "mAP_sample_wise_with_null_verb": 0.06437829798576751,
                    "mAP_standard_all_actions": 0.736381446421918,
                    "mAP_present_only_all_actions": 0.09097050490316577,
                    "mAP_freq_weighted_all_actions": 0.35981864522021045,
                    "mAP_sample_wise_all_actions": 0.06437829798576751,
                    "exact_match_with_null_verb": 0.08186141304347826,
                    "hamming_accuracy_with_null_verb": 0.9848539402173913,
                    "precision_with_null_verb": 0.4924269701086956,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49618459084678296,
                    "num_predictions": 2944,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 29,
                    "action_sparsity_with_null_verb": 0.71,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7712543364670457,
                    "mAP_present_only": 0.10407948449592874,
                    "mAP_freq_weighted": 0.39426846649544833,
                    "exact_match": 0.09612771739130435,
                    "hamming_accuracy": 0.9854446114708603,
                    "precision": 0.49272230573543013,
                    "recall": 0.5,
                    "f1": 0.4963344763069576,
                    "num_actions_total": 94,
                    "num_actions_present": 24,
                    "action_sparsity": 0.7446808510638299
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.059055726872932424,
                    "exact_match": 0.00033967391304347825,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.07900823623710312,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.10407948449592874,
                    "exact_match": 0.09612771739130435,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID66": {
        "video_id": "VID66",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.08201972353420502,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 20,
                        "subset_action_sparsity": 0.7872340425531915
                    },
                    "mAP_standard_with_null_verb": 0.7274135540866843,
                    "mAP_present_only_with_null_verb": 0.07571110472471387,
                    "mAP_freq_weighted_with_null_verb": 0.2968029879138433,
                    "mAP_sample_wise_with_null_verb": 0.12517502189199717,
                    "mAP_standard_all_actions": 0.7274135540866843,
                    "mAP_present_only_all_actions": 0.07571110472471387,
                    "mAP_freq_weighted_all_actions": 0.2968029879138433,
                    "mAP_sample_wise_all_actions": 0.12517502189199717,
                    "exact_match_with_null_verb": 0.0010964912280701754,
                    "hamming_accuracy_with_null_verb": 0.9559978070175439,
                    "precision_with_null_verb": 0.5343846562780765,
                    "recall_with_null_verb": 0.5718558551122843,
                    "f1_with_null_verb": 0.5447058245322574,
                    "num_predictions": 1824,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 23,
                    "action_sparsity_with_null_verb": 0.77,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7514935581987671,
                    "mAP_present_only": 0.08201972353420502,
                    "mAP_freq_weighted": 0.3052980295638841,
                    "exact_match": 0.0010964912280701754,
                    "hamming_accuracy": 0.9537315696155282,
                    "precision": 0.5342258166798335,
                    "recall": 0.5736985384967795,
                    "f1": 0.5446909930650007,
                    "num_actions_total": 94,
                    "num_actions_present": 20,
                    "action_sparsity": 0.7872340425531915
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.11853586452217979,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 20,
                        "subset_action_sparsity": 0.7872340425531915
                    },
                    "mAP_standard_with_null_verb": 0.6341422788243272,
                    "mAP_present_only_with_null_verb": 0.10496642967098774,
                    "mAP_freq_weighted_with_null_verb": 0.2896857035368473,
                    "mAP_sample_wise_with_null_verb": 0.038238282215524055,
                    "mAP_standard_all_actions": 0.6341422788243272,
                    "mAP_present_only_all_actions": 0.10496642967098774,
                    "mAP_freq_weighted_all_actions": 0.2896857035368473,
                    "mAP_sample_wise_all_actions": 0.038238282215524055,
                    "exact_match_with_null_verb": 0.0,
                    "hamming_accuracy_with_null_verb": 0.9655646929824562,
                    "precision_with_null_verb": 0.4947515757075106,
                    "recall_with_null_verb": 0.49378752452874847,
                    "f1_with_null_verb": 0.49424622899987486,
                    "num_predictions": 1824,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 23,
                    "action_sparsity_with_null_verb": 0.77,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6422416733025915,
                    "mAP_present_only": 0.11853586452217979,
                    "mAP_freq_weighted": 0.29861589195751964,
                    "exact_match": 0.0,
                    "hamming_accuracy": 0.9638974430757745,
                    "precision": 0.4945001257788127,
                    "recall": 0.4932831026673152,
                    "f1": 0.4938581425751124,
                    "num_actions_total": 94,
                    "num_actions_present": 20,
                    "action_sparsity": 0.7872340425531915
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.12212666632386635,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 20,
                        "subset_action_sparsity": 0.7872340425531915
                    },
                    "mAP_standard_with_null_verb": 0.7948773752061062,
                    "mAP_present_only_with_null_verb": 0.1081625008961139,
                    "mAP_freq_weighted_with_null_verb": 0.38428772098465597,
                    "mAP_sample_wise_with_null_verb": 0.10559264312359565,
                    "mAP_standard_all_actions": 0.7948773752061062,
                    "mAP_present_only_all_actions": 0.1081625008961139,
                    "mAP_freq_weighted_all_actions": 0.38428772098465597,
                    "mAP_sample_wise_all_actions": 0.10559264312359565,
                    "exact_match_with_null_verb": 0.06578947368421052,
                    "hamming_accuracy_with_null_verb": 0.9841611842105263,
                    "precision_with_null_verb": 0.49208059210526317,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.49600868721868085,
                    "num_predictions": 1824,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 23,
                    "action_sparsity_with_null_verb": 0.77,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8132184396433757,
                    "mAP_present_only": 0.12212666632386635,
                    "mAP_freq_weighted": 0.3962732326883961,
                    "exact_match": 0.07291666666666667,
                    "hamming_accuracy": 0.9836809443822322,
                    "precision": 0.4918404721911161,
                    "recall": 0.5,
                    "f1": 0.49588667329189623,
                    "num_actions_total": 94,
                    "num_actions_present": 20,
                    "action_sparsity": 0.7872340425531915
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.08201972353420502,
                    "exact_match": 0.0010964912280701754,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.11853586452217979,
                    "exact_match": 0.0,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.12212666632386635,
                    "exact_match": 0.07291666666666667,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID79": {
        "video_id": "VID79",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
                "metrics": {
                    "mAP": 0.049850177977441915,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 31,
                        "subset_action_sparsity": 0.6702127659574468
                    },
                    "mAP_standard_with_null_verb": 0.6461918601930372,
                    "mAP_present_only_with_null_verb": 0.04497738942510337,
                    "mAP_freq_weighted_with_null_verb": 0.24041146278309142,
                    "mAP_sample_wise_with_null_verb": 0.17636940241105095,
                    "mAP_standard_all_actions": 0.6461918601930372,
                    "mAP_present_only_all_actions": 0.04497738942510337,
                    "mAP_freq_weighted_all_actions": 0.24041146278309142,
                    "mAP_sample_wise_all_actions": 0.17636940241105095,
                    "exact_match_with_null_verb": 0.00029291154071470416,
                    "hamming_accuracy_with_null_verb": 0.9567252489748096,
                    "precision_with_null_verb": 0.5453456133081424,
                    "recall_with_null_verb": 0.5900952641822577,
                    "f1_with_null_verb": 0.5586751026267511,
                    "num_predictions": 3414,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 36,
                    "action_sparsity_with_null_verb": 0.64,
                    "task": "single_step_action_prediction",
                    "method_name": "WorldModelRL_world_model_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6760144203968159,
                    "mAP_present_only": 0.049850177977441915,
                    "mAP_freq_weighted": 0.2523006681423955,
                    "exact_match": 0.002050380785002929,
                    "hamming_accuracy": 0.9549040870508171,
                    "precision": 0.5454043673730575,
                    "recall": 0.5948987570484897,
                    "f1": 0.5594437866844274,
                    "num_actions_total": 94,
                    "num_actions_present": 31,
                    "action_sparsity": 0.6702127659574468
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
                "metrics": {
                    "mAP": 0.0631067678270965,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 31,
                        "subset_action_sparsity": 0.6702127659574468
                    },
                    "mAP_standard_with_null_verb": 0.5403867401332549,
                    "mAP_present_only_with_null_verb": 0.056629833703486114,
                    "mAP_freq_weighted_with_null_verb": 0.260277991311172,
                    "mAP_sample_wise_with_null_verb": 0.032779705956136655,
                    "mAP_standard_all_actions": 0.5403867401332549,
                    "mAP_present_only_all_actions": 0.056629833703486114,
                    "mAP_freq_weighted_all_actions": 0.260277991311172,
                    "mAP_sample_wise_all_actions": 0.032779705956136655,
                    "exact_match_with_null_verb": 0.00029291154071470416,
                    "hamming_accuracy_with_null_verb": 0.9660281195079086,
                    "precision_with_null_verb": 0.4922174664520561,
                    "recall_with_null_verb": 0.49188109636417143,
                    "f1_with_null_verb": 0.49204889391819917,
                    "num_predictions": 3414,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 36,
                    "action_sparsity_with_null_verb": 0.64,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_ppo",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.5633649979004254,
                    "mAP_present_only": 0.0631067678270965,
                    "mAP_freq_weighted": 0.2730285304963846,
                    "exact_match": 0.00029291154071470416,
                    "hamming_accuracy": 0.964778945269167,
                    "precision": 0.4921357798922931,
                    "recall": 0.4913551185937784,
                    "f1": 0.49174344001296866,
                    "num_actions_total": 94,
                    "num_actions_present": 31,
                    "action_sparsity": 0.6702127659574468
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
                "metrics": {
                    "mAP": 0.0801893405474723,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 31,
                        "subset_action_sparsity": 0.6702127659574468
                    },
                    "mAP_standard_with_null_verb": 0.665774410794667,
                    "mAP_present_only_with_null_verb": 0.07159558554074191,
                    "mAP_freq_weighted_with_null_verb": 0.31493721371411226,
                    "mAP_sample_wise_with_null_verb": 0.1047379910043325,
                    "mAP_standard_all_actions": 0.665774410794667,
                    "mAP_present_only_all_actions": 0.07159558554074191,
                    "mAP_freq_weighted_all_actions": 0.31493721371411226,
                    "mAP_sample_wise_all_actions": 0.1047379910043325,
                    "exact_match_with_null_verb": 0.04979496192149971,
                    "hamming_accuracy_with_null_verb": 0.9833567662565905,
                    "precision_with_null_verb": 0.49167838312829526,
                    "recall_with_null_verb": 0.5,
                    "f1_with_null_verb": 0.4958042763595119,
                    "num_predictions": 3414,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 36,
                    "action_sparsity_with_null_verb": 0.64,
                    "task": "single_step_action_prediction",
                    "method_name": "DirectVideoRL_a2c",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.6966581867762941,
                    "mAP_present_only": 0.0801893405474723,
                    "mAP_freq_weighted": 0.33060379456694255,
                    "exact_match": 0.06854130052724078,
                    "hamming_accuracy": 0.9832136758528711,
                    "precision": 0.49160683792643556,
                    "recall": 0.5,
                    "f1": 0.4957678982472955,
                    "num_actions_total": 94,
                    "num_actions_present": 31,
                    "action_sparsity": 0.6702127659574468
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": false
            }
        },
        "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
        },
        "summary": {
            "primary_comparison": {
                "WorldModelRL_world_model_ppo": {
                    "mAP": 0.049850177977441915,
                    "exact_match": 0.002050380785002929,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_ppo": {
                    "mAP": 0.0631067678270965,
                    "exact_match": 0.00029291154071470416,
                    "task": "single_step_action_prediction"
                },
                "DirectVideoRL_a2c": {
                    "mAP": 0.0801893405474723,
                    "exact_match": 0.06854130052724078,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    }
}