{
  "experiment_name": "fixed_rl_2025-06-15_07-38-18",
  "config": {
    "debug": false,
    "test_on_train_data": true,
    "training_mode": "rl",
    "preprocess": {
      "extract_rewards": false,
      "rewards": {
        "grounded": {
          "phase_completion": true,
          "phase_transition": true,
          "phase_progression": true,
          "global_progression": true
        },
        "imitation": {
          "action_distribution": true
        },
        "expert_knowledge": {
          "risk_score": true,
          "frame_risk_agg": "max"
        }
      }
    },
    "experiment": {
      "max_videos": 50,
      "train": {
        "max_videos": 40
      },
      "test": {
        "max_videos": 10
      },
      "dual_world_model": {
        "train": true,
        "best_model_path": null
      },
      "autoregressive_il": {
        "enabled": false,
        "il_model_path": null
      },
      "world_model": {
        "enabled": true,
        "wm_model_path": null
      },
      "rl_experiments": {
        "enabled": true,
        "eval_episodes": 10
      }
    },
    "training": {
      "epochs": 5,
      "batch_size": 16,
      "learning_rate": 3e-05,
      "log_every_n_steps": 50,
      "scheduler": {
        "type": "cosine",
        "warmup_steps": 100
      },
      "weight_decay": 0.01,
      "gradient_clip_val": 1.0,
      "dropout": 0.1,
      "num_workers": 4,
      "pin_memory": true,
      "log_dir": "logs",
      "checkpoint_dir": "checkpoints",
      "eval_epoch_interval": 1,
      "save_model": true
    },
    "evaluation": {
      "prediction_horizon": 15,
      "supervised": {
        "action_prediction": true
      },
      "rl": {
        "rollout_horizon": 15,
        "use_best_actions": true
      },
      "comparison": {
        "statistical_tests": true,
        "effect_size_threshold": 0.2
      },
      "world_model": {
        "use_memory": false,
        "overall_horizon": 1
      }
    },
    "rl_training": {
      "outcome_based_rewards": true,
      "rl_horizon": 30,
      "reward_mode": "dense",
      "normalize_rewards": true,
      "early_termination": true,
      "timesteps": 50000,
      "reward_weights": {
        "expert_matching": 10.0,
        "action_sparsity": 1.0,
        "world_model_rewards": 0.5,
        "completion_bonus": 5.0,
        "consistency_bonus": 1.0,
        "phase_completion": 1.0,
        "risk_penalty": -0.5
      },
      "ppo": {
        "learning_rate": "5e-5",
        "n_steps": 512,
        "batch_size": 64,
        "n_epochs": 10,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "clip_range": 0.1,
        "ent_coef": 0.05,
        "vf_coef": 0.5,
        "max_grad_norm": 0.5
      },
      "a2c": {
        "learning_rate": "1e-4",
        "n_steps": 32,
        "gamma": 0.95,
        "gae_lambda": 0.9,
        "ent_coef": 0.05,
        "vf_coef": 0.25,
        "max_grad_norm": 0.5
      }
    },
    "data": {
      "context_length": 20,
      "train_shift": 1,
      "padding_value": 0.0,
      "max_horizon": 15,
      "paths": {
        "data_dir": "/nfs/home/mboels/datasets/CholecT50",
        "class_labels_file_path": "./data/labels.json",
        "fold": 0,
        "metadata_file": "embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv",
        "video_global_outcome_file": "embeddings_f0_swin_bas_129_with_enhanced_global_metrics.csv"
      },
      "frame_risk_agg": "max"
    },
    "models": {
      "autoregressive_il": {
        "hidden_dim": 768,
        "embedding_dim": 1024,
        "n_layer": 6,
        "num_action_classes": 100,
        "num_phase_classes": 7,
        "dropout": 0.1,
        "max_length": 1024
      },
      "conditional_world_model": {
        "hidden_dim": 512,
        "embedding_dim": 1024,
        "action_embedding_dim": 128,
        "n_layer": 4,
        "num_action_classes": 100,
        "num_phase_classes": 7,
        "dropout": 0.1,
        "max_sequence_length": 512
      }
    },
    "fair_evaluation": {
      "enabled": true,
      "include_traditional_metrics": true,
      "include_clinical_metrics": true,
      "clinical_outcome_weights": {
        "phase_progression": 2.0,
        "innovation": 0.5
      }
    },
    "supervised_learning": {
      "data_augmentation": false
    },
    "research_comparison": {
      "methods": [
        "autoregressive_il",
        "conditional_world_model",
        "direct_video_rl"
      ]
    },
    "advanced": {
      "mixed_precision": false
    },
    "hardware": {
      "persistent_workers": true
    },
    "rl_debugging": {
      "enabled": true,
      "save_training_curves": true,
      "monitor_expert_matching": true,
      "log_action_distributions": true,
      "convergence_analysis": true,
      "episode_log_frequency": 10,
      "eval_frequency": 1000,
      "reward_improvement_threshold": 0.1,
      "expert_matching_threshold": 0.5,
      "debug_dir": "rl_debug",
      "plot_dir": "rl_plots"
    }
  },
  "timestamp": "2025-06-15_07-38-18",
  "results_dir": "results/fixed_rl_2025-06-15_07-38-18",
  "method_1_autoregressive_il": {
    "status": "skipped",
    "reason": "Autoregressive IL disabled in config"
  },
  "method_2_conditional_world_model": {
    "status": "success",
    "world_model_path": "results/fixed_rl_2025-06-15_07-38-18/logs/checkpoints/world_model_best_epoch_5.pt",
    "world_model_evaluation": {
      "overall_metrics": {
        "state_loss": 0.09308694892858581,
        "reward_risk_penalty_loss": 0.015456393548711003,
        "reward_phase_completion_loss": 0.006970136062239574,
        "reward_phase_initiation_loss": 0.012114600201205644,
        "reward_phase_progression_loss": 0.014239397326194567,
        "reward_action_probability_loss": 0.004738401293205153,
        "total_reward_loss": 0.01070378586683934,
        "phase_loss": 0.022657980240522756,
        "total_loss": 0.31199740782008045
      },
      "model_type": "ConditionalWorldModel",
      "evaluation_summary": {
        "best_metric": "state_loss",
        "best_value": 0.09308694892858581,
        "strength": "Action-conditioned state-reward prediction",
        "architecture": "ConditionalWorldModel with action conditioning"
      }
    },
    "world_model_pretrained": null,
    "rl_models": {
      "world_model_ppo": {
        "algorithm": "PPO_WorldModel",
        "mean_reward": 14.823908199999996,
        "std_reward": 188.03319650389773,
        "model_path": "results/fixed_rl_2025-06-15_07-38-18/logs/rl_training/ppo_world_model_final.zip",
        "status": "success",
        "training_timesteps": 50000,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": -719.9987937817352,
          "std_reward": 0.049307629640904585,
          "episodes": 1672,
          "last_reward": -720.0,
          "reward_trend": "stable",
          "avg_expert_matching": 0.6829253492273003,
          "std_expert_matching": 0.009785955830966258,
          "last_expert_matching": 0.6679310344827587
        },
        "monitoring_data": [
          {
            "timestep": 2500,
            "mean_reward": -513.2885486000001,
            "std_reward": 3.7260894474164137
          },
          {
            "timestep": 5000,
            "mean_reward": -513.95021,
            "std_reward": 1.448617624308066
          },
          {
            "timestep": 7500,
            "mean_reward": -419.9218106,
            "std_reward": 172.0854309483777
          },
          {
            "timestep": 10000,
            "mean_reward": -421.55154819999996,
            "std_reward": 175.5523049850482
          },
          {
            "timestep": 12500,
            "mean_reward": -510.99151739999996,
            "std_reward": 4.304340917882869
          },
          {
            "timestep": 15000,
            "mean_reward": -307.266573,
            "std_reward": 200.62189141791282
          },
          {
            "timestep": 17500,
            "mean_reward": -420.53242579999994,
            "std_reward": 137.55876051634968
          },
          {
            "timestep": 20000,
            "mean_reward": -233.21502040000001,
            "std_reward": 326.730516720668
          },
          {
            "timestep": 22500,
            "mean_reward": -254.61082919999998,
            "std_reward": 321.2868688396561
          },
          {
            "timestep": 25000,
            "mean_reward": -280.34449199999995,
            "std_reward": 273.7527169397478
          },
          {
            "timestep": 27500,
            "mean_reward": -237.04102240000003,
            "std_reward": 398.9370560670454
          },
          {
            "timestep": 30000,
            "mean_reward": 102.5340152,
            "std_reward": 80.62111233752165
          },
          {
            "timestep": 32500,
            "mean_reward": 140.762038,
            "std_reward": 48.98655176849901
          },
          {
            "timestep": 35000,
            "mean_reward": 333.7963234,
            "std_reward": 343.8330319791841
          },
          {
            "timestep": 37500,
            "mean_reward": -16.148877000000002,
            "std_reward": 232.60156302634803
          },
          {
            "timestep": 40000,
            "mean_reward": 225.16215259999998,
            "std_reward": 245.33330765450083
          },
          {
            "timestep": 42500,
            "mean_reward": -47.2105444,
            "std_reward": 218.5908119111153
          },
          {
            "timestep": 45000,
            "mean_reward": 326.3251694,
            "std_reward": 239.75741965363687
          },
          {
            "timestep": 47500,
            "mean_reward": -3.2650825999999995,
            "std_reward": 194.93980054924194
          },
          {
            "timestep": 50000,
            "mean_reward": -35.552412600000004,
            "std_reward": 225.9006491859562
          }
        ],
        "expert_matching_enabled": true,
        "reward_design": "expert_demonstration_matching",
        "optimal_threshold": 0.5,
        "threshold_map": 0.37727272727272726
      },
      "direct_video_ppo": {
        "algorithm": "PPO_DirectVideo",
        "status": "failed",
        "error": "No module named 'rl_environment'"
      }
    },
    "model_type": "ConditionalWorldModel",
    "approach": "FIXED: Action-conditioned world model + improved RL (TRAINED)",
    "method_description": "World model-based RL with fixed rewards and debugging (trained WM)",
    "improvements": [
      "Expert demonstration matching rewards",
      "Proper action space handling",
      "Enhanced monitoring and debugging",
      "Optimized hyperparameters",
      "Trained world model from scratch"
    ]
  },
  "method_3_direct_video_rl": {
    "status": "success",
    "rl_models": {
      "ppo": {
        "algorithm": "PPO_DirectVideo",
        "approach": "Direct RL on video sequences (no world model)",
        "mean_reward": 43.577785399999996,
        "std_reward": 2.2196503511346646,
        "model_path": "results/fixed_rl_2025-06-15_07-38-18/logs/direct_video_rl/ppo_direct_video.zip",
        "status": "success",
        "training_timesteps": 50000,
        "uses_world_model": false,
        "uses_real_frames": true,
        "simulation_based": false,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": 36.807904070610675,
          "episodes": 100,
          "last_length": 30,
          "last_reward": 44.46263157894736,
          "using_real_frames": true
        }
      },
      "a2c": {
        "algorithm": "A2C_DirectVideo",
        "approach": "Direct RL on video sequences (no world model)",
        "mean_reward": 49.0033552,
        "std_reward": 3.2545380230962064,
        "model_path": "results/fixed_rl_2025-06-15_07-38-18/logs/direct_video_rl/a2c_direct_video.zip",
        "status": "success",
        "training_timesteps": 50000,
        "uses_world_model": false,
        "uses_real_frames": true,
        "simulation_based": false,
        "episode_stats": {
          "avg_length": 30.0,
          "avg_reward": 36.08387300049307,
          "episodes": 100,
          "last_length": 30,
          "last_reward": 52.01954162768942,
          "using_real_frames": true
        }
      }
    },
    "model_type": "DirectVideoRL",
    "approach": "FIXED: Direct RL on video sequences with improved rewards",
    "method_description": "Model-free RL on offline video episodes with fixed reward design",
    "improvements": [
      "Expert demonstration matching rewards",
      "Proper continuous action space [0,1]",
      "Better episode termination",
      "Meaningful reward functions"
    ]
  },
  "comprehensive_evaluation": {
    "evaluator": "<evaluation.integrated_evaluation.IntegratedEvaluationFramework object at 0x7f03dd1e4970>",
    "results": {
      "status": "success",
      "evaluation_type": "comprehensive_evaluation_with_proper_batches",
      "num_models": 3,
      "num_videos": 40,
      "horizon": 15,
      "video_results": {
        "VID01": {
          "video_id": "VID01",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.0819530375838689,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.737800633360508,
                "mAP_present_only_with_null_verb": 0.08091196982049113,
                "mAP_freq_weighted_with_null_verb": 0.2248405218902906,
                "mAP_sample_wise_with_null_verb": 0.14446791880203602,
                "mAP_standard_all_actions": 0.737800633360508,
                "mAP_present_only_all_actions": 0.08091196982049113,
                "mAP_freq_weighted_all_actions": 0.2248405218902906,
                "mAP_sample_wise_all_actions": 0.14446791880203602,
                "exact_match_with_null_verb": 0.004039238315060588,
                "hamming_accuracy_with_null_verb": 0.9248759376803232,
                "precision_with_null_verb": 0.5237936057564948,
                "recall_with_null_verb": 0.6042835586847672,
                "f1_with_null_verb": 0.5290133042563301,
                "num_predictions": 1733,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "22",
                "action_sparsity_with_null_verb": 0.78,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7506075288733352,
                "mAP_present_only": 0.0819530375838689,
                "mAP_freq_weighted": 0.23624558618621344,
                "exact_match": 0.005770340450086555,
                "hamming_accuracy": 0.9266675670034745,
                "precision": 0.5264393720079416,
                "recall": 0.6234652971791612,
                "f1": 0.5333340031231383,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08543613036704142,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.5479256851606713,
                "mAP_present_only_with_null_verb": 0.08148038709396105,
                "mAP_freq_weighted_with_null_verb": 0.2809332288114948,
                "mAP_sample_wise_with_null_verb": 0.0689293936673034,
                "mAP_standard_all_actions": 0.5479256851606713,
                "mAP_present_only_all_actions": 0.08148038709396105,
                "mAP_freq_weighted_all_actions": 0.2809332288114948,
                "mAP_sample_wise_all_actions": 0.0689293936673034,
                "exact_match_with_null_verb": 0.0017311021350259665,
                "hamming_accuracy_with_null_verb": 0.9357530294287363,
                "precision_with_null_verb": 0.49921803688972605,
                "recall_with_null_verb": 0.4974067349343483,
                "f1_with_null_verb": 0.4936856379643535,
                "num_predictions": 1733,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "22",
                "action_sparsity_with_null_verb": 0.78,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5491838986912105,
                "mAP_present_only": 0.08543613036704142,
                "mAP_freq_weighted": 0.30539075252899134,
                "exact_match": 0.0017311021350259665,
                "hamming_accuracy": 0.9381223066629016,
                "precision": 0.5002490706428552,
                "recall": 0.5008618742629839,
                "f1": 0.4951792719790349,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10872426010310532,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.8029214242932291,
                "mAP_present_only_with_null_verb": 0.10418829224195077,
                "mAP_freq_weighted_with_null_verb": 0.35901833111151377,
                "mAP_sample_wise_with_null_verb": 0.06731761854285723,
                "mAP_standard_all_actions": 0.8029214242932291,
                "mAP_present_only_all_actions": 0.10418829224195077,
                "mAP_freq_weighted_all_actions": 0.35901833111151377,
                "mAP_sample_wise_all_actions": 0.06731761854285723,
                "exact_match_with_null_verb": 0.1102135025966532,
                "hamming_accuracy_with_null_verb": 0.9852336987882285,
                "precision_with_null_verb": 0.49261684939411426,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49628096651271214,
                "num_predictions": 1733,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "22",
                "action_sparsity_with_null_verb": 0.78,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8198485206591383,
                "mAP_present_only": 0.10872426010310532,
                "mAP_freq_weighted": 0.38959963131316444,
                "exact_match": 0.18118869013271782,
                "hamming_accuracy": 0.9862125695203251,
                "precision": 0.49310628476016255,
                "recall": 0.5,
                "f1": 0.49652921578202364,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.0819530375838689,
                "exact_match": 0.005770340450086555,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08543613036704142,
                "exact_match": 0.0017311021350259665,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10872426010310532,
                "exact_match": 0.18118869013271782,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID04": {
          "video_id": "VID04",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.07348453405231663,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.7377981860324698,
                "mAP_present_only_with_null_verb": 0.08899093016234938,
                "mAP_freq_weighted_with_null_verb": 0.22550451330886276,
                "mAP_sample_wise_with_null_verb": 0.1262253976982085,
                "mAP_standard_all_actions": 0.7377981860324698,
                "mAP_present_only_all_actions": 0.08899093016234938,
                "mAP_freq_weighted_all_actions": 0.22550451330886276,
                "mAP_sample_wise_all_actions": 0.1262253976982085,
                "exact_match_with_null_verb": 0.001314060446780552,
                "hamming_accuracy_with_null_verb": 0.9271616294349541,
                "precision_with_null_verb": 0.5189400911013707,
                "recall_with_null_verb": 0.588270633000995,
                "f1_with_null_verb": 0.5215727611206291,
                "num_predictions": 1522,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7579706072222274,
                "mAP_present_only": 0.07348453405231663,
                "mAP_freq_weighted": 0.20446967587482834,
                "exact_match": 0.0019710906701708277,
                "hamming_accuracy": 0.9289568596751195,
                "precision": 0.5213492251799229,
                "recall": 0.6109497140288052,
                "f1": 0.5254075881751553,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0936586085914512,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.548181079562584,
                "mAP_present_only_with_null_verb": 0.09090539781292013,
                "mAP_freq_weighted_with_null_verb": 0.25048843941672155,
                "mAP_sample_wise_with_null_verb": 0.05108415510671812,
                "mAP_standard_all_actions": 0.548181079562584,
                "mAP_present_only_all_actions": 0.09090539781292013,
                "mAP_freq_weighted_all_actions": 0.25048843941672155,
                "mAP_sample_wise_all_actions": 0.05108415510671812,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.925,
                "precision_with_null_verb": 0.4968945831229261,
                "recall_with_null_verb": 0.4862072414446601,
                "f1_with_null_verb": 0.4867388138131715,
                "num_predictions": 1522,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5594914504899432,
                "mAP_present_only": 0.0936586085914512,
                "mAP_freq_weighted": 0.27422304969986383,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9276497889115666,
                "precision": 0.4953942818263426,
                "recall": 0.4778045943700405,
                "f1": 0.4840156725868905,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.09337023142605029,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.818088040330602,
                "mAP_present_only_with_null_verb": 0.09044020165300956,
                "mAP_freq_weighted_with_null_verb": 0.335787961164293,
                "mAP_sample_wise_with_null_verb": 0.07489704695463201,
                "mAP_standard_all_actions": 0.818088040330602,
                "mAP_present_only_all_actions": 0.09044020165300956,
                "mAP_freq_weighted_all_actions": 0.335787961164293,
                "mAP_sample_wise_all_actions": 0.07489704695463201,
                "exact_match_with_null_verb": 0.19250985545335086,
                "hamming_accuracy_with_null_verb": 0.9866162943495401,
                "precision_with_null_verb": 0.49330814717477006,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4966315322972718,
                "num_predictions": 1522,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8360350418536473,
                "mAP_present_only": 0.09337023142605029,
                "mAP_freq_weighted": 0.37585762028698716,
                "exact_match": 0.22864651773981604,
                "hamming_accuracy": 0.9880126932647413,
                "precision": 0.49400634663237064,
                "recall": 0.5,
                "f1": 0.49698510306904203,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.07348453405231663,
                "exact_match": 0.0019710906701708277,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0936586085914512,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.09337023142605029,
                "exact_match": 0.22864651773981604,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID05": {
          "video_id": "VID05",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.07994469712771357,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.6796276189809345,
                "mAP_present_only_with_null_verb": 0.0754908422343635,
                "mAP_freq_weighted_with_null_verb": 0.20957701390991978,
                "mAP_sample_wise_with_null_verb": 0.030387496069497224,
                "mAP_standard_all_actions": 0.6796276189809345,
                "mAP_present_only_all_actions": 0.0754908422343635,
                "mAP_freq_weighted_all_actions": 0.20957701390991978,
                "mAP_sample_wise_all_actions": 0.030387496069497224,
                "exact_match_with_null_verb": 0.002559726962457338,
                "hamming_accuracy_with_null_verb": 0.931471843003413,
                "precision_with_null_verb": 0.4975836669665508,
                "recall_with_null_verb": 0.4894050109265257,
                "f1_with_null_verb": 0.4887042935582664,
                "num_predictions": 2344,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7004120003610363,
                "mAP_present_only": 0.07994469712771357,
                "mAP_freq_weighted": 0.22378903206169565,
                "exact_match": 0.0034129692832764505,
                "hamming_accuracy": 0.9326120107472224,
                "precision": 0.4980888623275804,
                "recall": 0.49149467138169356,
                "f1": 0.4895300023494906,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08209898292255936,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.47989955183467103,
                "mAP_present_only_with_null_verb": 0.07653673782565773,
                "mAP_freq_weighted_with_null_verb": 0.3227588865796147,
                "mAP_sample_wise_with_null_verb": 0.04820187392350809,
                "mAP_standard_all_actions": 0.47989955183467103,
                "mAP_present_only_all_actions": 0.07653673782565773,
                "mAP_freq_weighted_all_actions": 0.3227588865796147,
                "mAP_sample_wise_all_actions": 0.04820187392350809,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9084982935153584,
                "precision_with_null_verb": 0.49488720375024464,
                "recall_with_null_verb": 0.4694604310726314,
                "f1_with_null_verb": 0.4786192253323995,
                "num_predictions": 2344,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.48817315539594536,
                "mAP_present_only": 0.08209898292255936,
                "mAP_freq_weighted": 0.34845860052077415,
                "exact_match": 0.0004266211604095563,
                "hamming_accuracy": 0.9116894197952219,
                "precision": 0.4939127135761718,
                "recall": 0.46401568245810504,
                "f1": 0.47761940616538284,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.0812200455608666,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.7596080782578839,
                "mAP_present_only_with_null_verb": 0.07541568560724606,
                "mAP_freq_weighted_with_null_verb": 0.3693904028348791,
                "mAP_sample_wise_with_null_verb": 0.23723958604537115,
                "mAP_standard_all_actions": 0.7596080782578839,
                "mAP_present_only_all_actions": 0.07541568560724606,
                "mAP_freq_weighted_all_actions": 0.3693904028348791,
                "mAP_sample_wise_all_actions": 0.23723958604537115,
                "exact_match_with_null_verb": 0.23378839590443687,
                "hamming_accuracy_with_null_verb": 0.9875853242320819,
                "precision_with_null_verb": 0.49379266211604095,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49687694520165704,
                "num_predictions": 2344,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7751921388074461,
                "mAP_present_only": 0.0812200455608666,
                "mAP_freq_weighted": 0.3998252356691194,
                "exact_match": 0.2598122866894198,
                "hamming_accuracy": 0.9879229903420231,
                "precision": 0.49396149517101157,
                "recall": 0.5,
                "f1": 0.49696240505375433,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.07994469712771357,
                "exact_match": 0.0034129692832764505,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08209898292255936,
                "exact_match": 0.0004266211604095563,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.0812200455608666,
                "exact_match": 0.2598122866894198,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID08": {
          "video_id": "VID08",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09391454858309299,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.6917044262758197,
                "mAP_present_only_with_null_verb": 0.08347856259930676,
                "mAP_freq_weighted_with_null_verb": 0.20172038040754742,
                "mAP_sample_wise_with_null_verb": 0.032493849397186414,
                "mAP_standard_all_actions": 0.6917044262758197,
                "mAP_present_only_all_actions": 0.08347856259930676,
                "mAP_freq_weighted_all_actions": 0.20172038040754742,
                "mAP_sample_wise_all_actions": 0.032493849397186414,
                "exact_match_with_null_verb": 0.0019749835418038184,
                "hamming_accuracy_with_null_verb": 0.9313759052007899,
                "precision_with_null_verb": 0.49762675270214307,
                "recall_with_null_verb": 0.4899717010002299,
                "f1_with_null_verb": 0.4890386984005389,
                "num_predictions": 1519,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7241076603066812,
                "mAP_present_only": 0.09391454858309299,
                "mAP_freq_weighted": 0.21501248445879975,
                "exact_match": 0.0032916392363396972,
                "hamming_accuracy": 0.9314988864454499,
                "precision": 0.4980021789488675,
                "recall": 0.4914094567150951,
                "f1": 0.4895125135359347,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08973314140927936,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.510664241091563,
                "mAP_present_only_with_null_verb": 0.07947785035216562,
                "mAP_freq_weighted_with_null_verb": 0.30429194436677576,
                "mAP_sample_wise_with_null_verb": 0.039735298216951535,
                "mAP_standard_all_actions": 0.510664241091563,
                "mAP_present_only_all_actions": 0.07947785035216562,
                "mAP_freq_weighted_all_actions": 0.30429194436677576,
                "mAP_sample_wise_all_actions": 0.039735298216951535,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9039170506912443,
                "precision_with_null_verb": 0.49411655492853607,
                "recall_with_null_verb": 0.46441628431526727,
                "f1_with_null_verb": 0.4765376035101894,
                "num_predictions": 1519,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5316396713936611,
                "mAP_present_only": 0.08973314140927936,
                "mAP_freq_weighted": 0.32626780169287917,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9061042399114759,
                "precision": 0.4939641092200019,
                "recall": 0.46377443224420056,
                "f1": 0.4767054490626065,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.09349131563427862,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.7615333797820422,
                "mAP_present_only_with_null_verb": 0.08282069146939322,
                "mAP_freq_weighted_with_null_verb": 0.3470071699839136,
                "mAP_sample_wise_with_null_verb": 0.22540440234917644,
                "mAP_standard_all_actions": 0.7615333797820422,
                "mAP_present_only_all_actions": 0.08282069146939322,
                "mAP_freq_weighted_all_actions": 0.3470071699839136,
                "mAP_sample_wise_all_actions": 0.22540440234917644,
                "exact_match_with_null_verb": 0.17577353522053982,
                "hamming_accuracy_with_null_verb": 0.9871691902567479,
                "precision_with_null_verb": 0.49358459512837394,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4967715859811629,
                "num_predictions": 1519,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7878383930207887,
                "mAP_present_only": 0.09349131563427862,
                "mAP_freq_weighted": 0.37271384503297716,
                "exact_match": 0.21593153390388414,
                "hamming_accuracy": 0.9873657081226451,
                "precision": 0.49368285406132256,
                "recall": 0.5,
                "f1": 0.4968213470158721,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09391454858309299,
                "exact_match": 0.0032916392363396972,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08973314140927936,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.09349131563427862,
                "exact_match": 0.21593153390388414,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID10": {
          "video_id": "VID10",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09121818442980341,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "15",
                  "subset_action_sparsity": 0.8404255319148937
                },
                "mAP_standard_with_null_verb": 0.7564226026938355,
                "mAP_present_only_with_null_verb": 0.08211301346917713,
                "mAP_freq_weighted_with_null_verb": 0.22324381920037822,
                "mAP_sample_wise_with_null_verb": 0.10266401839237449,
                "mAP_standard_all_actions": 0.7564226026938355,
                "mAP_present_only_all_actions": 0.08211301346917713,
                "mAP_freq_weighted_all_actions": 0.22324381920037822,
                "mAP_sample_wise_all_actions": 0.10266401839237449,
                "exact_match_with_null_verb": 0.030303030303030304,
                "hamming_accuracy_with_null_verb": 0.9431160663236134,
                "precision_with_null_verb": 0.5089086114381525,
                "recall_with_null_verb": 0.524542259697791,
                "f1_with_null_verb": 0.5094488752786751,
                "num_predictions": 1749,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.791151837940926,
                "mAP_present_only": 0.09121818442980341,
                "mAP_freq_weighted": 0.22837207720164265,
                "exact_match": 0.036020583190394515,
                "hamming_accuracy": 0.9430130287215795,
                "precision": 0.5099616230363615,
                "recall": 0.5290389460424102,
                "f1": 0.510836395181662,
                "num_actions_total": 94,
                "num_actions_present": "15",
                "action_sparsity": 0.8404255319148937
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.10088937128569601,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "15",
                  "subset_action_sparsity": 0.8404255319148937
                },
                "mAP_standard_with_null_verb": 0.586514920849396,
                "mAP_present_only_with_null_verb": 0.08257460424698018,
                "mAP_freq_weighted_with_null_verb": 0.26989313040843416,
                "mAP_sample_wise_with_null_verb": 0.049027012748367835,
                "mAP_standard_all_actions": 0.586514920849396,
                "mAP_present_only_all_actions": 0.08257460424698018,
                "mAP_freq_weighted_all_actions": 0.26989313040843416,
                "mAP_sample_wise_all_actions": 0.049027012748367835,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9240594625500286,
                "precision_with_null_verb": 0.4980558524090489,
                "recall_with_null_verb": 0.4926433104820106,
                "f1_with_null_verb": 0.4897777279152263,
                "num_predictions": 1749,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6118440486094197,
                "mAP_present_only": 0.10088937128569601,
                "mAP_freq_weighted": 0.28975807667330955,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9251547997031738,
                "precision": 0.4987692884133779,
                "recall": 0.4951944876207884,
                "f1": 0.4907357875368103,
                "num_actions_total": 94,
                "num_actions_present": "15",
                "action_sparsity": 0.8404255319148937
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10294657132810824,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "15",
                  "subset_action_sparsity": 0.8404255319148937
                },
                "mAP_standard_with_null_verb": 0.8171521277758181,
                "mAP_present_only_with_null_verb": 0.08576063887908991,
                "mAP_freq_weighted_with_null_verb": 0.28764287191719157,
                "mAP_sample_wise_with_null_verb": 0.13408119604728708,
                "mAP_standard_all_actions": 0.8171521277758181,
                "mAP_present_only_all_actions": 0.08576063887908991,
                "mAP_freq_weighted_all_actions": 0.28764287191719157,
                "mAP_sample_wise_all_actions": 0.13408119604728708,
                "exact_match_with_null_verb": 0.13607775871926817,
                "hamming_accuracy_with_null_verb": 0.9844253859348199,
                "precision_with_null_verb": 0.49221269296740994,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4960757874356049,
                "num_predictions": 1749,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8568531762757619,
                "mAP_present_only": 0.10294657132810824,
                "mAP_freq_weighted": 0.3074826283724987,
                "exact_match": 0.14922813036020582,
                "hamming_accuracy": 0.9850127124314197,
                "precision": 0.4925063562157099,
                "recall": 0.5,
                "f1": 0.4962248887690441,
                "num_actions_total": 94,
                "num_actions_present": "15",
                "action_sparsity": 0.8404255319148937
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09121818442980341,
                "exact_match": 0.036020583190394515,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.10088937128569601,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10294657132810824,
                "exact_match": 0.14922813036020582,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID103": {
          "video_id": "VID103",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.06273465743476503,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.6676935787345363,
                "mAP_present_only_with_null_verb": 0.0631913526233437,
                "mAP_freq_weighted_with_null_verb": 0.16889135965843452,
                "mAP_sample_wise_with_null_verb": 0.0829754778020874,
                "mAP_standard_all_actions": 0.6676935787345363,
                "mAP_present_only_all_actions": 0.0631913526233437,
                "mAP_freq_weighted_all_actions": 0.16889135965843452,
                "mAP_sample_wise_all_actions": 0.0829754778020874,
                "exact_match_with_null_verb": 0.00811176205497972,
                "hamming_accuracy_with_null_verb": 0.942789544840018,
                "precision_with_null_verb": 0.49535007039982076,
                "recall_with_null_verb": 0.488697078013359,
                "f1_with_null_verb": 0.4907252629178362,
                "num_predictions": 2219,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6868975152752035,
                "mAP_present_only": 0.06273465743476503,
                "mAP_freq_weighted": 0.17600037416235575,
                "exact_match": 0.00946372239747634,
                "hamming_accuracy": 0.94485727709434,
                "precision": 0.4962715176429136,
                "recall": 0.4905268857228118,
                "f1": 0.4918312710220474,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08166748762550491,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.5322099995491066,
                "mAP_present_only_with_null_verb": 0.0793214269610951,
                "mAP_freq_weighted_with_null_verb": 0.23844159877693671,
                "mAP_sample_wise_with_null_verb": 0.049681434092682775,
                "mAP_standard_all_actions": 0.5322099995491066,
                "mAP_present_only_all_actions": 0.0793214269610951,
                "mAP_freq_weighted_all_actions": 0.23844159877693671,
                "mAP_sample_wise_all_actions": 0.049681434092682775,
                "exact_match_with_null_verb": 0.0018026137899954935,
                "hamming_accuracy_with_null_verb": 0.9156466876971608,
                "precision_with_null_verb": 0.49676837793640943,
                "recall_with_null_verb": 0.4872067872323719,
                "f1_with_null_verb": 0.4864737869838342,
                "num_predictions": 2219,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5429966722408258,
                "mAP_present_only": 0.08166748762550491,
                "mAP_freq_weighted": 0.25894252789180794,
                "exact_match": 0.0018026137899954935,
                "hamming_accuracy": 0.9178324528012427,
                "precision": 0.49780240956392874,
                "recall": 0.49082838416806324,
                "f1": 0.4878374094460024,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.09305690561288568,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "25",
                  "subset_action_sparsity": 0.7340425531914894
                },
                "mAP_standard_with_null_verb": 0.7458612738863322,
                "mAP_present_only_with_null_verb": 0.09236169245118672,
                "mAP_freq_weighted_with_null_verb": 0.2786164412300945,
                "mAP_sample_wise_with_null_verb": 0.12990176247905835,
                "mAP_standard_all_actions": 0.7458612738863322,
                "mAP_present_only_all_actions": 0.09236169245118672,
                "mAP_freq_weighted_all_actions": 0.2786164412300945,
                "mAP_sample_wise_all_actions": 0.12990176247905835,
                "exact_match_with_null_verb": 0.23073456511942317,
                "hamming_accuracy_with_null_verb": 0.9834429923388914,
                "precision_with_null_verb": 0.4917214961694457,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4958261952259126,
                "num_predictions": 2219,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.758791730216193,
                "mAP_present_only": 0.09305690561288568,
                "mAP_freq_weighted": 0.30243986981283805,
                "exact_match": 0.28841820639927895,
                "hamming_accuracy": 0.9845243688454642,
                "precision": 0.4922621844227321,
                "recall": 0.5,
                "f1": 0.496100921863827,
                "num_actions_total": 94,
                "num_actions_present": "25",
                "action_sparsity": 0.7340425531914894
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.06273465743476503,
                "exact_match": 0.00946372239747634,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08166748762550491,
                "exact_match": 0.0018026137899954935,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.09305690561288568,
                "exact_match": 0.28841820639927895,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID110": {
          "video_id": "VID110",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08205274345285318,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.6692024933585938,
                "mAP_present_only_with_null_verb": 0.07112034577256973,
                "mAP_freq_weighted_with_null_verb": 0.14059425895776873,
                "mAP_sample_wise_with_null_verb": 0.0903366147549695,
                "mAP_standard_all_actions": 0.6692024933585938,
                "mAP_present_only_all_actions": 0.07112034577256973,
                "mAP_freq_weighted_all_actions": 0.14059425895776873,
                "mAP_sample_wise_all_actions": 0.0903366147549695,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9302895220588235,
                "precision_with_null_verb": 0.511711287384148,
                "recall_with_null_verb": 0.5604666342051622,
                "f1_with_null_verb": 0.5101429795796315,
                "num_predictions": 2176,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "27",
                "action_sparsity_with_null_verb": 0.73,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7098202937501056,
                "mAP_present_only": 0.08205274345285318,
                "mAP_freq_weighted": 0.15035689970709507,
                "exact_match": 0.001838235294117647,
                "hamming_accuracy": 0.9320390722778473,
                "precision": 0.513565322855108,
                "recall": 0.5775824520682862,
                "f1": 0.5130286529818368,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.05635125626264997,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.5035900136052743,
                "mAP_present_only_with_null_verb": 0.05033338372323835,
                "mAP_freq_weighted_with_null_verb": 0.1342654410406201,
                "mAP_sample_wise_with_null_verb": 0.04600471863860305,
                "mAP_standard_all_actions": 0.5035900136052743,
                "mAP_present_only_all_actions": 0.05033338372323835,
                "mAP_freq_weighted_all_actions": 0.1342654410406201,
                "mAP_sample_wise_all_actions": 0.04600471863860305,
                "exact_match_with_null_verb": 0.00045955882352941176,
                "hamming_accuracy_with_null_verb": 0.9247334558823529,
                "precision_with_null_verb": 0.49657761453414156,
                "recall_with_null_verb": 0.48179313462936196,
                "f1_with_null_verb": 0.4847387303484189,
                "num_predictions": 2176,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "27",
                "action_sparsity_with_null_verb": 0.73,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5338657061863367,
                "mAP_present_only": 0.05635125626264997,
                "mAP_freq_weighted": 0.14534870537451947,
                "exact_match": 0.00045955882352941176,
                "hamming_accuracy": 0.9275754849812266,
                "precision": 0.49692266623116904,
                "recall": 0.48224376648388106,
                "f1": 0.48509283918828056,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08147467169314629,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7491041677671589,
                "mAP_present_only_with_null_verb": 0.07075617691540328,
                "mAP_freq_weighted_with_null_verb": 0.25451243801469425,
                "mAP_sample_wise_with_null_verb": 0.12644964457519656,
                "mAP_standard_all_actions": 0.7491041677671589,
                "mAP_present_only_all_actions": 0.07075617691540328,
                "mAP_freq_weighted_all_actions": 0.25451243801469425,
                "mAP_sample_wise_all_actions": 0.12644964457519656,
                "exact_match_with_null_verb": 0.2426470588235294,
                "hamming_accuracy_with_null_verb": 0.9885340073529412,
                "precision_with_null_verb": 0.4942670036764706,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4971169734576675,
                "num_predictions": 2176,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "27",
                "action_sparsity_with_null_verb": 0.73,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7947975330378306,
                "mAP_present_only": 0.08147467169314629,
                "mAP_freq_weighted": 0.28985667206655313,
                "exact_match": 0.3102022058823529,
                "hamming_accuracy": 0.9897088157071339,
                "precision": 0.49485440785356694,
                "recall": 0.5,
                "f1": 0.4974138968949563,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08205274345285318,
                "exact_match": 0.001838235294117647,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.05635125626264997,
                "exact_match": 0.00045955882352941176,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08147467169314629,
                "exact_match": 0.3102022058823529,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID12": {
          "video_id": "VID12",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.12723328611555743,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.7332863139825733,
                "mAP_present_only_with_null_verb": 0.11643156991286582,
                "mAP_freq_weighted_with_null_verb": 0.22163003920713906,
                "mAP_sample_wise_with_null_verb": 0.170667397183188,
                "mAP_standard_all_actions": 0.7332863139825733,
                "mAP_present_only_all_actions": 0.11643156991286582,
                "mAP_freq_weighted_all_actions": 0.22163003920713906,
                "mAP_sample_wise_all_actions": 0.170667397183188,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9230550458715596,
                "precision_with_null_verb": 0.5320709396606447,
                "recall_with_null_verb": 0.6410928632955454,
                "f1_with_null_verb": 0.5416425715563513,
                "num_predictions": 1090,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7570528283400476,
                "mAP_present_only": 0.12723328611555743,
                "mAP_freq_weighted": 0.23302173370390375,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9242338473550654,
                "precision": 0.5349120532331679,
                "recall": 0.6592101150397717,
                "f1": 0.5462643326915038,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.1154220096443313,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.5810669203140728,
                "mAP_present_only_with_null_verb": 0.10533460157036396,
                "mAP_freq_weighted_with_null_verb": 0.28767691698194886,
                "mAP_sample_wise_with_null_verb": 0.05411938063793497,
                "mAP_standard_all_actions": 0.5810669203140728,
                "mAP_present_only_all_actions": 0.10533460157036396,
                "mAP_freq_weighted_all_actions": 0.28767691698194886,
                "mAP_sample_wise_all_actions": 0.05411938063793497,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9133853211009174,
                "precision_with_null_verb": 0.4940000022165469,
                "recall_with_null_verb": 0.4737890339865614,
                "f1_with_null_verb": 0.480934294905214,
                "num_predictions": 1090,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5953422783399323,
                "mAP_present_only": 0.1154220096443313,
                "mAP_freq_weighted": 0.30756475567201563,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9144251415186414,
                "precision": 0.4944610599853095,
                "recall": 0.47508348293471875,
                "f1": 0.48149012993562895,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.13429129299384132,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.824511489729023,
                "mAP_present_only_with_null_verb": 0.12255744864511524,
                "mAP_freq_weighted_with_null_verb": 0.37842613699936684,
                "mAP_sample_wise_with_null_verb": 0.13557472585678876,
                "mAP_standard_all_actions": 0.824511489729023,
                "mAP_present_only_all_actions": 0.12255744864511524,
                "mAP_freq_weighted_all_actions": 0.37842613699936684,
                "mAP_sample_wise_all_actions": 0.13557472585678876,
                "exact_match_with_null_verb": 0.20825688073394497,
                "hamming_accuracy_with_null_verb": 0.9845137614678899,
                "precision_with_null_verb": 0.49225688073394497,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49609822848478125,
                "num_predictions": 1090,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8434356593712267,
                "mAP_present_only": 0.13429129299384132,
                "mAP_freq_weighted": 0.4057706284250928,
                "exact_match": 0.23577981651376148,
                "hamming_accuracy": 0.985057583447199,
                "precision": 0.4925287917235995,
                "recall": 0.5,
                "f1": 0.4962362762981282,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.12723328611555743,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.1154220096443313,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.13429129299384132,
                "exact_match": 0.23577981651376148,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID13": {
          "video_id": "VID13",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.23440148514935932,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.7578057704977798,
                "mAP_present_only_with_null_verb": 0.19897773946199884,
                "mAP_freq_weighted_with_null_verb": 0.36179913410157066,
                "mAP_sample_wise_with_null_verb": 0.15866153484525208,
                "mAP_standard_all_actions": 0.7578057704977798,
                "mAP_present_only_all_actions": 0.19897773946199884,
                "mAP_freq_weighted_all_actions": 0.36179913410157066,
                "mAP_sample_wise_all_actions": 0.15866153484525208,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9240570846075433,
                "precision_with_null_verb": 0.538680426463518,
                "recall_with_null_verb": 0.6333212132777606,
                "f1_with_null_verb": 0.5515791878445123,
                "num_predictions": 981,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "19",
                "action_sparsity_with_null_verb": 0.81,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7845789761956355,
                "mAP_present_only": 0.23440148514935932,
                "mAP_freq_weighted": 0.3659303268241174,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9236449996746698,
                "precision": 0.5411102091357924,
                "recall": 0.6353376139455125,
                "f1": 0.5549864610673916,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.14013892324691052,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.6126857667964258,
                "mAP_present_only_with_null_verb": 0.11939877261276788,
                "mAP_freq_weighted_with_null_verb": 0.40572958197119435,
                "mAP_sample_wise_with_null_verb": 0.04822280002606709,
                "mAP_standard_all_actions": 0.6126857667964258,
                "mAP_present_only_all_actions": 0.11939877261276788,
                "mAP_freq_weighted_all_actions": 0.40572958197119435,
                "mAP_sample_wise_all_actions": 0.04822280002606709,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9079918450560652,
                "precision_with_null_verb": 0.4912573330746487,
                "recall_with_null_verb": 0.46833500129052114,
                "f1_with_null_verb": 0.4781991695960724,
                "num_predictions": 981,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "19",
                "action_sparsity_with_null_verb": 0.81,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6302364124675592,
                "mAP_present_only": 0.14013892324691052,
                "mAP_freq_weighted": 0.41038786521765924,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9059904136031405,
                "precision": 0.4906377859251899,
                "recall": 0.46730865104844915,
                "f1": 0.47751520867040637,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.1438615169138608,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.8334760206125132,
                "mAP_present_only_with_null_verb": 0.12355800322375321,
                "mAP_freq_weighted_with_null_verb": 0.43416144753944796,
                "mAP_sample_wise_with_null_verb": 0.12005399461836946,
                "mAP_standard_all_actions": 0.8334760206125132,
                "mAP_present_only_all_actions": 0.12355800322375321,
                "mAP_freq_weighted_all_actions": 0.43416144753944796,
                "mAP_sample_wise_all_actions": 0.12005399461836946,
                "exact_match_with_null_verb": 0.05402650356778797,
                "hamming_accuracy_with_null_verb": 0.9808766564729867,
                "precision_with_null_verb": 0.49043832823649336,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49517301002449515,
                "num_predictions": 981,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "19",
                "action_sparsity_with_null_verb": 0.81,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8542743007512954,
                "mAP_present_only": 0.1438615169138608,
                "mAP_freq_weighted": 0.43906778031890836,
                "exact_match": 0.0581039755351682,
                "hamming_accuracy": 0.9798945930119071,
                "precision": 0.48994729650595353,
                "recall": 0.5,
                "f1": 0.49492260672384897,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.23440148514935932,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.14013892324691052,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.1438615169138608,
                "exact_match": 0.0581039755351682,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID15": {
          "video_id": "VID15",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.06575704552019271,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.6296132901940483,
                "mAP_present_only_with_null_verb": 0.06326867804531716,
                "mAP_freq_weighted_with_null_verb": 0.25628948438564286,
                "mAP_sample_wise_with_null_verb": 0.02998985458683304,
                "mAP_standard_all_actions": 0.6296132901940483,
                "mAP_present_only_all_actions": 0.06326867804531716,
                "mAP_freq_weighted_all_actions": 0.25628948438564286,
                "mAP_sample_wise_all_actions": 0.02998985458683304,
                "exact_match_with_null_verb": 0.003401360544217687,
                "hamming_accuracy_with_null_verb": 0.9333722060252673,
                "precision_with_null_verb": 0.4963000626181328,
                "recall_with_null_verb": 0.48706986485768133,
                "f1_with_null_verb": 0.488671399299199,
                "num_predictions": 2058,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6472467795166531,
                "mAP_present_only": 0.06575704552019271,
                "mAP_freq_weighted": 0.27145775038772996,
                "exact_match": 0.005344995140913508,
                "hamming_accuracy": 0.9334666997498088,
                "precision": 0.49667809665643414,
                "recall": 0.4881258299760847,
                "f1": 0.4890775071670627,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0735587636427862,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.4614438401404468,
                "mAP_present_only_with_null_verb": 0.06917367787240893,
                "mAP_freq_weighted_with_null_verb": 0.3750106586017193,
                "mAP_sample_wise_with_null_verb": 0.040398599492242164,
                "mAP_standard_all_actions": 0.4614438401404468,
                "mAP_present_only_all_actions": 0.06917367787240893,
                "mAP_freq_weighted_all_actions": 0.3750106586017193,
                "mAP_sample_wise_all_actions": 0.040398599492242164,
                "exact_match_with_null_verb": 0.0029154518950437317,
                "hamming_accuracy_with_null_verb": 0.9153644314868805,
                "precision_with_null_verb": 0.4941381763073282,
                "recall_with_null_verb": 0.4731599584199278,
                "f1_with_null_verb": 0.4809246256390631,
                "num_predictions": 2058,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.4687196317233831,
                "mAP_present_only": 0.0735587636427862,
                "mAP_freq_weighted": 0.40213138779704544,
                "exact_match": 0.003401360544217687,
                "hamming_accuracy": 0.9173955296404276,
                "precision": 0.4933095280524341,
                "recall": 0.469622536028167,
                "f1": 0.47995379089300827,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.07362240610587112,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "28",
                  "subset_action_sparsity": 0.7021276595744681
                },
                "mAP_standard_with_null_verb": 0.7119037970698244,
                "mAP_present_only_with_null_verb": 0.07065740990265935,
                "mAP_freq_weighted_with_null_verb": 0.3494009906798926,
                "mAP_sample_wise_with_null_verb": 0.1785104832555642,
                "mAP_standard_all_actions": 0.7119037970698244,
                "mAP_present_only_all_actions": 0.07065740990265935,
                "mAP_freq_weighted_all_actions": 0.3494009906798926,
                "mAP_sample_wise_all_actions": 0.1785104832555642,
                "exact_match_with_null_verb": 0.1360544217687075,
                "hamming_accuracy_with_null_verb": 0.9854567541302235,
                "precision_with_null_verb": 0.49272837706511174,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4963375566253148,
                "num_predictions": 2058,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7240577379889829,
                "mAP_present_only": 0.07362240610587112,
                "mAP_freq_weighted": 0.37131345305884844,
                "exact_match": 0.17784256559766765,
                "hamming_accuracy": 0.9857277257407522,
                "precision": 0.4928638628703761,
                "recall": 0.5,
                "f1": 0.4964062862007117,
                "num_actions_total": 94,
                "num_actions_present": "28",
                "action_sparsity": 0.7021276595744681
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.06575704552019271,
                "exact_match": 0.005344995140913508,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0735587636427862,
                "exact_match": 0.003401360544217687,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.07362240610587112,
                "exact_match": 0.17784256559766765,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID18": {
          "video_id": "VID18",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.1265820335250911,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.6961064072033513,
                "mAP_present_only_with_null_verb": 0.10877669668063088,
                "mAP_freq_weighted_with_null_verb": 0.279543867188049,
                "mAP_sample_wise_with_null_verb": 0.02771707512281958,
                "mAP_standard_all_actions": 0.6961064072033513,
                "mAP_present_only_all_actions": 0.10877669668063088,
                "mAP_freq_weighted_all_actions": 0.279543867188049,
                "mAP_sample_wise_all_actions": 0.02771707512281958,
                "exact_match_with_null_verb": 0.004634397528321318,
                "hamming_accuracy_with_null_verb": 0.9271266735324408,
                "precision_with_null_verb": 0.4964050716644101,
                "recall_with_null_verb": 0.4866246980903937,
                "f1_with_null_verb": 0.48770704936007264,
                "num_predictions": 1942,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "24",
                "action_sparsity_with_null_verb": 0.76,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7290600071329982,
                "mAP_present_only": 0.1265820335250911,
                "mAP_freq_weighted": 0.29365066593651346,
                "exact_match": 0.005149330587023687,
                "hamming_accuracy": 0.9269616758332055,
                "precision": 0.49660984547960096,
                "recall": 0.4874739132604527,
                "f1": 0.48806314584251337,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.09723742188185108,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.4902858766713761,
                "mAP_present_only_with_null_verb": 0.08452448613073364,
                "mAP_freq_weighted_with_null_verb": 0.3227172243720021,
                "mAP_sample_wise_with_null_verb": 0.03693506696323716,
                "mAP_standard_all_actions": 0.4902858766713761,
                "mAP_present_only_all_actions": 0.08452448613073364,
                "mAP_freq_weighted_all_actions": 0.3227172243720021,
                "mAP_sample_wise_all_actions": 0.03693506696323716,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9007054582904223,
                "precision_with_null_verb": 0.4931517408740574,
                "recall_with_null_verb": 0.4643089170319874,
                "f1_with_null_verb": 0.47604224759199104,
                "num_predictions": 1942,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "24",
                "action_sparsity_with_null_verb": 0.76,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5100505152940109,
                "mAP_present_only": 0.09723742188185108,
                "mAP_freq_weighted": 0.3390866681516067,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9029022503670268,
                "precision": 0.4925068205550259,
                "recall": 0.46232705387611195,
                "f1": 0.4757218111162117,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10549156751869379,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "20",
                  "subset_action_sparsity": 0.7872340425531915
                },
                "mAP_standard_with_null_verb": 0.7821129596661018,
                "mAP_present_only_with_null_verb": 0.09213733194209073,
                "mAP_freq_weighted_with_null_verb": 0.4129209752836228,
                "mAP_sample_wise_with_null_verb": 0.19071678544359108,
                "mAP_standard_all_actions": 0.7821129596661018,
                "mAP_present_only_all_actions": 0.09213733194209073,
                "mAP_freq_weighted_all_actions": 0.4129209752836228,
                "mAP_sample_wise_all_actions": 0.19071678544359108,
                "exact_match_with_null_verb": 0.061277033985581875,
                "hamming_accuracy_with_null_verb": 0.9849073120494336,
                "precision_with_null_verb": 0.4924536560247168,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4961981378528494,
                "num_predictions": 1942,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "24",
                "action_sparsity_with_null_verb": 0.76,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8096790569188711,
                "mAP_present_only": 0.10549156751869379,
                "mAP_freq_weighted": 0.43416039066938944,
                "exact_match": 0.07157569515962925,
                "hamming_accuracy": 0.9847766067007033,
                "precision": 0.49238830335035166,
                "recall": 0.5,
                "f1": 0.4961649605179994,
                "num_actions_total": 94,
                "num_actions_present": "20",
                "action_sparsity": 0.7872340425531915
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.1265820335250911,
                "exact_match": 0.005149330587023687,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.09723742188185108,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10549156751869379,
                "exact_match": 0.07157569515962925,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID22": {
          "video_id": "VID22",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08691179099813175,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.688731506006469,
                "mAP_present_only_with_null_verb": 0.0720442538710352,
                "mAP_freq_weighted_with_null_verb": 0.2866640745228644,
                "mAP_sample_wise_with_null_verb": 0.03132247732014359,
                "mAP_standard_all_actions": 0.688731506006469,
                "mAP_present_only_all_actions": 0.0720442538710352,
                "mAP_freq_weighted_all_actions": 0.2866640745228644,
                "mAP_sample_wise_all_actions": 0.03132247732014359,
                "exact_match_with_null_verb": 0.0019582245430809398,
                "hamming_accuracy_with_null_verb": 0.9252806788511749,
                "precision_with_null_verb": 0.4939362187650129,
                "recall_with_null_verb": 0.47858873519434875,
                "f1_with_null_verb": 0.4842320409790126,
                "num_predictions": 1532,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7215441235208593,
                "mAP_present_only": 0.08691179099813175,
                "mAP_freq_weighted": 0.2957666828561409,
                "exact_match": 0.0019582245430809398,
                "hamming_accuracy": 0.9245666907394033,
                "precision": 0.4938268858592055,
                "recall": 0.4787278648739858,
                "f1": 0.48423324749346575,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.11950487208719343,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.5455759208933779,
                "mAP_present_only_with_null_verb": 0.09836892651299232,
                "mAP_freq_weighted_with_null_verb": 0.4780593865719407,
                "mAP_sample_wise_with_null_verb": 0.0406302368449647,
                "mAP_standard_all_actions": 0.5455759208933779,
                "mAP_present_only_all_actions": 0.09836892651299232,
                "mAP_freq_weighted_all_actions": 0.4780593865719407,
                "mAP_sample_wise_all_actions": 0.0406302368449647,
                "exact_match_with_null_verb": 0.0013054830287206266,
                "hamming_accuracy_with_null_verb": 0.9127872062663185,
                "precision_with_null_verb": 0.49259256487238534,
                "recall_with_null_verb": 0.46883706559777755,
                "f1_with_null_verb": 0.479062944061907,
                "num_predictions": 1532,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5692510884450113,
                "mAP_present_only": 0.11950487208719343,
                "mAP_freq_weighted": 0.493547200432836,
                "exact_match": 0.0013054830287206266,
                "hamming_accuracy": 0.9145533581467696,
                "precision": 0.49231458120976057,
                "recall": 0.46950252755272076,
                "f1": 0.4794629262347513,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.09541641047615707,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7611190965662924,
                "mAP_present_only_with_null_verb": 0.08122729448573986,
                "mAP_freq_weighted_with_null_verb": 0.3741416959737837,
                "mAP_sample_wise_with_null_verb": 0.17668756551118434,
                "mAP_standard_all_actions": 0.7611190965662924,
                "mAP_present_only_all_actions": 0.08122729448573986,
                "mAP_freq_weighted_all_actions": 0.3741416959737837,
                "mAP_sample_wise_all_actions": 0.17668756551118434,
                "exact_match_with_null_verb": 0.06462140992167102,
                "hamming_accuracy_with_null_verb": 0.9839621409921671,
                "precision_with_null_verb": 0.49198107049608353,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4959581237271462,
                "num_predictions": 1532,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7979121768085031,
                "mAP_present_only": 0.09541641047615707,
                "mAP_freq_weighted": 0.3855759132815276,
                "exact_match": 0.08485639686684072,
                "hamming_accuracy": 0.9834870284984167,
                "precision": 0.49174351424920837,
                "recall": 0.5,
                "f1": 0.4958373885827516,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08691179099813175,
                "exact_match": 0.0019582245430809398,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.11950487208719343,
                "exact_match": 0.0013054830287206266,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.09541641047615707,
                "exact_match": 0.08485639686684072,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID26": {
          "video_id": "VID26",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08121741515182063,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7084073739654684,
                "mAP_present_only_with_null_verb": 0.0707975921748782,
                "mAP_freq_weighted_with_null_verb": 0.2272788987018078,
                "mAP_sample_wise_with_null_verb": 0.028573334108082476,
                "mAP_standard_all_actions": 0.7084073739654684,
                "mAP_present_only_all_actions": 0.0707975921748782,
                "mAP_freq_weighted_all_actions": 0.2272788987018078,
                "mAP_sample_wise_all_actions": 0.028573334108082476,
                "exact_match_with_null_verb": 0.0028200789622109417,
                "hamming_accuracy_with_null_verb": 0.929785673998872,
                "precision_with_null_verb": 0.4984751882411292,
                "recall_with_null_verb": 0.49354164707590326,
                "f1_with_null_verb": 0.4901686272677805,
                "num_predictions": 1773,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7415485714700877,
                "mAP_present_only": 0.08121741515182063,
                "mAP_freq_weighted": 0.2452349286813619,
                "exact_match": 0.0056401579244218835,
                "hamming_accuracy": 0.9294500246006888,
                "precision": 0.4988984047820838,
                "recall": 0.49513131075015376,
                "f1": 0.4904790430414088,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08912190000318838,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.499872187128977,
                "mAP_present_only_with_null_verb": 0.07643148895760385,
                "mAP_freq_weighted_with_null_verb": 0.3082275030498802,
                "mAP_sample_wise_with_null_verb": 0.037399393475574874,
                "mAP_standard_all_actions": 0.499872187128977,
                "mAP_present_only_all_actions": 0.07643148895760385,
                "mAP_freq_weighted_all_actions": 0.3082275030498802,
                "mAP_sample_wise_all_actions": 0.037399393475574874,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9113761985335589,
                "precision_with_null_verb": 0.49569419633976664,
                "recall_with_null_verb": 0.47658535527687373,
                "f1_with_null_verb": 0.48122274270737314,
                "num_predictions": 1773,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.51991021170284,
                "mAP_present_only": 0.08912190000318838,
                "mAP_freq_weighted": 0.3345556768958546,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9138315872844439,
                "precision": 0.49600181241455876,
                "recall": 0.47815472949434135,
                "f1": 0.4820325868286373,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08699237757271013,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7609373913471624,
                "mAP_present_only_with_null_verb": 0.08052842825831674,
                "mAP_freq_weighted_with_null_verb": 0.3872565958545441,
                "mAP_sample_wise_with_null_verb": 0.17383303282106707,
                "mAP_standard_all_actions": 0.7609373913471624,
                "mAP_present_only_all_actions": 0.08052842825831674,
                "mAP_freq_weighted_all_actions": 0.3872565958545441,
                "mAP_sample_wise_all_actions": 0.17383303282106707,
                "exact_match_with_null_verb": 0.116751269035533,
                "hamming_accuracy_with_null_verb": 0.9868697123519459,
                "precision_with_null_verb": 0.49343485617597294,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49669573511377574,
                "num_predictions": 1773,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7960302120109245,
                "mAP_present_only": 0.08699237757271013,
                "mAP_freq_weighted": 0.4158617993424674,
                "exact_match": 0.13423575860124085,
                "hamming_accuracy": 0.9872616433260132,
                "precision": 0.4936308216630066,
                "recall": 0.5,
                "f1": 0.4967949975996449,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08121741515182063,
                "exact_match": 0.0056401579244218835,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08912190000318838,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08699237757271013,
                "exact_match": 0.13423575860124085,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID27": {
          "video_id": "VID27",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.12797766312045977,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.6791067040630636,
                "mAP_present_only_with_null_verb": 0.11194886178101476,
                "mAP_freq_weighted_with_null_verb": 0.3939144316436201,
                "mAP_sample_wise_with_null_verb": 0.04306119665342731,
                "mAP_standard_all_actions": 0.6791067040630636,
                "mAP_present_only_all_actions": 0.11194886178101476,
                "mAP_freq_weighted_all_actions": 0.3939144316436201,
                "mAP_sample_wise_all_actions": 0.04306119665342731,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9357965451055662,
                "precision_with_null_verb": 0.5017266490390376,
                "recall_with_null_verb": 0.5048898572153633,
                "f1_with_null_verb": 0.498406275448098,
                "num_predictions": 2084,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.71080328285798,
                "mAP_present_only": 0.12797766312045977,
                "mAP_freq_weighted": 0.4125831090947339,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9357056805651979,
                "precision": 0.502301736259106,
                "recall": 0.5064595015064975,
                "f1": 0.49928644674301853,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.09738947773133301,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.5420416712261032,
                "mAP_present_only_with_null_verb": 0.08477565856193564,
                "mAP_freq_weighted_with_null_verb": 0.46889075006119296,
                "mAP_sample_wise_with_null_verb": 0.04097874743112574,
                "mAP_standard_all_actions": 0.5420416712261032,
                "mAP_present_only_all_actions": 0.08477565856193564,
                "mAP_freq_weighted_all_actions": 0.46889075006119296,
                "mAP_sample_wise_all_actions": 0.04097874743112574,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.894841650671785,
                "precision_with_null_verb": 0.4920910434067196,
                "recall_with_null_verb": 0.46138649193826325,
                "f1_with_null_verb": 0.4742900789985722,
                "num_predictions": 2084,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5653464735115886,
                "mAP_present_only": 0.09738947773133301,
                "mAP_freq_weighted": 0.49246074867321427,
                "exact_match": 0.0,
                "hamming_accuracy": 0.897302650385919,
                "precision": 0.49214739052064227,
                "recall": 0.46305103599506314,
                "f1": 0.47515624578104554,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08875441870896118,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.760126500172368,
                "mAP_present_only_with_null_verb": 0.07740961604756948,
                "mAP_freq_weighted_with_null_verb": 0.42560439947609835,
                "mAP_sample_wise_with_null_verb": 0.2607625703890151,
                "mAP_standard_all_actions": 0.760126500172368,
                "mAP_present_only_all_actions": 0.07740961604756948,
                "mAP_freq_weighted_all_actions": 0.42560439947609835,
                "mAP_sample_wise_all_actions": 0.2607625703890151,
                "exact_match_with_null_verb": 0.030230326295585412,
                "hamming_accuracy_with_null_verb": 0.9831525911708253,
                "precision_with_null_verb": 0.49157629558541266,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4957523669877495,
                "num_predictions": 2084,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7867297575701825,
                "mAP_present_only": 0.08875441870896118,
                "mAP_freq_weighted": 0.44677391335827504,
                "exact_match": 0.03502879078694818,
                "hamming_accuracy": 0.9829807653040389,
                "precision": 0.49149038265201944,
                "recall": 0.5,
                "f1": 0.495708673781979,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.12797766312045977,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.09738947773133301,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08875441870896118,
                "exact_match": 0.03502879078694818,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID29": {
          "video_id": "VID29",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08142288532818953,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.6795690844775989,
                "mAP_present_only_with_null_verb": 0.07526570952922645,
                "mAP_freq_weighted_with_null_verb": 0.21871500500106372,
                "mAP_sample_wise_with_null_verb": 0.027486242852938447,
                "mAP_standard_all_actions": 0.6795690844775989,
                "mAP_present_only_all_actions": 0.07526570952922645,
                "mAP_freq_weighted_all_actions": 0.21871500500106372,
                "mAP_sample_wise_all_actions": 0.027486242852938447,
                "exact_match_with_null_verb": 0.006382978723404255,
                "hamming_accuracy_with_null_verb": 0.9303957446808511,
                "precision_with_null_verb": 0.49434804115590825,
                "recall_with_null_verb": 0.47793979511504725,
                "f1_with_null_verb": 0.4844623158112804,
                "num_predictions": 2350,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7007736847079613,
                "mAP_present_only": 0.08142288532818953,
                "mAP_freq_weighted": 0.2299896334707713,
                "exact_match": 0.006808510638297872,
                "hamming_accuracy": 0.9296921684019919,
                "precision": 0.4944348950578845,
                "recall": 0.47798188697797006,
                "f1": 0.48440514190671274,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08629574895620813,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.5305305534339398,
                "mAP_present_only_with_null_verb": 0.07896366705361482,
                "mAP_freq_weighted_with_null_verb": 0.3252748663594508,
                "mAP_sample_wise_with_null_verb": 0.03896437794956846,
                "mAP_standard_all_actions": 0.5305305534339398,
                "mAP_present_only_all_actions": 0.07896366705361482,
                "mAP_freq_weighted_all_actions": 0.3252748663594508,
                "mAP_sample_wise_all_actions": 0.03896437794956846,
                "exact_match_with_null_verb": 0.000851063829787234,
                "hamming_accuracy_with_null_verb": 0.9186553191489362,
                "precision_with_null_verb": 0.49465495620392286,
                "recall_with_null_verb": 0.47503372661603216,
                "f1_with_null_verb": 0.481966713376127,
                "num_predictions": 2350,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5423915130424765,
                "mAP_present_only": 0.08629574895620813,
                "mAP_freq_weighted": 0.3441751300171472,
                "exact_match": 0.000851063829787234,
                "hamming_accuracy": 0.9210276143051155,
                "precision": 0.4948477833423193,
                "recall": 0.4766716003787212,
                "f1": 0.48285534547909853,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08539683005128375,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.7603796046371601,
                "mAP_present_only_with_null_verb": 0.07838309475830771,
                "mAP_freq_weighted_with_null_verb": 0.3544318835764663,
                "mAP_sample_wise_with_null_verb": 0.12749309512380724,
                "mAP_standard_all_actions": 0.7603796046371601,
                "mAP_present_only_all_actions": 0.07838309475830771,
                "mAP_freq_weighted_all_actions": 0.3544318835764663,
                "mAP_sample_wise_all_actions": 0.12749309512380724,
                "exact_match_with_null_verb": 0.10127659574468086,
                "hamming_accuracy_with_null_verb": 0.9862255319148936,
                "precision_with_null_verb": 0.4931127659574468,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49653250150504646,
                "num_predictions": 2350,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7762141179912716,
                "mAP_present_only": 0.08539683005128375,
                "mAP_freq_weighted": 0.3749887857075714,
                "exact_match": 0.12340425531914893,
                "hamming_accuracy": 0.9862426437301947,
                "precision": 0.49312132186509733,
                "recall": 0.5,
                "f1": 0.49653683896244194,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08142288532818953,
                "exact_match": 0.006808510638297872,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08629574895620813,
                "exact_match": 0.000851063829787234,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08539683005128375,
                "exact_match": 0.12340425531914893,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID31": {
          "video_id": "VID31",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.11715269839945425,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.7166569856945699,
                "mAP_present_only_with_null_verb": 0.10252686805603789,
                "mAP_freq_weighted_with_null_verb": 0.4784691889956112,
                "mAP_sample_wise_with_null_verb": 0.04035562740397984,
                "mAP_standard_all_actions": 0.7166569856945699,
                "mAP_present_only_all_actions": 0.10252686805603789,
                "mAP_freq_weighted_all_actions": 0.4784691889956112,
                "mAP_sample_wise_all_actions": 0.04035562740397984,
                "exact_match_with_null_verb": 0.0012674271229404308,
                "hamming_accuracy_with_null_verb": 0.9363168567807351,
                "precision_with_null_verb": 0.49763088347588236,
                "recall_with_null_verb": 0.4944248062015504,
                "f1_with_null_verb": 0.49379807838084283,
                "num_predictions": 3945,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7508229719658297,
                "mAP_present_only": 0.11715269839945425,
                "mAP_freq_weighted": 0.49726063664466286,
                "exact_match": 0.0022813688212927757,
                "hamming_accuracy": 0.9343877248334816,
                "precision": 0.49763713626825196,
                "recall": 0.4943602648750468,
                "f1": 0.4936086329042433,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.10551421106082098,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.5139838120528375,
                "mAP_present_only_with_null_verb": 0.09224543097245198,
                "mAP_freq_weighted_with_null_verb": 0.5358845063220538,
                "mAP_sample_wise_with_null_verb": 0.04569923920185459,
                "mAP_standard_all_actions": 0.5139838120528375,
                "mAP_present_only_all_actions": 0.09224543097245198,
                "mAP_freq_weighted_all_actions": 0.5358845063220538,
                "mAP_sample_wise_all_actions": 0.04569923920185459,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.874020278833967,
                "precision_with_null_verb": 0.493706169549647,
                "recall_with_null_verb": 0.4673144702842377,
                "f1_with_null_verb": 0.47298877286365915,
                "num_predictions": 3945,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5353331132270006,
                "mAP_present_only": 0.10551421106082098,
                "mAP_freq_weighted": 0.5574305180608133,
                "exact_match": 0.00025348542458808617,
                "hamming_accuracy": 0.8768654100261576,
                "precision": 0.49330180121494455,
                "recall": 0.46680308254919384,
                "f1": 0.4734194803140548,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.10555424061882722,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.7648776809591264,
                "mAP_present_only_with_null_verb": 0.09568338830433243,
                "mAP_freq_weighted_with_null_verb": 0.5374606453150347,
                "mAP_sample_wise_with_null_verb": 0.28472158537312875,
                "mAP_standard_all_actions": 0.7648776809591264,
                "mAP_present_only_all_actions": 0.09568338830433243,
                "mAP_freq_weighted_all_actions": 0.5374606453150347,
                "mAP_sample_wise_all_actions": 0.28472158537312875,
                "exact_match_with_null_verb": 0.03624841571609633,
                "hamming_accuracy_with_null_verb": 0.9809885931558935,
                "precision_with_null_verb": 0.49049429657794674,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4952015355086372,
                "num_predictions": 3945,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7906616307831297,
                "mAP_present_only": 0.10555424061882722,
                "mAP_freq_weighted": 0.5588689341104984,
                "exact_match": 0.04258555133079848,
                "hamming_accuracy": 0.9806137583259176,
                "precision": 0.4903068791629588,
                "recall": 0.5,
                "f1": 0.49510600146227696,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.11715269839945425,
                "exact_match": 0.0022813688212927757,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.10551421106082098,
                "exact_match": 0.00025348542458808617,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.10555424061882722,
                "exact_match": 0.04258555133079848,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID32": {
          "video_id": "VID32",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.07985878389239381,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "32",
                  "subset_action_sparsity": 0.6595744680851063
                },
                "mAP_standard_with_null_verb": 0.5758623041422886,
                "mAP_present_only_with_null_verb": 0.06989811930348266,
                "mAP_freq_weighted_with_null_verb": 0.3109772594096373,
                "mAP_sample_wise_with_null_verb": 0.03434843104918041,
                "mAP_standard_all_actions": 0.5758623041422886,
                "mAP_present_only_all_actions": 0.06989811930348266,
                "mAP_freq_weighted_all_actions": 0.3109772594096373,
                "mAP_sample_wise_all_actions": 0.03434843104918041,
                "exact_match_with_null_verb": 0.0023629489603024575,
                "hamming_accuracy_with_null_verb": 0.9294187145557656,
                "precision_with_null_verb": 0.5033931033881086,
                "recall_with_null_verb": 0.510494772079969,
                "f1_with_null_verb": 0.4998165127120995,
                "num_predictions": 2116,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "37",
                "action_sparsity_with_null_verb": 0.63,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6122923519633682,
                "mAP_present_only": 0.07985878389239381,
                "mAP_freq_weighted": 0.31504169977491114,
                "exact_match": 0.0023629489603024575,
                "hamming_accuracy": 0.9286489965008246,
                "precision": 0.5037218816054715,
                "recall": 0.5109912534037275,
                "f1": 0.5005208792353304,
                "num_actions_total": 94,
                "num_actions_present": "32",
                "action_sparsity": 0.6595744680851063
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.07777334816248094,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "32",
                  "subset_action_sparsity": 0.6595744680851063
                },
                "mAP_standard_with_null_verb": 0.40512634638819994,
                "mAP_present_only_with_null_verb": 0.0679090442924321,
                "mAP_freq_weighted_with_null_verb": 0.3815957550636259,
                "mAP_sample_wise_with_null_verb": 0.03783342230108474,
                "mAP_standard_all_actions": 0.40512634638819994,
                "mAP_present_only_all_actions": 0.0679090442924321,
                "mAP_freq_weighted_all_actions": 0.3815957550636259,
                "mAP_sample_wise_all_actions": 0.03783342230108474,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9056332703213611,
                "precision_with_null_verb": 0.4956145003336274,
                "recall_with_null_verb": 0.48140904223790837,
                "f1_with_null_verb": 0.4828163199413158,
                "num_predictions": 2116,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "37",
                "action_sparsity_with_null_verb": 0.63,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.43073135256595096,
                "mAP_present_only": 0.07777334816248094,
                "mAP_freq_weighted": 0.3866466485097752,
                "exact_match": 0.0,
                "hamming_accuracy": 0.908020954832482,
                "precision": 0.49560136928059434,
                "recall": 0.4828826983476717,
                "f1": 0.48399713492735025,
                "num_actions_total": 94,
                "num_actions_present": "32",
                "action_sparsity": 0.6595744680851063
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.06787928798573493,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "32",
                  "subset_action_sparsity": 0.6595744680851063
                },
                "mAP_standard_with_null_verb": 0.6519881613446828,
                "mAP_present_only_with_null_verb": 0.05942746309373769,
                "mAP_freq_weighted_with_null_verb": 0.3587239481538624,
                "mAP_sample_wise_with_null_verb": 0.19431475489044253,
                "mAP_standard_all_actions": 0.6519881613446828,
                "mAP_present_only_all_actions": 0.05942746309373769,
                "mAP_freq_weighted_all_actions": 0.3587239481538624,
                "mAP_sample_wise_all_actions": 0.19431475489044253,
                "exact_match_with_null_verb": 0.09829867674858223,
                "hamming_accuracy_with_null_verb": 0.9826417769376181,
                "precision_with_null_verb": 0.49132088846880906,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49562245099838625,
                "num_predictions": 2116,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "37",
                "action_sparsity_with_null_verb": 0.63,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.6826823108036546,
                "mAP_present_only": 0.06787928798573493,
                "mAP_freq_weighted": 0.3634663783600275,
                "exact_match": 0.0997164461247637,
                "hamming_accuracy": 0.9817801552507742,
                "precision": 0.4908900776253871,
                "recall": 0.5,
                "f1": 0.49540316197511824,
                "num_actions_total": 94,
                "num_actions_present": "32",
                "action_sparsity": 0.6595744680851063
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.07985878389239381,
                "exact_match": 0.0023629489603024575,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.07777334816248094,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.06787928798573493,
                "exact_match": 0.0997164461247637,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID35": {
          "video_id": "VID35",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.10670756237959843,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.6481915082242828,
                "mAP_present_only_with_null_verb": 0.09721209732511318,
                "mAP_freq_weighted_with_null_verb": 0.2990563612461582,
                "mAP_sample_wise_with_null_verb": 0.02691839718436172,
                "mAP_standard_all_actions": 0.6481915082242828,
                "mAP_present_only_all_actions": 0.09721209732511318,
                "mAP_freq_weighted_all_actions": 0.2990563612461582,
                "mAP_sample_wise_all_actions": 0.02691839718436172,
                "exact_match_with_null_verb": 0.004273504273504274,
                "hamming_accuracy_with_null_verb": 0.9316951566951567,
                "precision_with_null_verb": 0.49457090497985945,
                "recall_with_null_verb": 0.48466074419251004,
                "f1_with_null_verb": 0.4877452708420784,
                "num_predictions": 2106,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.667812730019889,
                "mAP_present_only": 0.10670756237959843,
                "mAP_freq_weighted": 0.3064976769351143,
                "exact_match": 0.004748338081671415,
                "hamming_accuracy": 0.9305681841142834,
                "precision": 0.4944323804017587,
                "recall": 0.4846656287684996,
                "f1": 0.487692566494971,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0955260745380638,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.4952201622018981,
                "mAP_present_only_with_null_verb": 0.08696607655826899,
                "mAP_freq_weighted_with_null_verb": 0.3666666092508238,
                "mAP_sample_wise_with_null_verb": 0.04088967973278637,
                "mAP_standard_all_actions": 0.4952201622018981,
                "mAP_present_only_all_actions": 0.08696607655826899,
                "mAP_freq_weighted_all_actions": 0.3666666092508238,
                "mAP_sample_wise_all_actions": 0.04088967973278637,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8998622981956316,
                "precision_with_null_verb": 0.4909451089569044,
                "recall_with_null_verb": 0.4602398709348322,
                "f1_with_null_verb": 0.4744486334867969,
                "num_predictions": 2106,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5051455099786134,
                "mAP_present_only": 0.0955260745380638,
                "mAP_freq_weighted": 0.3759610147899409,
                "exact_match": 0.0004748338081671415,
                "hamming_accuracy": 0.9001485118506395,
                "precision": 0.49044356531858646,
                "recall": 0.4599274048677283,
                "f1": 0.4742796739182234,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08855300879370166,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "26",
                  "subset_action_sparsity": 0.7234042553191489
                },
                "mAP_standard_with_null_verb": 0.7367715858258391,
                "mAP_present_only_with_null_verb": 0.09231581319254865,
                "mAP_freq_weighted_with_null_verb": 0.38471851979403077,
                "mAP_sample_wise_with_null_verb": 0.20968777453971058,
                "mAP_standard_all_actions": 0.7367715858258391,
                "mAP_present_only_all_actions": 0.09231581319254865,
                "mAP_freq_weighted_all_actions": 0.38471851979403077,
                "mAP_sample_wise_all_actions": 0.20968777453971058,
                "exact_match_with_null_verb": 0.029439696106362774,
                "hamming_accuracy_with_null_verb": 0.9824121557454891,
                "precision_with_null_verb": 0.49120607787274456,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4955640293559699,
                "num_predictions": 2106,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "29",
                "action_sparsity_with_null_verb": 0.71,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7478976407301726,
                "mAP_present_only": 0.08855300879370166,
                "mAP_freq_weighted": 0.39283144065391606,
                "exact_match": 0.03561253561253561,
                "hamming_accuracy": 0.9817694126204765,
                "precision": 0.49088470631023823,
                "recall": 0.5,
                "f1": 0.49540042669358353,
                "num_actions_total": 94,
                "num_actions_present": "26",
                "action_sparsity": 0.7234042553191489
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.10670756237959843,
                "exact_match": 0.004748338081671415,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0955260745380638,
                "exact_match": 0.0004748338081671415,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08855300879370166,
                "exact_match": 0.03561253561253561,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID36": {
          "video_id": "VID36",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.11657757348919323,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.7326192660166249,
                "mAP_present_only_with_null_verb": 0.10771079055535715,
                "mAP_freq_weighted_with_null_verb": 0.35180844014126966,
                "mAP_sample_wise_with_null_verb": 0.022946293322684018,
                "mAP_standard_all_actions": 0.7326192660166249,
                "mAP_present_only_all_actions": 0.10771079055535715,
                "mAP_freq_weighted_all_actions": 0.35180844014126966,
                "mAP_sample_wise_all_actions": 0.022946293322684018,
                "exact_match_with_null_verb": 0.0008378718056137411,
                "hamming_accuracy_with_null_verb": 0.9215919564306662,
                "precision_with_null_verb": 0.4955068306126913,
                "recall_with_null_verb": 0.4809595012966343,
                "f1_with_null_verb": 0.48456228181805727,
                "num_predictions": 2387,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7469678074073902,
                "mAP_present_only": 0.11657757348919323,
                "mAP_freq_weighted": 0.3606049089832759,
                "exact_match": 0.0012568077084206116,
                "hamming_accuracy": 0.9213737532200127,
                "precision": 0.49545365890394927,
                "recall": 0.48145158220222356,
                "f1": 0.48480214160311086,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.10547097370040016,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.5002888817441207,
                "mAP_present_only_with_null_verb": 0.09661372259105054,
                "mAP_freq_weighted_with_null_verb": 0.49594913572032934,
                "mAP_sample_wise_with_null_verb": 0.036552119581574644,
                "mAP_standard_all_actions": 0.5002888817441207,
                "mAP_present_only_all_actions": 0.09661372259105054,
                "mAP_freq_weighted_all_actions": 0.49594913572032934,
                "mAP_sample_wise_all_actions": 0.036552119581574644,
                "exact_match_with_null_verb": 0.00041893590280687055,
                "hamming_accuracy_with_null_verb": 0.8973690825303728,
                "precision_with_null_verb": 0.4933480638623406,
                "recall_with_null_verb": 0.46240327833221995,
                "f1_with_null_verb": 0.4749811266246486,
                "num_predictions": 2387,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5000420053224214,
                "mAP_present_only": 0.10547097370040016,
                "mAP_freq_weighted": 0.5090341943704741,
                "exact_match": 0.00041893590280687055,
                "hamming_accuracy": 0.8985462032819617,
                "precision": 0.49299205442158106,
                "recall": 0.4624057949684276,
                "f1": 0.475157631324552,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.09810959615114499,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.808969589245321,
                "mAP_present_only_with_null_verb": 0.09033137735867157,
                "mAP_freq_weighted_with_null_verb": 0.4986934007001163,
                "mAP_sample_wise_with_null_verb": 0.22397955897509364,
                "mAP_standard_all_actions": 0.808969589245321,
                "mAP_present_only_all_actions": 0.09033137735867157,
                "mAP_freq_weighted_all_actions": 0.4986934007001163,
                "mAP_sample_wise_all_actions": 0.22397955897509364,
                "exact_match_with_null_verb": 0.09635525764558023,
                "hamming_accuracy_with_null_verb": 0.9855090071219104,
                "precision_with_null_verb": 0.4927545035609552,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4963508115989121,
                "num_predictions": 2387,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.817703003477359,
                "mAP_present_only": 0.09810959615114499,
                "mAP_freq_weighted": 0.5116357465168834,
                "exact_match": 0.10347716799329702,
                "hamming_accuracy": 0.9849940725026518,
                "precision": 0.4924970362513259,
                "recall": 0.5,
                "f1": 0.4962201581089789,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.11657757348919323,
                "exact_match": 0.0012568077084206116,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.10547097370040016,
                "exact_match": 0.00041893590280687055,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.09810959615114499,
                "exact_match": 0.10347716799329702,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID40": {
          "video_id": "VID40",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.10649758405597068,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.7382253636706104,
                "mAP_present_only_with_null_verb": 0.09112681835305232,
                "mAP_freq_weighted_with_null_verb": 0.35449083005412146,
                "mAP_sample_wise_with_null_verb": 0.02222540884773455,
                "mAP_standard_all_actions": 0.7382253636706104,
                "mAP_present_only_all_actions": 0.09112681835305232,
                "mAP_freq_weighted_all_actions": 0.35449083005412146,
                "mAP_sample_wise_all_actions": 0.02222540884773455,
                "exact_match_with_null_verb": 0.004050405040504051,
                "hamming_accuracy_with_null_verb": 0.9322682268226823,
                "precision_with_null_verb": 0.4921691706678707,
                "recall_with_null_verb": 0.4742690151419498,
                "f1_with_null_verb": 0.4828713700699601,
                "num_predictions": 2222,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7734463972861226,
                "mAP_present_only": 0.10649758405597068,
                "mAP_freq_weighted": 0.3729430408842112,
                "exact_match": 0.004050405040504051,
                "hamming_accuracy": 0.9311287511729897,
                "precision": 0.49214695312517537,
                "recall": 0.4737651603280971,
                "f1": 0.48258435939997957,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.11834006215115622,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.5396636789392923,
                "mAP_present_only_with_null_verb": 0.09831839469646185,
                "mAP_freq_weighted_with_null_verb": 0.451840923273336,
                "mAP_sample_wise_with_null_verb": 0.04033861566192808,
                "mAP_standard_all_actions": 0.5396636789392923,
                "mAP_present_only_all_actions": 0.09831839469646185,
                "mAP_freq_weighted_all_actions": 0.451840923273336,
                "mAP_sample_wise_all_actions": 0.04033861566192808,
                "exact_match_with_null_verb": 0.00045004500450045,
                "hamming_accuracy_with_null_verb": 0.8926282628262826,
                "precision_with_null_verb": 0.4924809850909118,
                "recall_with_null_verb": 0.45832337313646976,
                "f1_with_null_verb": 0.4730922235811998,
                "num_predictions": 2222,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5626961807916862,
                "mAP_present_only": 0.11834006215115622,
                "mAP_freq_weighted": 0.4774641307021873,
                "exact_match": 0.00045004500450045,
                "hamming_accuracy": 0.8955704081046403,
                "precision": 0.49184034978804303,
                "recall": 0.4563198411113449,
                "f1": 0.47291096111836356,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.12179411302903899,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.820399848521311,
                "mAP_present_only_with_null_verb": 0.10199924260655428,
                "mAP_freq_weighted_with_null_verb": 0.4580970116989769,
                "mAP_sample_wise_with_null_verb": 0.306260234022148,
                "mAP_standard_all_actions": 0.820399848521311,
                "mAP_present_only_all_actions": 0.10199924260655428,
                "mAP_freq_weighted_all_actions": 0.4580970116989769,
                "mAP_sample_wise_all_actions": 0.306260234022148,
                "exact_match_with_null_verb": 0.13906390639063906,
                "hamming_accuracy_with_null_verb": 0.9846444644464446,
                "precision_with_null_verb": 0.4923222322232223,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49613141400673033,
                "num_predictions": 2222,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "20",
                "action_sparsity_with_null_verb": 0.8,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.85051814689856,
                "mAP_present_only": 0.12179411302903899,
                "mAP_freq_weighted": 0.48408116876261464,
                "exact_match": 0.14806480648064807,
                "hamming_accuracy": 0.9845979278778941,
                "precision": 0.49229896393894707,
                "recall": 0.5,
                "f1": 0.4961195988603659,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.10649758405597068,
                "exact_match": 0.004050405040504051,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.11834006215115622,
                "exact_match": 0.00045004500450045,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.12179411302903899,
                "exact_match": 0.14806480648064807,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID42": {
          "video_id": "VID42",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08423857810844558,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7281196800389097,
                "mAP_present_only_with_null_verb": 0.07878121756047697,
                "mAP_freq_weighted_with_null_verb": 0.3357636941327934,
                "mAP_sample_wise_with_null_verb": 0.024091276495455914,
                "mAP_standard_all_actions": 0.7281196800389097,
                "mAP_present_only_all_actions": 0.07878121756047697,
                "mAP_freq_weighted_all_actions": 0.3357636941327934,
                "mAP_sample_wise_all_actions": 0.024091276495455914,
                "exact_match_with_null_verb": 0.0010775862068965517,
                "hamming_accuracy_with_null_verb": 0.9272036637931035,
                "precision_with_null_verb": 0.4950877808120862,
                "recall_with_null_verb": 0.4820813220803748,
                "f1_with_null_verb": 0.48587166112353924,
                "num_predictions": 3712,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7422235121306102,
                "mAP_present_only": 0.08423857810844558,
                "mAP_freq_weighted": 0.3419043561680567,
                "exact_match": 0.0021551724137931034,
                "hamming_accuracy": 0.9269677411958914,
                "precision": 0.49496586024449263,
                "recall": 0.48249239579616915,
                "f1": 0.486092538779294,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.09611540623113674,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.5004446723570612,
                "mAP_present_only_with_null_verb": 0.08888987981331005,
                "mAP_freq_weighted_with_null_verb": 0.43567982012952894,
                "mAP_sample_wise_with_null_verb": 0.037097123505395437,
                "mAP_standard_all_actions": 0.5004446723570612,
                "mAP_present_only_all_actions": 0.08888987981331005,
                "mAP_freq_weighted_all_actions": 0.43567982012952894,
                "mAP_sample_wise_all_actions": 0.037097123505395437,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8977397629310345,
                "precision_with_null_verb": 0.4950663703773296,
                "recall_with_null_verb": 0.47363564439454287,
                "f1_with_null_verb": 0.4783845249490085,
                "num_predictions": 3712,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5001959950090837,
                "mAP_present_only": 0.09611540623113674,
                "mAP_freq_weighted": 0.4439989966161436,
                "exact_match": 0.00026939655172413793,
                "hamming_accuracy": 0.900443644534116,
                "precision": 0.49495674106561827,
                "recall": 0.4750351099161286,
                "f1": 0.47942699054248883,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.1036822358579067,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "21",
                  "subset_action_sparsity": 0.7765957446808511
                },
                "mAP_standard_with_null_verb": 0.7920724634000613,
                "mAP_present_only_with_null_verb": 0.0959672321741801,
                "mAP_freq_weighted_with_null_verb": 0.522334637646767,
                "mAP_sample_wise_with_null_verb": 0.24230917895899037,
                "mAP_standard_all_actions": 0.7920724634000613,
                "mAP_present_only_all_actions": 0.0959672321741801,
                "mAP_freq_weighted_all_actions": 0.522334637646767,
                "mAP_sample_wise_all_actions": 0.24230917895899037,
                "exact_match_with_null_verb": 0.0603448275862069,
                "hamming_accuracy_with_null_verb": 0.9847332974137931,
                "precision_with_null_verb": 0.49236664870689656,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49615396622657054,
                "num_predictions": 3712,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "23",
                "action_sparsity_with_null_verb": 0.77,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.799758797372511,
                "mAP_present_only": 0.1036822358579067,
                "mAP_freq_weighted": 0.5323885833973782,
                "exact_match": 0.06842672413793104,
                "hamming_accuracy": 0.9840740783198826,
                "precision": 0.4920370391599413,
                "recall": 0.5,
                "f1": 0.49598656072015124,
                "num_actions_total": 94,
                "num_actions_present": "21",
                "action_sparsity": 0.7765957446808511
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08423857810844558,
                "exact_match": 0.0021551724137931034,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.09611540623113674,
                "exact_match": 0.00026939655172413793,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.1036822358579067,
                "exact_match": 0.06842672413793104,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID43": {
          "video_id": "VID43",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.080359679286811,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "29",
                  "subset_action_sparsity": 0.6914893617021276
                },
                "mAP_standard_with_null_verb": 0.6257897453883349,
                "mAP_present_only_with_null_verb": 0.07815074360101487,
                "mAP_freq_weighted_with_null_verb": 0.1265577718747316,
                "mAP_sample_wise_with_null_verb": 0.04035793289259419,
                "mAP_standard_all_actions": 0.6257897453883349,
                "mAP_present_only_all_actions": 0.07815074360101487,
                "mAP_freq_weighted_all_actions": 0.1265577718747316,
                "mAP_sample_wise_all_actions": 0.04035793289259419,
                "exact_match_with_null_verb": 0.0021168501270110076,
                "hamming_accuracy_with_null_verb": 0.9208721422523285,
                "precision_with_null_verb": 0.49856854451422294,
                "recall_with_null_verb": 0.4924674949307593,
                "f1_with_null_verb": 0.4874952677824427,
                "num_predictions": 2362,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "33",
                "action_sparsity_with_null_verb": 0.6699999999999999,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.641813092545931,
                "mAP_present_only": 0.080359679286811,
                "mAP_freq_weighted": 0.1286403148329197,
                "exact_match": 0.002540220152413209,
                "hamming_accuracy": 0.9215279153980579,
                "precision": 0.49887452929127774,
                "recall": 0.49416492506988313,
                "f1": 0.48819608298435735,
                "num_actions_total": 94,
                "num_actions_present": "29",
                "action_sparsity": 0.6914893617021276
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.051959527065827495,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "29",
                  "subset_action_sparsity": 0.6914893617021276
                },
                "mAP_standard_with_null_verb": 0.4257815183801374,
                "mAP_present_only_with_null_verb": 0.04782278297011333,
                "mAP_freq_weighted_with_null_verb": 0.10842070913750648,
                "mAP_sample_wise_with_null_verb": 0.03944446070544982,
                "mAP_standard_all_actions": 0.4257815183801374,
                "mAP_present_only_all_actions": 0.04782278297011333,
                "mAP_freq_weighted_all_actions": 0.10842070913750648,
                "mAP_sample_wise_all_actions": 0.03944446070544982,
                "exact_match_with_null_verb": 0.0021168501270110076,
                "hamming_accuracy_with_null_verb": 0.9332895850973751,
                "precision_with_null_verb": 0.4995963172897667,
                "recall_with_null_verb": 0.49824010818939296,
                "f1_with_null_verb": 0.49213832531003293,
                "num_predictions": 2362,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "33",
                "action_sparsity_with_null_verb": 0.6699999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.4309236838820106,
                "mAP_present_only": 0.051959527065827495,
                "mAP_freq_weighted": 0.1129029693536513,
                "exact_match": 0.0021168501270110076,
                "hamming_accuracy": 0.9359855513718991,
                "precision": 0.49917530336553556,
                "recall": 0.4966020827576843,
                "f1": 0.49216820217770246,
                "num_actions_total": 94,
                "num_actions_present": "29",
                "action_sparsity": 0.6914893617021276
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08004787458195962,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "29",
                  "subset_action_sparsity": 0.6914893617021276
                },
                "mAP_standard_with_null_verb": 0.6937769543494925,
                "mAP_present_only_with_null_verb": 0.07205137681664393,
                "mAP_freq_weighted_with_null_verb": 0.19460616567501143,
                "mAP_sample_wise_with_null_verb": 0.09970189575310706,
                "mAP_standard_all_actions": 0.6937769543494925,
                "mAP_present_only_all_actions": 0.07205137681664393,
                "mAP_freq_weighted_all_actions": 0.19460616567501143,
                "mAP_sample_wise_all_actions": 0.09970189575310706,
                "exact_match_with_null_verb": 0.14860287891617274,
                "hamming_accuracy_with_null_verb": 0.9877730736663844,
                "precision_with_null_verb": 0.4938865368331922,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4969244662543236,
                "num_predictions": 2362,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "33",
                "action_sparsity_with_null_verb": 0.6699999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.716184982583796,
                "mAP_present_only": 0.08004787458195962,
                "mAP_freq_weighted": 0.2043577796920254,
                "exact_match": 0.1651143099068586,
                "hamming_accuracy": 0.9876997495811339,
                "precision": 0.49384987479056697,
                "recall": 0.5,
                "f1": 0.4969059083441908,
                "num_actions_total": 94,
                "num_actions_present": "29",
                "action_sparsity": 0.6914893617021276
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.080359679286811,
                "exact_match": 0.002540220152413209,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.051959527065827495,
                "exact_match": 0.0021168501270110076,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08004787458195962,
                "exact_match": 0.1651143099068586,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID47": {
          "video_id": "VID47",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.0849644171888556,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "29",
                  "subset_action_sparsity": 0.6914893617021276
                },
                "mAP_standard_with_null_verb": 0.6255045342373727,
                "mAP_present_only_with_null_verb": 0.0750133359922727,
                "mAP_freq_weighted_with_null_verb": 0.2917282967050521,
                "mAP_sample_wise_with_null_verb": 0.027845498897601403,
                "mAP_standard_all_actions": 0.6255045342373727,
                "mAP_present_only_all_actions": 0.0750133359922727,
                "mAP_freq_weighted_all_actions": 0.2917282967050521,
                "mAP_sample_wise_all_actions": 0.027845498897601403,
                "exact_match_with_null_verb": 0.005312084993359893,
                "hamming_accuracy_with_null_verb": 0.9306418769366976,
                "precision_with_null_verb": 0.4962936937896375,
                "recall_with_null_verb": 0.4871236994544159,
                "f1_with_null_verb": 0.48845555371751237,
                "num_predictions": 2259,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6538720010476257,
                "mAP_present_only": 0.0849644171888556,
                "mAP_freq_weighted": 0.3033018382730764,
                "exact_match": 0.006640106241699867,
                "hamming_accuracy": 0.9306226630122536,
                "precision": 0.49643905942223815,
                "recall": 0.48789414624174476,
                "f1": 0.48885258575481133,
                "num_actions_total": 94,
                "num_actions_present": "29",
                "action_sparsity": 0.6914893617021276
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.0811692535132995,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "29",
                  "subset_action_sparsity": 0.6914893617021276
                },
                "mAP_standard_with_null_verb": 0.4441845011239545,
                "mAP_present_only_with_null_verb": 0.07113088565868984,
                "mAP_freq_weighted_with_null_verb": 0.3730459158633681,
                "mAP_sample_wise_with_null_verb": 0.03411313767863151,
                "mAP_standard_all_actions": 0.4441845011239545,
                "mAP_present_only_all_actions": 0.07113088565868984,
                "mAP_freq_weighted_all_actions": 0.3730459158633681,
                "mAP_sample_wise_all_actions": 0.03411313767863151,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9075475874280655,
                "precision_with_null_verb": 0.49311713866570867,
                "recall_with_null_verb": 0.46708611344289774,
                "f1_with_null_verb": 0.4778596681084417,
                "num_predictions": 2259,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.46121179097750736,
                "mAP_present_only": 0.0811692535132995,
                "mAP_freq_weighted": 0.38871403716220804,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9100430429581908,
                "precision": 0.4926512404791863,
                "recall": 0.4666690769261922,
                "f1": 0.47801370352960293,
                "num_actions_total": 94,
                "num_actions_present": "29",
                "action_sparsity": 0.6914893617021276
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.06832422706648252,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "29",
                  "subset_action_sparsity": 0.6914893617021276
                },
                "mAP_standard_with_null_verb": 0.6804089650459957,
                "mAP_present_only_with_null_verb": 0.06002636778234011,
                "mAP_freq_weighted_with_null_verb": 0.40123779950875554,
                "mAP_sample_wise_with_null_verb": 0.20476299776929283,
                "mAP_standard_all_actions": 0.6804089650459957,
                "mAP_present_only_all_actions": 0.06002636778234011,
                "mAP_freq_weighted_all_actions": 0.40123779950875554,
                "mAP_sample_wise_all_actions": 0.20476299776929283,
                "exact_match_with_null_verb": 0.07215582115980522,
                "hamming_accuracy_with_null_verb": 0.9847941567065073,
                "precision_with_null_verb": 0.49239707835325364,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4961694155431401,
                "num_predictions": 2259,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "34",
                "action_sparsity_with_null_verb": 0.6599999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7125681126056169,
                "mAP_present_only": 0.06832422706648252,
                "mAP_freq_weighted": 0.4183119671440799,
                "exact_match": 0.07702523240371846,
                "hamming_accuracy": 0.9845111280645739,
                "precision": 0.49225556403228693,
                "recall": 0.5,
                "f1": 0.49609755981803644,
                "num_actions_total": 94,
                "num_actions_present": "29",
                "action_sparsity": 0.6914893617021276
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.0849644171888556,
                "exact_match": 0.006640106241699867,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.0811692535132995,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.06832422706648252,
                "exact_match": 0.07702523240371846,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID48": {
          "video_id": "VID48",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.16292556200374186,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.7477835036058341,
                "mAP_present_only_with_null_verb": 0.1462289663464956,
                "mAP_freq_weighted_with_null_verb": 0.3405264368073074,
                "mAP_sample_wise_with_null_verb": 0.0250760882218357,
                "mAP_standard_all_actions": 0.7477835036058341,
                "mAP_present_only_all_actions": 0.1462289663464956,
                "mAP_freq_weighted_all_actions": 0.3405264368073074,
                "mAP_sample_wise_all_actions": 0.0250760882218357,
                "exact_match_with_null_verb": 0.0010905125408942203,
                "hamming_accuracy_with_null_verb": 0.9323227917121046,
                "precision_with_null_verb": 0.49569096317621625,
                "recall_with_null_verb": 0.4853926021961985,
                "f1_with_null_verb": 0.4879005877325158,
                "num_predictions": 1834,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "19",
                "action_sparsity_with_null_verb": 0.81,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7724128616176582,
                "mAP_present_only": 0.16292556200374186,
                "mAP_freq_weighted": 0.3640167870583647,
                "exact_match": 0.0016357688113413304,
                "hamming_accuracy": 0.9330378895101975,
                "precision": 0.49614898092954973,
                "recall": 0.48668085314693615,
                "f1": 0.48849441234459134,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.1093850173995024,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.5685754993825975,
                "mAP_present_only_with_null_verb": 0.09776578622419752,
                "mAP_freq_weighted_with_null_verb": 0.349706284180563,
                "mAP_sample_wise_with_null_verb": 0.035947188399955386,
                "mAP_standard_all_actions": 0.5685754993825975,
                "mAP_present_only_all_actions": 0.09776578622419752,
                "mAP_freq_weighted_all_actions": 0.349706284180563,
                "mAP_sample_wise_all_actions": 0.035947188399955386,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9006761177753544,
                "precision_with_null_verb": 0.4960563837875088,
                "recall_with_null_verb": 0.47927242842759815,
                "f1_with_null_verb": 0.48056860297915904,
                "num_predictions": 1834,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "19",
                "action_sparsity_with_null_verb": 0.81,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5824485135999153,
                "mAP_present_only": 0.1093850173995024,
                "mAP_freq_weighted": 0.3766714823333878,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9019292791016033,
                "precision": 0.4943673021471471,
                "recall": 0.4701183244055919,
                "f1": 0.477964997010271,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.11455897249424929,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.8296128270344444,
                "mAP_present_only_with_null_verb": 0.10322540544444457,
                "mAP_freq_weighted_with_null_verb": 0.41426773431948877,
                "mAP_sample_wise_with_null_verb": 0.24874384944052827,
                "mAP_standard_all_actions": 0.8296128270344444,
                "mAP_present_only_all_actions": 0.10322540544444457,
                "mAP_freq_weighted_all_actions": 0.41426773431948877,
                "mAP_sample_wise_all_actions": 0.24874384944052827,
                "exact_match_with_null_verb": 0.05179934569247546,
                "hamming_accuracy_with_null_verb": 0.9848854961832061,
                "precision_with_null_verb": 0.49244274809160304,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49619260056918696,
                "num_predictions": 1834,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "19",
                "action_sparsity_with_null_verb": 0.81,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8492866336160424,
                "mAP_present_only": 0.11455897249424929,
                "mAP_freq_weighted": 0.4456037109257399,
                "exact_match": 0.064340239912759,
                "hamming_accuracy": 0.9852664795006845,
                "precision": 0.49263323975034223,
                "recall": 0.5,
                "f1": 0.49628928391945115,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.16292556200374186,
                "exact_match": 0.0016357688113413304,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.1093850173995024,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.11455897249424929,
                "exact_match": 0.064340239912759,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID49": {
          "video_id": "VID49",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08268062502716265,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.6237656312773023,
                "mAP_present_only_with_null_verb": 0.07426759774156974,
                "mAP_freq_weighted_with_null_verb": 0.2336710129982374,
                "mAP_sample_wise_with_null_verb": 0.037704957649041165,
                "mAP_standard_all_actions": 0.6237656312773023,
                "mAP_present_only_all_actions": 0.07426759774156974,
                "mAP_freq_weighted_all_actions": 0.2336710129982374,
                "mAP_sample_wise_all_actions": 0.037704957649041165,
                "exact_match_with_null_verb": 0.0023937761819269898,
                "hamming_accuracy_with_null_verb": 0.9244045481747457,
                "precision_with_null_verb": 0.4996495996956751,
                "recall_with_null_verb": 0.4984333695533132,
                "f1_with_null_verb": 0.49095720979775714,
                "num_predictions": 1671,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "32",
                "action_sparsity_with_null_verb": 0.6799999999999999,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.662046562507802,
                "mAP_present_only": 0.08268062502716265,
                "mAP_freq_weighted": 0.2527553379019774,
                "exact_match": 0.004189108318372232,
                "hamming_accuracy": 0.9253791206692387,
                "precision": 0.5003654988476376,
                "recall": 0.5016751021732206,
                "f1": 0.49202562280534407,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.07381583033712903,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.4910111333328966,
                "mAP_present_only_with_null_verb": 0.06565979166530177,
                "mAP_freq_weighted_with_null_verb": 0.32284521518588216,
                "mAP_sample_wise_with_null_verb": 0.034829220659547244,
                "mAP_standard_all_actions": 0.4910111333328966,
                "mAP_present_only_all_actions": 0.06565979166530177,
                "mAP_freq_weighted_all_actions": 0.32284521518588216,
                "mAP_sample_wise_all_actions": 0.034829220659547244,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9210053859964094,
                "precision_with_null_verb": 0.49339699908112383,
                "recall_with_null_verb": 0.4696529807397036,
                "f1_with_null_verb": 0.4804204992902241,
                "num_predictions": 1671,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "32",
                "action_sparsity_with_null_verb": 0.6799999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.521202419352154,
                "mAP_present_only": 0.07381583033712903,
                "mAP_freq_weighted": 0.35141431606107254,
                "exact_match": 0.0,
                "hamming_accuracy": 0.924697913085552,
                "precision": 0.4937208834667389,
                "recall": 0.47160359897392706,
                "f1": 0.48153294026178445,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.0745489951881151,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.7013732405193883,
                "mAP_present_only_with_null_verb": 0.06679137662308843,
                "mAP_freq_weighted_with_null_verb": 0.34876909431126835,
                "mAP_sample_wise_with_null_verb": 0.1393758973299333,
                "mAP_standard_all_actions": 0.7013732405193883,
                "mAP_present_only_all_actions": 0.06679137662308843,
                "mAP_freq_weighted_all_actions": 0.34876909431126835,
                "mAP_sample_wise_all_actions": 0.1393758973299333,
                "exact_match_with_null_verb": 0.14302812687013763,
                "hamming_accuracy_with_null_verb": 0.9864751645721125,
                "precision_with_null_verb": 0.49323758228605624,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49659577031993735,
                "num_predictions": 1671,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "32",
                "action_sparsity_with_null_verb": 0.6799999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7341789667029692,
                "mAP_present_only": 0.0745489951881151,
                "mAP_freq_weighted": 0.3796809948100563,
                "exact_match": 0.1663674446439258,
                "hamming_accuracy": 0.9868978952595592,
                "precision": 0.4934489476297796,
                "recall": 0.5,
                "f1": 0.49670287417091225,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08268062502716265,
                "exact_match": 0.004189108318372232,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.07381583033712903,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.0745489951881151,
                "exact_match": 0.1663674446439258,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID52": {
          "video_id": "VID52",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08470002053899013,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "32",
                  "subset_action_sparsity": 0.6595744680851063
                },
                "mAP_standard_with_null_verb": 0.6087209621338239,
                "mAP_present_only_with_null_verb": 0.07762422198330757,
                "mAP_freq_weighted_with_null_verb": 0.2922610754744261,
                "mAP_sample_wise_with_null_verb": 0.03235304884328982,
                "mAP_standard_all_actions": 0.6087209621338239,
                "mAP_present_only_all_actions": 0.07762422198330757,
                "mAP_freq_weighted_all_actions": 0.2922610754744261,
                "mAP_sample_wise_all_actions": 0.03235304884328982,
                "exact_match_with_null_verb": 0.000508646998982706,
                "hamming_accuracy_with_null_verb": 0.9279094608341811,
                "precision_with_null_verb": 0.4966166950640685,
                "recall_with_null_verb": 0.48929280842062695,
                "f1_with_null_verb": 0.48948112027123836,
                "num_predictions": 1966,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "37",
                "action_sparsity_with_null_verb": 0.63,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6352170282685924,
                "mAP_present_only": 0.08470002053899013,
                "mAP_freq_weighted": 0.3169998465957467,
                "exact_match": 0.000508646998982706,
                "hamming_accuracy": 0.9280535053353823,
                "precision": 0.49721897228082396,
                "recall": 0.49083319788934676,
                "f1": 0.4900498994065364,
                "num_actions_total": 94,
                "num_actions_present": "32",
                "action_sparsity": 0.6595744680851063
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08203986800065535,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "32",
                  "subset_action_sparsity": 0.6595744680851063
                },
                "mAP_standard_with_null_verb": 0.41754476987666933,
                "mAP_present_only_with_null_verb": 0.07444532399099818,
                "mAP_freq_weighted_with_null_verb": 0.40502970956395645,
                "mAP_sample_wise_with_null_verb": 0.04078876806514723,
                "mAP_standard_all_actions": 0.41754476987666933,
                "mAP_present_only_all_actions": 0.07444532399099818,
                "mAP_freq_weighted_all_actions": 0.40502970956395645,
                "mAP_sample_wise_all_actions": 0.04078876806514723,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.906088504577823,
                "precision_with_null_verb": 0.4944658907104772,
                "recall_with_null_verb": 0.4762886181556901,
                "f1_with_null_verb": 0.48097466895401464,
                "num_predictions": 1966,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "37",
                "action_sparsity_with_null_verb": 0.63,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.43218378485128695,
                "mAP_present_only": 0.08203986800065535,
                "mAP_freq_weighted": 0.4421697315001894,
                "exact_match": 0.001017293997965412,
                "hamming_accuracy": 0.9096664574359863,
                "precision": 0.49490706592321837,
                "recall": 0.4782617290132925,
                "f1": 0.4821374524784442,
                "num_actions_total": 94,
                "num_actions_present": "32",
                "action_sparsity": 0.6595744680851063
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.06749925326021794,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "32",
                  "subset_action_sparsity": 0.6595744680851063
                },
                "mAP_standard_with_null_verb": 0.6529824880742582,
                "mAP_present_only_with_null_verb": 0.062114832633130786,
                "mAP_freq_weighted_with_null_verb": 0.40311852833512773,
                "mAP_sample_wise_with_null_verb": 0.2299282498157626,
                "mAP_standard_all_actions": 0.6529824880742582,
                "mAP_present_only_all_actions": 0.062114832633130786,
                "mAP_freq_weighted_all_actions": 0.40311852833512773,
                "mAP_sample_wise_all_actions": 0.2299282498157626,
                "exact_match_with_null_verb": 0.04984740590030519,
                "hamming_accuracy_with_null_verb": 0.98293997965412,
                "precision_with_null_verb": 0.49146998982706,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49569830138054516,
                "num_predictions": 1966,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "37",
                "action_sparsity_with_null_verb": 0.63,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.6825529372800742,
                "mAP_present_only": 0.06749925326021794,
                "mAP_freq_weighted": 0.43972192783939457,
                "exact_match": 0.05747711088504578,
                "hamming_accuracy": 0.9834960282244973,
                "precision": 0.49174801411224867,
                "recall": 0.5,
                "f1": 0.4958396761221962,
                "num_actions_total": 94,
                "num_actions_present": "32",
                "action_sparsity": 0.6595744680851063
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08470002053899013,
                "exact_match": 0.000508646998982706,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08203986800065535,
                "exact_match": 0.001017293997965412,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.06749925326021794,
                "exact_match": 0.05747711088504578,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID56": {
          "video_id": "VID56",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.12246571205162982,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "14",
                  "subset_action_sparsity": 0.851063829787234
                },
                "mAP_standard_with_null_verb": 0.7487548281788851,
                "mAP_present_only_with_null_verb": 0.11032251869932358,
                "mAP_freq_weighted_with_null_verb": 0.31049130870620434,
                "mAP_sample_wise_with_null_verb": 0.0294514641938179,
                "mAP_standard_all_actions": 0.7487548281788851,
                "mAP_present_only_all_actions": 0.11032251869932358,
                "mAP_freq_weighted_all_actions": 0.31049130870620434,
                "mAP_sample_wise_all_actions": 0.0294514641938179,
                "exact_match_with_null_verb": 0.0032679738562091504,
                "hamming_accuracy_with_null_verb": 0.931900871459695,
                "precision_with_null_verb": 0.4977971495432563,
                "recall_with_null_verb": 0.49127294888074435,
                "f1_with_null_verb": 0.48985009169664445,
                "num_predictions": 1836,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "17",
                "action_sparsity_with_null_verb": 0.83,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7735587230715193,
                "mAP_present_only": 0.12246571205162982,
                "mAP_freq_weighted": 0.3411266785805183,
                "exact_match": 0.004357298474945534,
                "hamming_accuracy": 0.9323981365595884,
                "precision": 0.4984789270025536,
                "recall": 0.4935874036956603,
                "f1": 0.4905104721125702,
                "num_actions_total": 94,
                "num_actions_present": "14",
                "action_sparsity": 0.851063829787234
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.11100176981184652,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "14",
                  "subset_action_sparsity": 0.851063829787234
                },
                "mAP_standard_with_null_verb": 0.5869739104015574,
                "mAP_present_only_with_null_verb": 0.09984653177386732,
                "mAP_freq_weighted_with_null_verb": 0.3995766880626888,
                "mAP_sample_wise_with_null_verb": 0.05362440521865321,
                "mAP_standard_all_actions": 0.5869739104015574,
                "mAP_present_only_all_actions": 0.09984653177386732,
                "mAP_freq_weighted_all_actions": 0.3995766880626888,
                "mAP_sample_wise_all_actions": 0.05362440521865321,
                "exact_match_with_null_verb": 0.0005446623093681918,
                "hamming_accuracy_with_null_verb": 0.9032407407407408,
                "precision_with_null_verb": 0.4956904870225052,
                "recall_with_null_verb": 0.474950451629458,
                "f1_with_null_verb": 0.4793624345174957,
                "num_predictions": 1836,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "17",
                "action_sparsity_with_null_verb": 0.83,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6016385614613389,
                "mAP_present_only": 0.11100176981184652,
                "mAP_freq_weighted": 0.442407357245101,
                "exact_match": 0.0005446623093681918,
                "hamming_accuracy": 0.9063876141473137,
                "precision": 0.49378844560070917,
                "recall": 0.4630769178704459,
                "f1": 0.4765566773447486,
                "num_actions_total": 94,
                "num_actions_present": "14",
                "action_sparsity": 0.851063829787234
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.12499656991843669,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "14",
                  "subset_action_sparsity": 0.851063829787234
                },
                "mAP_standard_with_null_verb": 0.8490492014911224,
                "mAP_present_only_with_null_verb": 0.11205412641836798,
                "mAP_freq_weighted_with_null_verb": 0.4840656879118924,
                "mAP_sample_wise_with_null_verb": 0.24499517580162755,
                "mAP_standard_all_actions": 0.8490492014911224,
                "mAP_present_only_all_actions": 0.11205412641836798,
                "mAP_freq_weighted_all_actions": 0.4840656879118924,
                "mAP_sample_wise_all_actions": 0.24499517580162755,
                "exact_match_with_null_verb": 0.14270152505446623,
                "hamming_accuracy_with_null_verb": 0.9865468409586057,
                "precision_with_null_verb": 0.49327342047930284,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4966139335947139,
                "num_predictions": 1836,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "17",
                "action_sparsity_with_null_verb": 0.83,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8696803402006181,
                "mAP_present_only": 0.12499656991843669,
                "mAP_freq_weighted": 0.5363641524488384,
                "exact_match": 0.1781045751633987,
                "hamming_accuracy": 0.9872989384879247,
                "precision": 0.49364946924396236,
                "recall": 0.5,
                "f1": 0.49680444112707595,
                "num_actions_total": 94,
                "num_actions_present": "14",
                "action_sparsity": 0.851063829787234
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.12246571205162982,
                "exact_match": 0.004357298474945534,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.11100176981184652,
                "exact_match": 0.0005446623093681918,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.12499656991843669,
                "exact_match": 0.1781045751633987,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID57": {
          "video_id": "VID57",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08944349354277437,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.6814482756769356,
                "mAP_present_only_with_null_verb": 0.08249336798821329,
                "mAP_freq_weighted_with_null_verb": 0.27110275936234074,
                "mAP_sample_wise_with_null_verb": 0.02950204023373117,
                "mAP_standard_all_actions": 0.6814482756769356,
                "mAP_present_only_all_actions": 0.08249336798821329,
                "mAP_freq_weighted_all_actions": 0.27110275936234074,
                "mAP_sample_wise_all_actions": 0.02950204023373117,
                "exact_match_with_null_verb": 0.0022796352583586625,
                "hamming_accuracy_with_null_verb": 0.9283662613981762,
                "precision_with_null_verb": 0.4982395397825575,
                "recall_with_null_verb": 0.49226139228575794,
                "f1_with_null_verb": 0.48934920483725414,
                "num_predictions": 2632,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7027361739519554,
                "mAP_present_only": 0.08944349354277437,
                "mAP_freq_weighted": 0.28632110070925954,
                "exact_match": 0.00303951367781155,
                "hamming_accuracy": 0.9287290629244002,
                "precision": 0.49860883722501737,
                "recall": 0.4938936599603276,
                "f1": 0.48998618548886896,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.083022185582904,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.48978549366920815,
                "mAP_present_only_with_null_verb": 0.07609805257387761,
                "mAP_freq_weighted_with_null_verb": 0.3893423488874867,
                "mAP_sample_wise_with_null_verb": 0.05135292110695471,
                "mAP_standard_all_actions": 0.48978549366920815,
                "mAP_present_only_all_actions": 0.07609805257387761,
                "mAP_freq_weighted_all_actions": 0.3893423488874867,
                "mAP_sample_wise_all_actions": 0.05135292110695471,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9089855623100304,
                "precision_with_null_verb": 0.4953732043737941,
                "recall_with_null_verb": 0.4737667534702432,
                "f1_with_null_verb": 0.4799640966587329,
                "num_predictions": 2632,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.4990373432809233,
                "mAP_present_only": 0.083022185582904,
                "mAP_freq_weighted": 0.4130244053775301,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9120036538834637,
                "precision": 0.49482882851265964,
                "recall": 0.4716930326377809,
                "f1": 0.4798594226635372,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.07963652768901334,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.7597269920733936,
                "mAP_present_only_with_null_verb": 0.07587304643612952,
                "mAP_freq_weighted_with_null_verb": 0.4382139612448227,
                "mAP_sample_wise_with_null_verb": 0.22663724637652982,
                "mAP_standard_all_actions": 0.7597269920733936,
                "mAP_present_only_all_actions": 0.07587304643612952,
                "mAP_freq_weighted_all_actions": 0.4382139612448227,
                "mAP_sample_wise_all_actions": 0.22663724637652982,
                "exact_match_with_null_verb": 0.1474164133738602,
                "hamming_accuracy_with_null_verb": 0.9870364741641338,
                "precision_with_null_verb": 0.4935182370820669,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49673797486749216,
                "num_predictions": 2632,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7748046823068861,
                "mAP_present_only": 0.07963652768901334,
                "mAP_freq_weighted": 0.462021451707106,
                "exact_match": 0.189209726443769,
                "hamming_accuracy": 0.9870699411498416,
                "precision": 0.4935349705749208,
                "recall": 0.5,
                "f1": 0.4967464509974228,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08944349354277437,
                "exact_match": 0.00303951367781155,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.083022185582904,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.07963652768901334,
                "exact_match": 0.189209726443769,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID60": {
          "video_id": "VID60",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.05973702523766521,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.6471151134654004,
                "mAP_present_only_with_null_verb": 0.055210043436775116,
                "mAP_freq_weighted_with_null_verb": 0.18664392160816412,
                "mAP_sample_wise_with_null_verb": 0.029758447244546855,
                "mAP_standard_all_actions": 0.6471151134654004,
                "mAP_present_only_all_actions": 0.055210043436775116,
                "mAP_freq_weighted_all_actions": 0.18664392160816412,
                "mAP_sample_wise_all_actions": 0.029758447244546855,
                "exact_match_with_null_verb": 0.004344391785150079,
                "hamming_accuracy_with_null_verb": 0.9258372827804108,
                "precision_with_null_verb": 0.4939861290369339,
                "recall_with_null_verb": 0.474901781936698,
                "f1_with_null_verb": 0.4828105581259427,
                "num_predictions": 2532,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.676732975334223,
                "mAP_present_only": 0.05973702523766521,
                "mAP_freq_weighted": 0.19772824664501135,
                "exact_match": 0.008688783570300158,
                "hamming_accuracy": 0.9256789687741589,
                "precision": 0.49413839435820095,
                "recall": 0.47516557110033986,
                "f1": 0.48289443854045466,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08178307229126805,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.4229441762419618,
                "mAP_present_only_with_null_verb": 0.07401347174826348,
                "mAP_freq_weighted_with_null_verb": 0.23526584955502775,
                "mAP_sample_wise_with_null_verb": 0.05975762658017992,
                "mAP_standard_all_actions": 0.4229441762419618,
                "mAP_present_only_all_actions": 0.07401347174826348,
                "mAP_freq_weighted_all_actions": 0.23526584955502775,
                "mAP_sample_wise_all_actions": 0.05975762658017992,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9262124802527646,
                "precision_with_null_verb": 0.5048572525375971,
                "recall_with_null_verb": 0.5209574843009099,
                "f1_with_null_verb": 0.4996195969252019,
                "num_predictions": 2532,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.43838449948791736,
                "mAP_present_only": 0.08178307229126805,
                "mAP_freq_weighted": 0.25044031887582147,
                "exact_match": 0.00039494470774091627,
                "hamming_accuracy": 0.9290486034082888,
                "precision": 0.5052492937412874,
                "recall": 0.5219445841466757,
                "f1": 0.5007729582605751,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.07970271076637508,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.7124598699191744,
                "mAP_present_only_with_null_verb": 0.07245119328765937,
                "mAP_freq_weighted_with_null_verb": 0.28040767008886885,
                "mAP_sample_wise_with_null_verb": 0.1076420017793762,
                "mAP_standard_all_actions": 0.7124598699191744,
                "mAP_present_only_all_actions": 0.07245119328765937,
                "mAP_freq_weighted_all_actions": 0.28040767008886885,
                "mAP_sample_wise_all_actions": 0.1076420017793762,
                "exact_match_with_null_verb": 0.15916271721958924,
                "hamming_accuracy_with_null_verb": 0.9861611374407583,
                "precision_with_null_verb": 0.49308056872037914,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4965161782953135,
                "num_predictions": 2532,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "31",
                "action_sparsity_with_null_verb": 0.69,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7356592892626822,
                "mAP_present_only": 0.07970271076637508,
                "mAP_freq_weighted": 0.29876246413884383,
                "exact_match": 0.21484992101105846,
                "hamming_accuracy": 0.9863029814123895,
                "precision": 0.49315149070619474,
                "recall": 0.5,
                "f1": 0.4965521326011727,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.05973702523766521,
                "exact_match": 0.008688783570300158,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08178307229126805,
                "exact_match": 0.00039494470774091627,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.07970271076637508,
                "exact_match": 0.21484992101105846,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID62": {
          "video_id": "VID62",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.12474350043411112,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.7030115821956963,
                "mAP_present_only_with_null_verb": 0.10957896283664927,
                "mAP_freq_weighted_with_null_verb": 0.3102249373190412,
                "mAP_sample_wise_with_null_verb": 0.019308015550962804,
                "mAP_standard_all_actions": 0.7030115821956963,
                "mAP_present_only_all_actions": 0.10957896283664927,
                "mAP_freq_weighted_all_actions": 0.3102249373190412,
                "mAP_sample_wise_all_actions": 0.019308015550962804,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9227411417322835,
                "precision_with_null_verb": 0.4915377315458058,
                "recall_with_null_verb": 0.4688181345388354,
                "f1_with_null_verb": 0.4799091888682138,
                "num_predictions": 2032,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.735325952206169,
                "mAP_present_only": 0.12474350043411112,
                "mAP_freq_weighted": 0.3224320148754659,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9223645082928463,
                "precision": 0.4914721372938472,
                "recall": 0.46868332703736615,
                "f1": 0.47980729165248226,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.1098022430884096,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.5094482592457347,
                "mAP_present_only_with_null_verb": 0.09261075831302265,
                "mAP_freq_weighted_with_null_verb": 0.3614495042584421,
                "mAP_sample_wise_with_null_verb": 0.037089469124767405,
                "mAP_standard_all_actions": 0.5094482592457347,
                "mAP_present_only_all_actions": 0.09261075831302265,
                "mAP_freq_weighted_all_actions": 0.3614495042584421,
                "mAP_sample_wise_all_actions": 0.037089469124767405,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8985088582677165,
                "precision_with_null_verb": 0.49346055196209565,
                "recall_with_null_verb": 0.4670216190861516,
                "f1_with_null_verb": 0.476584801804595,
                "num_predictions": 2032,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5304961503457762,
                "mAP_present_only": 0.1098022430884096,
                "mAP_freq_weighted": 0.379513160321169,
                "exact_match": 0.0004921259842519685,
                "hamming_accuracy": 0.9001926620874519,
                "precision": 0.4925519590292429,
                "recall": 0.46353124627811604,
                "f1": 0.4757174447171334,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.13520766971034398,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "17",
                  "subset_action_sparsity": 0.8191489361702128
                },
                "mAP_standard_with_null_verb": 0.8138743404304367,
                "mAP_present_only_with_null_verb": 0.11368733538303194,
                "mAP_freq_weighted_with_null_verb": 0.4658979704155164,
                "mAP_sample_wise_with_null_verb": 0.20770367305809298,
                "mAP_standard_all_actions": 0.8138743404304367,
                "mAP_present_only_all_actions": 0.11368733538303194,
                "mAP_freq_weighted_all_actions": 0.4658979704155164,
                "mAP_sample_wise_all_actions": 0.20770367305809298,
                "exact_match_with_null_verb": 0.03543307086614173,
                "hamming_accuracy_with_null_verb": 0.9841141732283465,
                "precision_with_null_verb": 0.49205708661417324,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4959967458057603,
                "num_predictions": 2032,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.843601387075275,
                "mAP_present_only": 0.13520766971034398,
                "mAP_freq_weighted": 0.4896896441889499,
                "exact_match": 0.056594488188976375,
                "hamming_accuracy": 0.9839954347461887,
                "precision": 0.49199771737309433,
                "recall": 0.5,
                "f1": 0.49596658213685385,
                "num_actions_total": 94,
                "num_actions_present": "17",
                "action_sparsity": 0.8191489361702128
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.12474350043411112,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.1098022430884096,
                "exact_match": 0.0004921259842519685,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.13520766971034398,
                "exact_match": 0.056594488188976375,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID65": {
          "video_id": "VID65",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09567791345056548,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.7560271547680594,
                "mAP_present_only_with_null_verb": 0.08903974871144163,
                "mAP_freq_weighted_with_null_verb": 0.372918407602023,
                "mAP_sample_wise_with_null_verb": 0.02391382350518452,
                "mAP_standard_all_actions": 0.7560271547680594,
                "mAP_present_only_all_actions": 0.08903974871144163,
                "mAP_freq_weighted_all_actions": 0.372918407602023,
                "mAP_sample_wise_all_actions": 0.02391382350518452,
                "exact_match_with_null_verb": 0.001601708489054992,
                "hamming_accuracy_with_null_verb": 0.9333368926855312,
                "precision_with_null_verb": 0.4926684977759906,
                "recall_with_null_verb": 0.4761369572520525,
                "f1_with_null_verb": 0.4837973375479338,
                "num_predictions": 1873,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7716047512256281,
                "mAP_present_only": 0.09567791345056548,
                "mAP_freq_weighted": 0.38795001229376347,
                "exact_match": 0.001601708489054992,
                "hamming_accuracy": 0.932557848939578,
                "precision": 0.4925573593366991,
                "recall": 0.47597377187319,
                "f1": 0.4836421689160991,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.12301005155374839,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.58023909549851,
                "mAP_present_only_with_null_verb": 0.11243941943616677,
                "mAP_freq_weighted_with_null_verb": 0.46659846416673084,
                "mAP_sample_wise_with_null_verb": 0.037780993216193985,
                "mAP_standard_all_actions": 0.58023909549851,
                "mAP_present_only_all_actions": 0.11243941943616677,
                "mAP_freq_weighted_all_actions": 0.46659846416673084,
                "mAP_sample_wise_all_actions": 0.037780993216193985,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8923972237052856,
                "precision_with_null_verb": 0.49200938596005944,
                "recall_with_null_verb": 0.45535004747504826,
                "f1_with_null_verb": 0.4722118338038741,
                "num_predictions": 1873,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5847676683495743,
                "mAP_present_only": 0.12301005155374839,
                "mAP_freq_weighted": 0.48618615388452985,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8945598709545501,
                "precision": 0.49181422155076704,
                "recall": 0.4561351085857506,
                "f1": 0.4727093511106196,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.12325162539527863,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "16",
                  "subset_action_sparsity": 0.8297872340425532
                },
                "mAP_standard_with_null_verb": 0.8403462597596915,
                "mAP_present_only_with_null_verb": 0.11303477644273029,
                "mAP_freq_weighted_with_null_verb": 0.5155551515941453,
                "mAP_sample_wise_with_null_verb": 0.2629071137796478,
                "mAP_standard_all_actions": 0.8403462597596915,
                "mAP_present_only_all_actions": 0.11303477644273029,
                "mAP_freq_weighted_all_actions": 0.5155551515941453,
                "mAP_sample_wise_all_actions": 0.2629071137796478,
                "exact_match_with_null_verb": 0.13721302722904433,
                "hamming_accuracy_with_null_verb": 0.9847463961558997,
                "precision_with_null_verb": 0.4923731980779498,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4961572914621123,
                "num_predictions": 1873,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "18",
                "action_sparsity_with_null_verb": 0.8200000000000001,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8507662341098347,
                "mAP_present_only": 0.12325162539527863,
                "mAP_freq_weighted": 0.5373586894029639,
                "exact_match": 0.1623064602242392,
                "hamming_accuracy": 0.9844713794004385,
                "precision": 0.49223568970021925,
                "recall": 0.5,
                "f1": 0.49608746672772547,
                "num_actions_total": 94,
                "num_actions_present": "16",
                "action_sparsity": 0.8297872340425532
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09567791345056548,
                "exact_match": 0.001601708489054992,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.12301005155374839,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.12325162539527863,
                "exact_match": 0.1623064602242392,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID68": {
          "video_id": "VID68",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08947662366463152,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.6717195320582392,
                "mAP_present_only_with_null_verb": 0.08044271132681143,
                "mAP_freq_weighted_with_null_verb": 0.25123094988143685,
                "mAP_sample_wise_with_null_verb": 0.03635262723618885,
                "mAP_standard_all_actions": 0.6717195320582392,
                "mAP_present_only_all_actions": 0.08044271132681143,
                "mAP_freq_weighted_all_actions": 0.25123094988143685,
                "mAP_sample_wise_all_actions": 0.03635262723618885,
                "exact_match_with_null_verb": 0.002028397565922921,
                "hamming_accuracy_with_null_verb": 0.9255831643002028,
                "precision_with_null_verb": 0.4982006026170204,
                "recall_with_null_verb": 0.49119745551824295,
                "f1_with_null_verb": 0.48811666664951786,
                "num_predictions": 1972,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "27",
                "action_sparsity_with_null_verb": 0.73,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7027442802583672,
                "mAP_present_only": 0.08947662366463152,
                "mAP_freq_weighted": 0.2643097151513443,
                "exact_match": 0.0030425963488843813,
                "hamming_accuracy": 0.9261792758189116,
                "precision": 0.4985560869518314,
                "recall": 0.4929771221429626,
                "f1": 0.48880776919995117,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.06608063594968798,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.5158833066440311,
                "mAP_present_only_with_null_verb": 0.0588270616445598,
                "mAP_freq_weighted_with_null_verb": 0.3427284779232001,
                "mAP_sample_wise_with_null_verb": 0.03915254050905983,
                "mAP_standard_all_actions": 0.5158833066440311,
                "mAP_present_only_all_actions": 0.0588270616445598,
                "mAP_freq_weighted_all_actions": 0.3427284779232001,
                "mAP_sample_wise_all_actions": 0.03915254050905983,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9141632860040568,
                "precision_with_null_verb": 0.49544404873811293,
                "recall_with_null_verb": 0.4741894543825975,
                "f1_with_null_verb": 0.48085845199784977,
                "num_predictions": 1972,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "27",
                "action_sparsity_with_null_verb": 0.73,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5374452619876896,
                "mAP_present_only": 0.06608063594968798,
                "mAP_freq_weighted": 0.36254115544358767,
                "exact_match": 0.0,
                "hamming_accuracy": 0.917332009839886,
                "precision": 0.4943969026271301,
                "recall": 0.4695732282315005,
                "f1": 0.48000028926591226,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08366821478393482,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.74998500656847,
                "mAP_present_only_with_null_verb": 0.07401854284618549,
                "mAP_freq_weighted_with_null_verb": 0.46836188794136446,
                "mAP_sample_wise_with_null_verb": 0.15508565892652787,
                "mAP_standard_all_actions": 0.74998500656847,
                "mAP_present_only_all_actions": 0.07401854284618549,
                "mAP_freq_weighted_all_actions": 0.46836188794136446,
                "mAP_sample_wise_all_actions": 0.15508565892652787,
                "exact_match_with_null_verb": 0.1308316430020284,
                "hamming_accuracy_with_null_verb": 0.9877332657200811,
                "precision_with_null_verb": 0.49386663286004057,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49691439125875997,
                "num_predictions": 1972,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "27",
                "action_sparsity_with_null_verb": 0.73,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7757911589364948,
                "mAP_present_only": 0.08366821478393482,
                "mAP_freq_weighted": 0.49621503029387426,
                "exact_match": 0.15162271805273833,
                "hamming_accuracy": 0.9877540891631781,
                "precision": 0.49387704458158904,
                "recall": 0.5,
                "f1": 0.4969196615155808,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08947662366463152,
                "exact_match": 0.0030425963488843813,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.06608063594968798,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08366821478393482,
                "exact_match": 0.15162271805273833,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID70": {
          "video_id": "VID70",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.07387184667833106,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.6585571324785914,
                "mAP_present_only_with_null_verb": 0.06627547313782642,
                "mAP_freq_weighted_with_null_verb": 0.31788015294400074,
                "mAP_sample_wise_with_null_verb": 0.033859233111807094,
                "mAP_standard_all_actions": 0.6585571324785914,
                "mAP_present_only_all_actions": 0.06627547313782642,
                "mAP_freq_weighted_all_actions": 0.31788015294400074,
                "mAP_sample_wise_all_actions": 0.033859233111807094,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9254355108877722,
                "precision_with_null_verb": 0.4941168723259832,
                "recall_with_null_verb": 0.4797289770131707,
                "f1_with_null_verb": 0.48475239047180474,
                "num_predictions": 1194,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6784353651093612,
                "mAP_present_only": 0.07387184667833106,
                "mAP_freq_weighted": 0.3315187247890502,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9250329662496881,
                "precision": 0.4939847785811085,
                "recall": 0.47955747663500525,
                "f1": 0.4846474785819601,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.09304019211468278,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.5136461916115286,
                "mAP_present_only_with_null_verb": 0.08445068432688799,
                "mAP_freq_weighted_with_null_verb": 0.4841801197701629,
                "mAP_sample_wise_with_null_verb": 0.04896273841240043,
                "mAP_standard_all_actions": 0.5136461916115286,
                "mAP_present_only_all_actions": 0.08445068432688799,
                "mAP_freq_weighted_all_actions": 0.4841801197701629,
                "mAP_sample_wise_all_actions": 0.04896273841240043,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9018844221105528,
                "precision_with_null_verb": 0.49746923937355064,
                "recall_with_null_verb": 0.4879195567305986,
                "f1_with_null_verb": 0.4839707010743396,
                "num_predictions": 1194,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5237549426675786,
                "mAP_present_only": 0.09304019211468278,
                "mAP_freq_weighted": 0.5046247032335591,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9038721978687765,
                "precision": 0.49628310526108804,
                "recall": 0.483038758586063,
                "f1": 0.4828488676514288,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.0802823244030422,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.7399696234223991,
                "mAP_present_only_with_null_verb": 0.07132008365142466,
                "mAP_freq_weighted_with_null_verb": 0.4240762500345754,
                "mAP_sample_wise_with_null_verb": 0.18744958754372792,
                "mAP_standard_all_actions": 0.7399696234223991,
                "mAP_present_only_all_actions": 0.07132008365142466,
                "mAP_freq_weighted_all_actions": 0.4240762500345754,
                "mAP_sample_wise_all_actions": 0.18744958754372792,
                "exact_match_with_null_verb": 0.04187604690117253,
                "hamming_accuracy_with_null_verb": 0.9836599664991624,
                "precision_with_null_verb": 0.4918299832495812,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4958813421209294,
                "num_predictions": 1194,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7651784658050321,
                "mAP_present_only": 0.0802823244030422,
                "mAP_freq_weighted": 0.4427499378086433,
                "exact_match": 0.059463986599664995,
                "hamming_accuracy": 0.9833921379949392,
                "precision": 0.4916960689974696,
                "recall": 0.5,
                "f1": 0.49581326816646304,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.07387184667833106,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.09304019211468278,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.0802823244030422,
                "exact_match": 0.059463986599664995,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID73": {
          "video_id": "VID73",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.08357804571613092,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.6436967980669234,
                "mAP_present_only_with_null_verb": 0.07405249395913553,
                "mAP_freq_weighted_with_null_verb": 0.31710457470402603,
                "mAP_sample_wise_with_null_verb": 0.02780214288121811,
                "mAP_standard_all_actions": 0.6436967980669234,
                "mAP_present_only_all_actions": 0.07405249395913553,
                "mAP_freq_weighted_all_actions": 0.31710457470402603,
                "mAP_sample_wise_all_actions": 0.02780214288121811,
                "exact_match_with_null_verb": 0.004424778761061947,
                "hamming_accuracy_with_null_verb": 0.933193215339233,
                "precision_with_null_verb": 0.49629630046247325,
                "recall_with_null_verb": 0.48783036469243135,
                "f1_with_null_verb": 0.4891424658603813,
                "num_predictions": 1356,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "32",
                "action_sparsity_with_null_verb": 0.6799999999999999,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.683580928024846,
                "mAP_present_only": 0.08357804571613092,
                "mAP_freq_weighted": 0.3393889535328874,
                "exact_match": 0.0058997050147492625,
                "hamming_accuracy": 0.9328280926379213,
                "precision": 0.4966309401337281,
                "recall": 0.4886473077512664,
                "f1": 0.48941232225217635,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08716584441231727,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.5045476988242605,
                "mAP_present_only_with_null_verb": 0.07671155882581379,
                "mAP_freq_weighted_with_null_verb": 0.43903932375704957,
                "mAP_sample_wise_with_null_verb": 0.05417995035915412,
                "mAP_standard_all_actions": 0.5045476988242605,
                "mAP_present_only_all_actions": 0.07671155882581379,
                "mAP_freq_weighted_all_actions": 0.43903932375704957,
                "mAP_sample_wise_all_actions": 0.05417995035915412,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8938716814159292,
                "precision_with_null_verb": 0.4934712977936978,
                "recall_with_null_verb": 0.46407537992007847,
                "f1_with_null_verb": 0.4749419647120412,
                "num_predictions": 1356,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "32",
                "action_sparsity_with_null_verb": 0.6799999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5356752957354529,
                "mAP_present_only": 0.08716584441231727,
                "mAP_freq_weighted": 0.4713519701137781,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8963315132115734,
                "precision": 0.4924799907333176,
                "recall": 0.4591329307830147,
                "f1": 0.47387033599966155,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08290509260344957,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "27",
                  "subset_action_sparsity": 0.7127659574468085
                },
                "mAP_standard_with_null_verb": 0.7044994063225407,
                "mAP_present_only_with_null_verb": 0.07656064475793975,
                "mAP_freq_weighted_with_null_verb": 0.4407885980952333,
                "mAP_sample_wise_with_null_verb": 0.268973509000196,
                "mAP_standard_all_actions": 0.7044994063225407,
                "mAP_present_only_all_actions": 0.07656064475793975,
                "mAP_freq_weighted_all_actions": 0.4407885980952333,
                "mAP_sample_wise_all_actions": 0.268973509000196,
                "exact_match_with_null_verb": 0.10914454277286136,
                "hamming_accuracy_with_null_verb": 0.9846681415929204,
                "precision_with_null_verb": 0.4923340707964602,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4961374251730634,
                "num_predictions": 1356,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "32",
                "action_sparsity_with_null_verb": 0.6799999999999999,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7365791223435441,
                "mAP_present_only": 0.08290509260344957,
                "mAP_freq_weighted": 0.4696441978396076,
                "exact_match": 0.14380530973451328,
                "hamming_accuracy": 0.984882005899705,
                "precision": 0.4924410029498525,
                "recall": 0.5,
                "f1": 0.4961917146572543,
                "num_actions_total": 94,
                "num_actions_present": "27",
                "action_sparsity": 0.7127659574468085
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.08357804571613092,
                "exact_match": 0.0058997050147492625,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08716584441231727,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08290509260344957,
                "exact_match": 0.14380530973451328,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID74": {
          "video_id": "VID74",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.0791428160837164,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.661312412330695,
                "mAP_present_only_with_null_verb": 0.07611575832391082,
                "mAP_freq_weighted_with_null_verb": 0.25153504434086044,
                "mAP_sample_wise_with_null_verb": 0.03805947449960164,
                "mAP_standard_all_actions": 0.661312412330695,
                "mAP_present_only_all_actions": 0.07611575832391082,
                "mAP_freq_weighted_all_actions": 0.25153504434086044,
                "mAP_sample_wise_all_actions": 0.03805947449960164,
                "exact_match_with_null_verb": 0.0036719706242350062,
                "hamming_accuracy_with_null_verb": 0.9373194614443084,
                "precision_with_null_verb": 0.4970032889730847,
                "recall_with_null_verb": 0.49111701644295486,
                "f1_with_null_verb": 0.49131633629627747,
                "num_predictions": 1634,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6904194424043533,
                "mAP_present_only": 0.0791428160837164,
                "mAP_freq_weighted": 0.26901299370446247,
                "exact_match": 0.004895960832313341,
                "hamming_accuracy": 0.937185864215214,
                "precision": 0.49762190147945295,
                "recall": 0.49246214554404116,
                "f1": 0.4917347034850095,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08825431350497166,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.5229785755905979,
                "mAP_present_only_with_null_verb": 0.08206634139499248,
                "mAP_freq_weighted_with_null_verb": 0.31942721219203185,
                "mAP_sample_wise_with_null_verb": 0.05960914640590392,
                "mAP_standard_all_actions": 0.5229785755905979,
                "mAP_present_only_all_actions": 0.08206634139499248,
                "mAP_freq_weighted_all_actions": 0.31942721219203185,
                "mAP_sample_wise_all_actions": 0.05960914640590392,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9118910648714811,
                "precision_with_null_verb": 0.49695533878884707,
                "recall_with_null_verb": 0.4864719463666085,
                "f1_with_null_verb": 0.48520537619000065,
                "num_predictions": 1634,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5438096119587161,
                "mAP_present_only": 0.08825431350497166,
                "mAP_freq_weighted": 0.34515440507356904,
                "exact_match": 0.0006119951040391676,
                "hamming_accuracy": 0.9146657465038152,
                "precision": 0.4928742757375546,
                "recall": 0.46812385494773323,
                "f1": 0.47908244852141235,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08621691551758058,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "24",
                  "subset_action_sparsity": 0.7446808510638299
                },
                "mAP_standard_with_null_verb": 0.7439663268824975,
                "mAP_present_only_with_null_verb": 0.0855940245803485,
                "mAP_freq_weighted_with_null_verb": 0.37509634559024957,
                "mAP_sample_wise_with_null_verb": 0.2073041821752879,
                "mAP_standard_all_actions": 0.7439663268824975,
                "mAP_present_only_all_actions": 0.0855940245803485,
                "mAP_freq_weighted_all_actions": 0.37509634559024957,
                "mAP_sample_wise_all_actions": 0.2073041821752879,
                "exact_match_with_null_verb": 0.037943696450428395,
                "hamming_accuracy_with_null_verb": 0.9843451652386781,
                "precision_with_null_verb": 0.49217258261933905,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49605541539960896,
                "num_predictions": 1634,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7666936805576802,
                "mAP_present_only": 0.08621691551758058,
                "mAP_freq_weighted": 0.4032527500502205,
                "exact_match": 0.0740514075887393,
                "hamming_accuracy": 0.9850972681580249,
                "precision": 0.49254863407901245,
                "recall": 0.5,
                "f1": 0.4962463471989452,
                "num_actions_total": 94,
                "num_actions_present": "24",
                "action_sparsity": 0.7446808510638299
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.0791428160837164,
                "exact_match": 0.004895960832313341,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08825431350497166,
                "exact_match": 0.0006119951040391676,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08621691551758058,
                "exact_match": 0.0740514075887393,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID75": {
          "video_id": "VID75",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09077082902270582,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "18",
                  "subset_action_sparsity": 0.8085106382978724
                },
                "mAP_standard_with_null_verb": 0.7269304806852638,
                "mAP_present_only_with_null_verb": 0.08062133659649391,
                "mAP_freq_weighted_with_null_verb": 0.2937398076469832,
                "mAP_sample_wise_with_null_verb": 0.018294050030664043,
                "mAP_standard_all_actions": 0.7269304806852638,
                "mAP_present_only_all_actions": 0.08062133659649391,
                "mAP_freq_weighted_all_actions": 0.2937398076469832,
                "mAP_sample_wise_all_actions": 0.018294050030664043,
                "exact_match_with_null_verb": 0.0005200208008320333,
                "hamming_accuracy_with_null_verb": 0.9281903276131045,
                "precision_with_null_verb": 0.49382649615047575,
                "recall_with_null_verb": 0.4713077349582477,
                "f1_with_null_verb": 0.4818846971430287,
                "num_predictions": 1923,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7514242013022201,
                "mAP_present_only": 0.09077082902270582,
                "mAP_freq_weighted": 0.3070413859170743,
                "exact_match": 0.0005200208008320333,
                "hamming_accuracy": 0.9286741682433255,
                "precision": 0.4937727465246714,
                "recall": 0.4717004307575043,
                "f1": 0.48205070154265206,
                "num_actions_total": 94,
                "num_actions_present": "18",
                "action_sparsity": 0.8085106382978724
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.07964577909862769,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "18",
                  "subset_action_sparsity": 0.8085106382978724
                },
                "mAP_standard_with_null_verb": 0.5548434816488966,
                "mAP_present_only_with_null_verb": 0.07068324594712669,
                "mAP_freq_weighted_with_null_verb": 0.35511434071897,
                "mAP_sample_wise_with_null_verb": 0.026532452882160343,
                "mAP_standard_all_actions": 0.5548434816488966,
                "mAP_present_only_all_actions": 0.07068324594712669,
                "mAP_freq_weighted_all_actions": 0.35511434071897,
                "mAP_sample_wise_all_actions": 0.026532452882160343,
                "exact_match_with_null_verb": 0.0005200208008320333,
                "hamming_accuracy_with_null_verb": 0.9124596983879355,
                "precision_with_null_verb": 0.49402407342708515,
                "recall_with_null_verb": 0.4654464500736476,
                "f1_with_null_verb": 0.47811887373175166,
                "num_predictions": 1923,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.568442808763567,
                "mAP_present_only": 0.07964577909862769,
                "mAP_freq_weighted": 0.371677915699507,
                "exact_match": 0.0005200208008320333,
                "hamming_accuracy": 0.9162434582489682,
                "precision": 0.4939600785707117,
                "recall": 0.4671728977652205,
                "f1": 0.479132542552923,
                "num_actions_total": 94,
                "num_actions_present": "18",
                "action_sparsity": 0.8085106382978724
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08870840009310783,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "18",
                  "subset_action_sparsity": 0.8085106382978724
                },
                "mAP_standard_with_null_verb": 0.8065910356347906,
                "mAP_present_only_with_null_verb": 0.07900493159424145,
                "mAP_freq_weighted_with_null_verb": 0.4243897101410244,
                "mAP_sample_wise_with_null_verb": 0.18711172705100462,
                "mAP_standard_all_actions": 0.8065910356347906,
                "mAP_present_only_all_actions": 0.07900493159424145,
                "mAP_freq_weighted_all_actions": 0.4243897101410244,
                "mAP_sample_wise_all_actions": 0.18711172705100462,
                "exact_match_with_null_verb": 0.0998439937597504,
                "hamming_accuracy_with_null_verb": 0.9877795111804473,
                "precision_with_null_verb": 0.49388975559022363,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4969260954872466,
                "num_predictions": 1923,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8254973532093185,
                "mAP_present_only": 0.08870840009310783,
                "mAP_freq_weighted": 0.44434446176550996,
                "exact_match": 0.12272490899635985,
                "hamming_accuracy": 0.9876246113674334,
                "precision": 0.4938123056837167,
                "recall": 0.5,
                "f1": 0.49688688986798857,
                "num_actions_total": 94,
                "num_actions_present": "18",
                "action_sparsity": 0.8085106382978724
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09077082902270582,
                "exact_match": 0.0005200208008320333,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.07964577909862769,
                "exact_match": 0.0005200208008320333,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08870840009310783,
                "exact_match": 0.12272490899635985,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID78": {
          "video_id": "VID78",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.1501661743499509,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.7641560818881403,
                "mAP_present_only_with_null_verb": 0.14231700786725085,
                "mAP_freq_weighted_with_null_verb": 0.28517837029108906,
                "mAP_sample_wise_with_null_verb": 0.07324110191212954,
                "mAP_standard_all_actions": 0.7641560818881403,
                "mAP_present_only_all_actions": 0.14231700786725085,
                "mAP_freq_weighted_all_actions": 0.28517837029108906,
                "mAP_sample_wise_all_actions": 0.07324110191212954,
                "exact_match_with_null_verb": 0.0013531799729364006,
                "hamming_accuracy_with_null_verb": 0.9240189445196211,
                "precision_with_null_verb": 0.5136164477261548,
                "recall_with_null_verb": 0.5535369079642419,
                "f1_with_null_verb": 0.5134491005034345,
                "num_predictions": 739,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "24",
                "action_sparsity_with_null_verb": 0.76,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7798261259116906,
                "mAP_present_only": 0.1501661743499509,
                "mAP_freq_weighted": 0.3013937407280249,
                "exact_match": 0.0013531799729364006,
                "hamming_accuracy": 0.9250280712866726,
                "precision": 0.5152793595484446,
                "recall": 0.5600904726556151,
                "f1": 0.5161319708927414,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.14109746958798208,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.5520400617204761,
                "mAP_present_only_with_null_verb": 0.13350025716865035,
                "mAP_freq_weighted_with_null_verb": 0.4222778567801215,
                "mAP_sample_wise_with_null_verb": 0.04800291727435347,
                "mAP_standard_all_actions": 0.5520400617204761,
                "mAP_present_only_all_actions": 0.13350025716865035,
                "mAP_freq_weighted_all_actions": 0.4222778567801215,
                "mAP_sample_wise_all_actions": 0.04800291727435347,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.8948173207036536,
                "precision_with_null_verb": 0.49393677409074976,
                "recall_with_null_verb": 0.4680612991970066,
                "f1_with_null_verb": 0.47631514211473397,
                "num_predictions": 739,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "24",
                "action_sparsity_with_null_verb": 0.76,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5542994077759108,
                "mAP_present_only": 0.14109746958798208,
                "mAP_freq_weighted": 0.4488374709196625,
                "exact_match": 0.0,
                "hamming_accuracy": 0.8969567846140558,
                "precision": 0.49327456262501534,
                "recall": 0.46510500237106994,
                "f1": 0.4757478694116337,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08861161829457874,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.7808390205699431,
                "mAP_present_only_with_null_verb": 0.08682925237476342,
                "mAP_freq_weighted_with_null_verb": 0.3467924613262031,
                "mAP_sample_wise_with_null_verb": 0.23198735633912893,
                "mAP_standard_all_actions": 0.7808390205699431,
                "mAP_present_only_all_actions": 0.08682925237476342,
                "mAP_freq_weighted_all_actions": 0.3467924613262031,
                "mAP_sample_wise_all_actions": 0.23198735633912893,
                "exact_match_with_null_verb": 0.12449255751014884,
                "hamming_accuracy_with_null_verb": 0.9841677943166441,
                "precision_with_null_verb": 0.49208389715832207,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.4960103662279206,
                "num_predictions": 739,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "24",
                "action_sparsity_with_null_verb": 0.76,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7866963361966035,
                "mAP_present_only": 0.08861161829457874,
                "mAP_freq_weighted": 0.36677524951037443,
                "exact_match": 0.16508795669824086,
                "hamming_accuracy": 0.9842944749949616,
                "precision": 0.4921472374974808,
                "recall": 0.5,
                "f1": 0.4960425417691398,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.1501661743499509,
                "exact_match": 0.0013531799729364006,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.14109746958798208,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08861161829457874,
                "exact_match": 0.16508795669824086,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID80": {
          "video_id": "VID80",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.09091393400787622,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.7378246915450231,
                "mAP_present_only_with_null_verb": 0.08487948354772934,
                "mAP_freq_weighted_with_null_verb": 0.21332926366640637,
                "mAP_sample_wise_with_null_verb": 0.0383351552948212,
                "mAP_standard_all_actions": 0.7378246915450231,
                "mAP_present_only_all_actions": 0.08487948354772934,
                "mAP_freq_weighted_all_actions": 0.21332926366640637,
                "mAP_sample_wise_all_actions": 0.0383351552948212,
                "exact_match_with_null_verb": 0.000580046403712297,
                "hamming_accuracy_with_null_verb": 0.9216647331786543,
                "precision_with_null_verb": 0.4944174386154335,
                "recall_with_null_verb": 0.4751517053834996,
                "f1_with_null_verb": 0.48241121309648455,
                "num_predictions": 1724,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7524187738952091,
                "mAP_present_only": 0.09091393400787622,
                "mAP_freq_weighted": 0.22036126672704626,
                "exact_match": 0.002320185614849188,
                "hamming_accuracy": 0.9217122476181073,
                "precision": 0.494385915247536,
                "recall": 0.4756398071699511,
                "f1": 0.482603114011261,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.08699382686178204,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.506965770225122,
                "mAP_present_only_with_null_verb": 0.08078938202439066,
                "mAP_freq_weighted_with_null_verb": 0.2957365486735676,
                "mAP_sample_wise_with_null_verb": 0.045134167446128276,
                "mAP_standard_all_actions": 0.506965770225122,
                "mAP_present_only_all_actions": 0.08078938202439066,
                "mAP_freq_weighted_all_actions": 0.2957365486735676,
                "mAP_sample_wise_all_actions": 0.045134167446128276,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9041357308584687,
                "precision_with_null_verb": 0.497637020295906,
                "recall_with_null_verb": 0.486727820476563,
                "f1_with_null_verb": 0.4829606021602004,
                "num_predictions": 1724,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.506945560748658,
                "mAP_present_only": 0.08699382686178204,
                "mAP_freq_weighted": 0.30660573084236625,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9048970726168731,
                "precision": 0.49723348750460494,
                "recall": 0.48497422567841714,
                "f1": 0.48274732229447503,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.11237634913928735,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "19",
                  "subset_action_sparsity": 0.7978723404255319
                },
                "mAP_standard_with_null_verb": 0.8119627635831368,
                "mAP_present_only_with_null_verb": 0.10458458849112764,
                "mAP_freq_weighted_with_null_verb": 0.38784099608941897,
                "mAP_sample_wise_with_null_verb": 0.12489280208097467,
                "mAP_standard_all_actions": 0.8119627635831368,
                "mAP_present_only_all_actions": 0.10458458849112764,
                "mAP_freq_weighted_all_actions": 0.38784099608941897,
                "mAP_sample_wise_all_actions": 0.12489280208097467,
                "exact_match_with_null_verb": 0.05162412993039443,
                "hamming_accuracy_with_null_verb": 0.9861658932714618,
                "precision_with_null_verb": 0.4930829466357309,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49651738387629046,
                "num_predictions": 1724,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "21",
                "action_sparsity_with_null_verb": 0.79,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.8205867088685793,
                "mAP_present_only": 0.11237634913928735,
                "mAP_freq_weighted": 0.4018916995910037,
                "exact_match": 0.0759860788863109,
                "hamming_accuracy": 0.9858690822925409,
                "precision": 0.49293454114627044,
                "recall": 0.5,
                "f1": 0.4964421326074662,
                "num_actions_total": 94,
                "num_actions_present": "19",
                "action_sparsity": 0.7978723404255319
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.09091393400787622,
                "exact_match": 0.002320185614849188,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.08699382686178204,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.11237634913928735,
                "exact_match": 0.0759860788863109,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID92": {
          "video_id": "VID92",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.0863004729135254,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.6632866145336811,
                "mAP_present_only_with_null_verb": 0.08316648047743234,
                "mAP_freq_weighted_with_null_verb": 0.1967595118624555,
                "mAP_sample_wise_with_null_verb": 0.13285371749995095,
                "mAP_standard_all_actions": 0.6632866145336811,
                "mAP_present_only_all_actions": 0.08316648047743234,
                "mAP_freq_weighted_all_actions": 0.1967595118624555,
                "mAP_sample_wise_all_actions": 0.13285371749995095,
                "exact_match_with_null_verb": 0.0004710315591144607,
                "hamming_accuracy_with_null_verb": 0.9229203956665096,
                "precision_with_null_verb": 0.5221045734681717,
                "recall_with_null_verb": 0.617201004071311,
                "f1_with_null_verb": 0.5255733691923069,
                "num_predictions": 2123,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.6913288391171393,
                "mAP_present_only": 0.0863004729135254,
                "mAP_freq_weighted": 0.2137956360008242,
                "exact_match": 0.0009420631182289214,
                "hamming_accuracy": 0.9261232098295267,
                "precision": 0.5252658574460959,
                "recall": 0.650417428228782,
                "f1": 0.5309565462556629,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.044643581350697376,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.49339545292621767,
                "mAP_present_only_with_null_verb": 0.04784090330792001,
                "mAP_freq_weighted_with_null_verb": 0.1407264724150391,
                "mAP_sample_wise_with_null_verb": 0.0804251177421571,
                "mAP_standard_all_actions": 0.49339545292621767,
                "mAP_present_only_all_actions": 0.04784090330792001,
                "mAP_freq_weighted_all_actions": 0.1407264724150391,
                "mAP_sample_wise_all_actions": 0.0804251177421571,
                "exact_match_with_null_verb": 0.0004710315591144607,
                "hamming_accuracy_with_null_verb": 0.9299764484220443,
                "precision_with_null_verb": 0.5006900854881967,
                "recall_with_null_verb": 0.5030335255857776,
                "f1_with_null_verb": 0.4934086155753791,
                "num_predictions": 2123,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.5109234294794259,
                "mAP_present_only": 0.044643581350697376,
                "mAP_freq_weighted": 0.14278585549731082,
                "exact_match": 0.0009420631182289214,
                "hamming_accuracy": 0.9339804171134786,
                "precision": 0.4991047960294256,
                "recall": 0.49573279542893595,
                "f1": 0.4908404315116045,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.08484016588930794,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "23",
                  "subset_action_sparsity": 0.7553191489361702
                },
                "mAP_standard_with_null_verb": 0.7432858132386124,
                "mAP_present_only_with_null_verb": 0.08316361870932949,
                "mAP_freq_weighted_with_null_verb": 0.287052588267797,
                "mAP_sample_wise_with_null_verb": 0.07871278222680288,
                "mAP_standard_all_actions": 0.7432858132386124,
                "mAP_present_only_all_actions": 0.08316361870932949,
                "mAP_freq_weighted_all_actions": 0.287052588267797,
                "mAP_sample_wise_all_actions": 0.07871278222680288,
                "exact_match_with_null_verb": 0.2086669806877061,
                "hamming_accuracy_with_null_verb": 0.9872256241168158,
                "precision_with_null_verb": 0.4936128120584079,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49678587682038833,
                "num_predictions": 2123,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "28",
                "action_sparsity_with_null_verb": 0.72,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7760779129303627,
                "mAP_present_only": 0.08484016588930794,
                "mAP_freq_weighted": 0.325560432209583,
                "exact_match": 0.30004710315591143,
                "hamming_accuracy": 0.9888054840099818,
                "precision": 0.4944027420049909,
                "recall": 0.5,
                "f1": 0.4971856181813601,
                "num_actions_total": 94,
                "num_actions_present": "23",
                "action_sparsity": 0.7553191489361702
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.0863004729135254,
                "exact_match": 0.0009420631182289214,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.044643581350697376,
                "exact_match": 0.0009420631182289214,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.08484016588930794,
                "exact_match": 0.30004710315591143,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        },
        "VID96": {
          "video_id": "VID96",
          "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
          "single_step_evaluation": {
            "WorldModelRL_world_model_ppo": {
              "metrics": {
                "mAP": 0.044591831901574494,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.691888664580629,
                "mAP_present_only_with_null_verb": 0.04572563300241967,
                "mAP_freq_weighted_with_null_verb": 0.15247617077198564,
                "mAP_sample_wise_with_null_verb": 0.19603766562505018,
                "mAP_standard_all_actions": 0.691888664580629,
                "mAP_present_only_all_actions": 0.04572563300241967,
                "mAP_freq_weighted_all_actions": 0.15247617077198564,
                "mAP_sample_wise_all_actions": 0.19603766562505018,
                "exact_match_with_null_verb": 0.004689331770222743,
                "hamming_accuracy_with_null_verb": 0.9238980070339976,
                "precision_with_null_verb": 0.5254118721377935,
                "recall_with_null_verb": 0.6586807783468988,
                "f1_with_null_verb": 0.5306770707947657,
                "num_predictions": 1706,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "WorldModelRL_world_model_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.7232023436365387,
                "mAP_present_only": 0.044591831901574494,
                "mAP_freq_weighted": 0.15699961532696188,
                "exact_match": 0.005275498241500586,
                "hamming_accuracy": 0.9259808934673618,
                "precision": 0.5281601984508908,
                "recall": 0.687505855849227,
                "f1": 0.5354564735693321,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_ppo": {
              "metrics": {
                "mAP": 0.04453454547824026,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.5411787514586455,
                "mAP_present_only_with_null_verb": 0.04299519791786734,
                "mAP_freq_weighted_with_null_verb": 0.1456092652723115,
                "mAP_sample_wise_with_null_verb": 0.06056919601026107,
                "mAP_standard_all_actions": 0.5411787514586455,
                "mAP_present_only_all_actions": 0.04299519791786734,
                "mAP_freq_weighted_all_actions": 0.1456092652723115,
                "mAP_sample_wise_all_actions": 0.06056919601026107,
                "exact_match_with_null_verb": 0.0,
                "hamming_accuracy_with_null_verb": 0.9273388042203986,
                "precision_with_null_verb": 0.4971205302008483,
                "recall_with_null_verb": 0.48459280986719777,
                "f1_with_null_verb": 0.48593684617162913,
                "num_predictions": 1706,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_ppo",
                "exclude_last_n": 6,
                "mAP_standard": 0.563614468090652,
                "mAP_present_only": 0.04453454547824026,
                "mAP_freq_weighted": 0.15547196618576414,
                "exact_match": 0.0,
                "hamming_accuracy": 0.9288431318749845,
                "precision": 0.49771780408273925,
                "recall": 0.4869901578810533,
                "f1": 0.4866650120734193,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            },
            "DirectVideoRL_a2c": {
              "metrics": {
                "mAP": 0.05353038592772207,
                "mAP_info": {
                  "calculated_from": "subset_excluding_last_classes_present_actions_only",
                  "excluded_classes": 6,
                  "subset_size": 94,
                  "present_actions_in_subset": "22",
                  "subset_action_sparsity": 0.7659574468085106
                },
                "mAP_standard_with_null_verb": 0.7532443383979585,
                "mAP_present_only_with_null_verb": 0.05093976306907112,
                "mAP_freq_weighted_with_null_verb": 0.2385616062635419,
                "mAP_sample_wise_with_null_verb": 0.07841081871635043,
                "mAP_standard_all_actions": 0.7532443383979585,
                "mAP_present_only_all_actions": 0.05093976306907112,
                "mAP_freq_weighted_all_actions": 0.2385616062635419,
                "mAP_sample_wise_all_actions": 0.07841081871635043,
                "exact_match_with_null_verb": 0.29308323563892147,
                "hamming_accuracy_with_null_verb": 0.9889566236811255,
                "precision_with_null_verb": 0.49447831184056273,
                "recall_with_null_verb": 0.5,
                "f1_with_null_verb": 0.49722382675735893,
                "num_predictions": 1706,
                "num_actions_total_with_null_verb": 100,
                "num_actions_present_with_null_verb": "26",
                "action_sparsity_with_null_verb": 0.74,
                "task": "single_step_action_prediction",
                "method_name": "DirectVideoRL_a2c",
                "exclude_last_n": 6,
                "mAP_standard": 0.7784858350043605,
                "mAP_present_only": 0.05353038592772207,
                "mAP_freq_weighted": 0.2630094588913233,
                "exact_match": 0.32942555685814773,
                "hamming_accuracy": 0.989748322566162,
                "precision": 0.494874161283081,
                "recall": 0.5,
                "f1": 0.4974238758446052,
                "num_actions_total": 94,
                "num_actions_present": "22",
                "action_sparsity": 0.7659574468085106
              },
              "evaluation_type": "next_action_prediction",
              "used_temporal_context": false
            }
          },
          "planning_evaluation": {
            "WorldModelRL_world_model_ppo": {},
            "DirectVideoRL_ppo": {},
            "DirectVideoRL_a2c": {}
          },
          "summary": {
            "primary_comparison": {
              "WorldModelRL_world_model_ppo": {
                "mAP": 0.044591831901574494,
                "exact_match": 0.005275498241500586,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_ppo": {
                "mAP": 0.04453454547824026,
                "exact_match": 0.0,
                "task": "single_step_action_prediction"
              },
              "DirectVideoRL_a2c": {
                "mAP": 0.05353038592772207,
                "exact_match": 0.32942555685814773,
                "task": "single_step_action_prediction"
              }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
              "fair_comparison_task": "single_step_action_prediction",
              "planning_analysis": "shows_method_specific_strengths",
              "evaluation_approach": "respects_training_paradigms"
            }
          },
          "fairness_report": {
            "evaluation_design": {
              "primary_evaluation": "single_step_fair_comparison",
              "secondary_evaluation": "method_specific_planning_analysis",
              "data_handling": "uses_dataloader_batches_like_training",
              "temporal_structure": "maintained_for_il_model",
              "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
              "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
              "WorldModelRL": "evaluated_with_consistent_batch_approach",
              "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
              "temporal_context": "preserved_for_models_that_need_it",
              "batch_structure": "mirrors_training_approach",
              "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
              "single_step_comparison": "valid_and_fair_with_proper_data_handling",
              "planning_comparison": "method_specific_respecting_capabilities",
              "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
          }
        }
      },
      "aggregate_results": {
        "single_step_comparison": {
          "DirectVideoRL_a2c": {
            "mean_mAP": 0.09406492992146645,
            "std_mAP": 0.02002124296261718,
            "mean_exact_match": 0.14514356481306076,
            "std_exact_match": 0.08013375333777752,
            "num_videos": 40,
            "evaluation_type": "single_step_fair_comparison"
          },
          "DirectVideoRL_ppo": {
            "mean_mAP": 0.09114222946264057,
            "std_mAP": 0.021792826368608364,
            "mean_exact_match": 0.00046211140382275694,
            "std_exact_match": 0.0007172666327043338,
            "num_videos": 40,
            "evaluation_type": "single_step_fair_comparison"
          },
          "WorldModelRL_world_model_ppo": {
            "mean_mAP": 0.09710873277497553,
            "std_mAP": 0.03239689279972203,
            "mean_exact_match": 0.004024380126520001,
            "std_exact_match": 0.0056499209620061224,
            "num_videos": 40,
            "evaluation_type": "single_step_fair_comparison"
          }
        },
        "planning_analysis": {},
        "method_rankings": {
          "single_step_ranking": [
            [
              "WorldModelRL_world_model_ppo",
              0.09710873277497553
            ],
            [
              "DirectVideoRL_a2c",
              0.09406492992146645
            ],
            [
              "DirectVideoRL_ppo",
              0.09114222946264057
            ]
          ],
          "planning_ranking": []
        }
      },
      "statistical_tests": {
        "DirectVideoRL_a2c_vs_DirectVideoRL_ppo": {
          "t_statistic": 0.6167650159596286,
          "p_value": 0.539185836096086,
          "cohens_d": 0.13966977260698252,
          "significant": "False",
          "mean_diff": 0.0029227004588258804,
          "method1_mean": 0.09406492992146645,
          "method2_mean": 0.09114222946264057
        },
        "DirectVideoRL_a2c_vs_WorldModelRL_world_model_ppo": {
          "t_statistic": -0.49911842870552314,
          "p_value": 0.6191007958087416,
          "cohens_d": -0.11302806682832013,
          "significant": "False",
          "mean_diff": -0.003043802853509081,
          "method1_mean": 0.09406492992146645,
          "method2_mean": 0.09710873277497553
        },
        "DirectVideoRL_ppo_vs_WorldModelRL_world_model_ppo": {
          "t_statistic": -0.9543122030759456,
          "p_value": 0.3428738536308301,
          "cohens_d": -0.21610915818936535,
          "significant": "False",
          "mean_diff": -0.0059665033123349615,
          "method1_mean": 0.09114222946264057,
          "method2_mean": 0.09710873277497553
        }
      },
      "evaluation_design": {
        "data_handling": "uses_dataloader_batches_like_training",
        "temporal_structure": "maintained_properly",
        "model_interfaces": "consistent_with_training",
        "primary_evaluation": "single_step_action_prediction_with_proper_context",
        "secondary_evaluation": "multi_step_planning_analysis",
        "fairness_approach": "respects_training_paradigms_and_data_structure"
      },
      "timestamp": "2025-06-15 17:16:56.396176"
    },
    "file_paths": {
      "evaluation": "results/fixed_rl_2025-06-15_07-38-18/integrated_evaluation/evaluation_results.json",
      "fair_comparison": "results/fixed_rl_2025-06-15_07-38-18/integrated_evaluation/fair_single_step_comparison.csv",
      "planning_analysis": "results/fixed_rl_2025-06-15_07-38-18/integrated_evaluation/planning_capability_analysis.csv"
    }
  }
}