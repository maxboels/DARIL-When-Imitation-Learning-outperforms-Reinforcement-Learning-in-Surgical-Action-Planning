{
    "VID02": {
        "video_id": "VID02",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.24074698670013087,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 28,
                        "subset_action_sparsity": 0.7021276595744681
                    },
                    "mAP_standard_with_null_verb": 0.6905325432104387,
                    "mAP_present_only_with_null_verb": 0.20744865650129066,
                    "mAP_freq_weighted_with_null_verb": 0.5749250666979124,
                    "mAP_sample_wise_with_null_verb": 0.6020647112047279,
                    "mAP_standard_all_actions": 0.6905325432104387,
                    "mAP_present_only_all_actions": 0.20744865650129066,
                    "mAP_freq_weighted_all_actions": 0.5749250666979124,
                    "mAP_sample_wise_all_actions": 0.6020647112047279,
                    "exact_match_with_null_verb": 0.32863684395914056,
                    "hamming_accuracy_with_null_verb": 0.9879816836914407,
                    "precision_with_null_verb": 0.798679140753191,
                    "recall_with_null_verb": 0.7313923602007786,
                    "f1_with_null_verb": 0.7606031737081023,
                    "num_predictions": 2839,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 34,
                    "action_sparsity_with_null_verb": 0.6599999999999999,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.7312863364638689,
                    "mAP_present_only": 0.24074698670013087,
                    "mAP_freq_weighted": 0.6041715552013276,
                    "exact_match": 0.36069038393800634,
                    "hamming_accuracy": 0.9884061663906231,
                    "precision": 0.8072857616709525,
                    "recall": 0.7439072938565797,
                    "f1": 0.7718162336367865,
                    "num_actions_total": 94,
                    "num_actions_present": 28,
                    "action_sparsity": 0.7021276595744681
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.24074698670013087,
                    "exact_match": 0.36069038393800634,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    },
    "VID06": {
        "video_id": "VID06",
        "evaluation_type": "comprehensive_fair_evaluation_with_proper_batches",
        "single_step_evaluation": {
            "AutoregressiveIL": {
                "metrics": {
                    "mAP": 0.3147151835841034,
                    "mAP_info": {
                        "calculated_from": "subset_excluding_last_classes_present_actions_only",
                        "excluded_classes": 6,
                        "subset_size": 94,
                        "present_actions_in_subset": 26,
                        "subset_action_sparsity": 0.7234042553191489
                    },
                    "mAP_standard_with_null_verb": 0.78660706710918,
                    "mAP_present_only_with_null_verb": 0.28869022369726705,
                    "mAP_freq_weighted_with_null_verb": 0.6512447569141718,
                    "mAP_sample_wise_with_null_verb": 0.7281993520413419,
                    "mAP_standard_all_actions": 0.78660706710918,
                    "mAP_present_only_all_actions": 0.28869022369726705,
                    "mAP_freq_weighted_all_actions": 0.6512447569141718,
                    "mAP_sample_wise_all_actions": 0.7281993520413419,
                    "exact_match_with_null_verb": 0.2712494194147701,
                    "hamming_accuracy_with_null_verb": 0.9878959591267998,
                    "precision_with_null_verb": 0.8398411547459523,
                    "recall_with_null_verb": 0.7769089949542399,
                    "f1_with_null_verb": 0.8050256563478667,
                    "num_predictions": 2153,
                    "num_actions_total_with_null_verb": 100,
                    "num_actions_present_with_null_verb": 30,
                    "action_sparsity_with_null_verb": 0.7,
                    "task": "single_step_action_prediction",
                    "method_name": "AutoregressiveIL",
                    "exclude_last_n": 6,
                    "mAP_standard": 0.8104531358849647,
                    "mAP_present_only": 0.3147151835841034,
                    "mAP_freq_weighted": 0.6850127366003094,
                    "exact_match": 0.32187645146307475,
                    "hamming_accuracy": 0.9884920595705151,
                    "precision": 0.8407423507731315,
                    "recall": 0.7983157980579412,
                    "f1": 0.8180598819621434,
                    "num_actions_total": 94,
                    "num_actions_present": 26,
                    "action_sparsity": 0.7234042553191489
                },
                "evaluation_type": "next_action_prediction",
                "used_temporal_context": true
            }
        },
        "planning_evaluation": {
            "AutoregressiveIL": {}
        },
        "summary": {
            "primary_comparison": {
                "AutoregressiveIL": {
                    "mAP": 0.3147151835841034,
                    "exact_match": 0.32187645146307475,
                    "task": "single_step_action_prediction"
                }
            },
            "secondary_analysis": {},
            "overall_ranking": {},
            "paradigm_insights": {
                "fair_comparison_task": "single_step_action_prediction",
                "planning_analysis": "shows_method_specific_strengths",
                "evaluation_approach": "respects_training_paradigms"
            }
        },
        "fairness_report": {
            "evaluation_design": {
                "primary_evaluation": "single_step_fair_comparison",
                "secondary_evaluation": "method_specific_planning_analysis",
                "data_handling": "uses_dataloader_batches_like_training",
                "temporal_structure": "maintained_for_il_model",
                "ground_truth_leakage": "eliminated"
            },
            "method_fairness": {
                "AutoregressiveIL": "evaluated_with_proper_temporal_sequences",
                "WorldModelRL": "evaluated_with_consistent_batch_approach",
                "DirectVideoRL": "evaluated_with_consistent_batch_approach"
            },
            "data_integrity": {
                "temporal_context": "preserved_for_models_that_need_it",
                "batch_structure": "mirrors_training_approach",
                "evaluation_consistency": "matches_model_training_interface"
            },
            "comparison_validity": {
                "single_step_comparison": "valid_and_fair_with_proper_data_handling",
                "planning_comparison": "method_specific_respecting_capabilities",
                "overall_approach": "methodologically_sound_and_consistent_with_training"
            }
        }
    }
}