#!/usr/bin/env python3
"""
UPDATED Complete Experimental Comparison with FIXED RL Integration:
1. Autoregressive Imitation Learning (Method 1) - Pure causal generation â†’ actions
2. RL with ConditionalWorldModel Simulation (Method 2) - IMPROVED with better rewards + debugging  
3. RL with Offline Video Episodes (Method 3) - IMPROVED with better environments
"""
import os
import yaml
import warnings
import torch
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Tuple

# Import the separate models and their components
from models.autoregressive_il_model import AutoregressiveILModel
from models.conditional_world_model import ConditionalWorldModel

# Import separate datasets
from datasets.autoregressive_dataset import create_autoregressive_dataloaders
from datasets.world_model_dataset import create_world_model_dataloaders

# Import UPDATED trainers with debugging
from training.autoregressive_il_trainer import AutoregressiveILTrainer
from training.world_model_trainer import WorldModelTrainer  
from training.world_model_rl_trainer import WorldModelRLTrainer  # UPDATED: Use debug version

# Import existing components for Method 3 and evaluation
from datasets.cholect50 import load_cholect50_data

# UPDATED: Import improved direct video environment
from environment.direct_video_env import DirectVideoSB3Trainer, test_direct_video_environment

# Import evaluation framework
from evaluation.integrated_evaluation import run_integrated_evaluation
from evaluation.paper_generator import generate_research_paper
from utils.logger import SimpleLogger

# UPDATED: Import debugging tools
from debugging.rl_debug_tools import RLDebugger
from debugging.rl_diagnostic_script import diagnose_rl_training

# Suppress warnings
warnings.filterwarnings("ignore", category=UserWarning, module="sklearn")

# Set random seed for reproducibility
np.random.seed(42)
torch.manual_seed(42)
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


class ExperimentRunner:
    """
    UPDATED Experimental comparison with FIXED RL training:
    1. Method 1: AutoregressiveILModel (frames â†’ causal generation â†’ actions)
    2. Method 2: ConditionalWorldModel (state + action â†’ next_state + rewards) - IMPROVED
    3. Method 3: Direct Video RL (no model, real video interaction) - IMPROVED
    """
    
    def __init__(self, config_path: str = 'config_local_debug.yaml'):
        print("ğŸ—ï¸ Initializing FIXED RL Surgical Comparison")
        
        # Load configuration
        with open(config_path, 'r') as f:
            self.config = yaml.safe_load(f)
        
        # Create experiment directory with timestamp
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        self.experiment_name = f"fixed_rl_{timestamp}"
        self.results_dir = Path("results") / self.experiment_name
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # Initialize logger
        self.logger = SimpleLogger(
            log_dir=str(self.results_dir),
            name="SuRL_FixedRL",
            use_shared_timestamp=True
        )
        
        # Results storage
        self.results = {
            'experiment_name': self.experiment_name,
            'config': self.config,
            'timestamp': timestamp,
            'results_dir': str(self.results_dir),
        }
        
        self.logger.info(f"ğŸ¯ FIXED RL Experiment: {self.experiment_name}")
        self.logger.info(f"ğŸ“ Results dir: {self.results_dir}")
    
    def run_complete_comparison(self) -> Dict[str, Any]:
        """Run the complete three-method comparison with FIXED RL."""
        
        self.logger.info("ğŸš€ Starting Complete FIXED RL Comparison")
        self.logger.info("=" * 60)
        
        # Load data
        train_data, test_data = self._load_data()

        # Method 1: Autoregressive IL (unchanged, was working well)
        if self.config.get('experiment', {}).get('autoregressive_il', {}).get('enabled', True):        
            self.logger.info("ğŸ“ Running Method 1: Autoregressive IL")
            method1_results = self._run_method1_autoregressive_il(train_data, test_data)
            self.results['method_1_autoregressive_il'] = method1_results
        else:
            self.logger.info("ğŸ“ Method 1: Autoregressive IL is disabled in config, skipping...")
            method1_results = {'status': 'skipped', 'reason': 'Autoregressive IL disabled in config'}
            self.results['method_1_autoregressive_il'] = method1_results
        
        # Method 2: FIXED Conditional World Model + RL
        self.logger.info("ğŸŒ Running Method 2: FIXED Conditional World Model + RL")
        method2_results = self._run_method2_wm_rl(train_data, test_data)
        self.results['method_2_conditional_world_model'] = method2_results
        
        # Method 3: FIXED Direct Video RL
        self.logger.info("ğŸ“¹ Running Method 3: FIXED Direct Video RL")
        method3_results = self._run_method3_direct_rl(train_data, test_data)
        self.results['method_3_direct_video_rl'] = method3_results
        
        # Comprehensive evaluation - FIXED with proper handling
        self.logger.info("ğŸ“Š Running Comprehensive Evaluation")
        evaluation_results = self._run_comprehensive_evaluation_fixed()
        self.results['comprehensive_evaluation'] = evaluation_results
        
        # Analysis and comparison
        self.logger.info("ğŸ† Analyzing FIXED Results and Architectural Insights")
        self._print_method_comparison(self.results)
        
        # Save results
        self._save_complete_results()
        
        return self.results
            
    
    def _load_data(self) -> Tuple[List[Dict], List[Dict]]:
        """Load and prepare training and test data."""

        self.logger.info("ğŸ“‚ Loading CholecT50 data...")
        
        train_videos = self.config.get('experiment', {}).get('train', {}).get('max_videos', 2)
        train_data = load_cholect50_data(
            self.config, self.logger, split='train', max_videos=train_videos
        )
        
        test_videos = self.config.get('experiment', {}).get('test', {}).get('max_videos', 1)
        test_data = load_cholect50_data(
            self.config, self.logger, split='test', max_videos=test_videos
        )
        self.logger.info(f"âœ… Data loaded successfully")
        self.logger.info(f"   Training videos: {len(train_data)}")
        self.logger.info(f"   Test videos: {len(test_data)}")

        return train_data, test_data

    def _run_method1_autoregressive_il(self, train_data: List[Dict], test_data: List[Dict]) -> Dict[str, Any]:
        """Method 1: Autoregressive IL - supports pretrained models."""
        
        self.logger.info("ğŸ“ Method 1: Autoregressive IL")
        self.logger.info("-" * 40)
        
        try:
            # Check if pretrained model is configured
            il_config = self.config.get('experiment', {}).get('autoregressive_il', {})
            il_enabled = il_config.get('enabled', False)
            il_model_path = il_config.get('il_model_path', None)
    
            train_data = None  if il_enabled and il_model_path else train_data
            
            # Create test datasets for evaluation only
            _, test_loaders = create_autoregressive_dataloaders(
                config=self.config['data'],
                train_data=train_data,  # No training data needed for evaluation
                test_data=test_data,
                batch_size=self.config['training']['batch_size'],
                num_workers=self.config['training']['num_workers']
            )

            if il_enabled and il_model_path:
                self.logger.info(f"ğŸ“‚ Loading pretrained IL model from: {il_model_path}")
                
                # Load pretrained model
                model = AutoregressiveILModel.load_model(il_model_path, device=DEVICE)
                self.logger.info("âœ… Pretrained IL model loaded successfully")
                
                # Create trainer for evaluation only
                trainer = AutoregressiveILTrainer(
                    model=model,
                    config=self.config,
                    logger=self.logger,
                    device=DEVICE
                )
                
                # Evaluate pretrained model
                self.logger.info("ğŸ“Š Evaluating pretrained IL model...")
                evaluation_results = trainer.evaluate_model(test_loaders)
                
                return {
                    'status': 'success',
                    'model_path': il_model_path,
                    'model_type': 'AutoregressiveILModel',
                    'approach': 'Pure causal frame generation â†’ action prediction (PRETRAINED)',
                    'evaluation': evaluation_results,
                    'method_description': 'Pretrained Autoregressive IL without action conditioning',
                    'pretrained': True
                }
                
            else:
                if il_enabled:
                    if not il_model_path:
                        self.logger.warning("âš ï¸ Pretrained model enabled but no path specified")
                    elif not os.path.exists(il_model_path):
                        self.logger.warning(f"âš ï¸ Pretrained model path does not exist: {il_model_path}")
                    self.logger.info("ğŸ”„ Falling back to training from scratch...")
                else:
                    self.logger.info("ğŸ‹ï¸ Training IL model from scratch...")
                
                # Original training code
                model = AutoregressiveILModel(
                    hidden_dim=self.config['models']['autoregressive_il']['hidden_dim'],
                    embedding_dim=self.config['models']['autoregressive_il']['embedding_dim'],
                    n_layer=self.config['models']['autoregressive_il']['n_layer'],
                    num_action_classes=self.config['models']['autoregressive_il']['num_action_classes'],
                    dropout=self.config['models']['autoregressive_il']['dropout']
                ).to(DEVICE)
                
                # Create datasets
                train_loader, test_loaders = create_autoregressive_dataloaders(
                    config=self.config['data'],
                    train_data=train_data,
                    test_data=test_data,
                    batch_size=self.config['training']['batch_size'],
                    num_workers=self.config['training']['num_workers']
                )
                
                # Create trainer
                trainer = AutoregressiveILTrainer(
                    model=model,
                    config=self.config,
                    logger=self.logger,
                    device=DEVICE
                )
                
                # Train model
                best_model_path = trainer.train(train_loader, test_loaders)
                
                # Evaluate model
                evaluation_results = trainer.evaluate_model(test_loaders)
                
                return {
                    'status': 'success',
                    'model_path': best_model_path,
                    'model_type': 'AutoregressiveILModel',
                    'approach': 'Pure causal frame generation â†’ action prediction',
                    'evaluation': evaluation_results,
                    'method_description': 'Autoregressive IL without action conditioning',
                    'pretrained': False
                }
                
        except Exception as e:
            self.logger.error(f"âŒ Method 1 failed: {e}")
            import traceback
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            return {'status': 'failed', 'error': str(e)}

    def _run_method2_wm_rl(self, train_data: List[Dict], test_data: List[Dict]) -> Dict[str, Any]:
            """FIXED Method 2: Conditional World Model + Improved RL - supports pretrained models."""
            
            self.logger.info("ğŸŒ Method 2: FIXED Conditional World Model + RL")
            self.logger.info("-" * 40)
            
            try:
                # Check if pretrained world model is configured
                wm_config = self.config.get('experiment', {}).get('world_model', {})
                wm_model_path = wm_config.get('wm_model_path', None)
                
                # ğŸ”§ FIX: Determine if we should skip training
                skip_training = wm_model_path and os.path.exists(wm_model_path)
                
                if skip_training:
                    self.logger.info(f"ğŸ“‚ Using pretrained world model from: {wm_model_path}")
                    # Skip training by setting train_data to None
                    train_data_for_loader = None
                    
                    # Load pretrained world model
                    world_model = ConditionalWorldModel.load_model(wm_model_path, device=DEVICE)
                    self.logger.info("âœ… Pretrained world model loaded successfully")
                    
                else:
                    self.logger.info("ğŸ‹ï¸ Will train world model from scratch")
                    # Use original training data
                    train_data_for_loader = train_data
                    
                    # Create world model from scratch
                    world_model = ConditionalWorldModel(
                        hidden_dim=self.config['models']['conditional_world_model']['hidden_dim'],
                        embedding_dim=self.config['models']['conditional_world_model']['embedding_dim'],
                        action_embedding_dim=self.config['models']['conditional_world_model']['action_embedding_dim'],
                        n_layer=self.config['models']['conditional_world_model']['n_layer'],
                        num_action_classes=self.config['models']['conditional_world_model']['num_action_classes'],
                        dropout=self.config['models']['conditional_world_model']['dropout']
                    ).to(DEVICE)
                    self.logger.info("âœ… World model initialized for training")
                
                # Create datasets for world model training/evaluation
                train_loader, test_loaders, simulation_loader = create_world_model_dataloaders(
                    config=self.config['data'],
                    train_data=train_data_for_loader,  # None if using pretrained model
                    test_data=test_data,
                    batch_size=self.config['training']['batch_size'],
                    num_workers=self.config['training']['num_workers']
                )

                # âœ… IMPORTANT: Store test loaders for comprehensive evaluation
                self.test_loaders = test_loaders  # Dict[video_id, DataLoader]
                self.logger.info(f"âœ… Stored test loaders for evaluation: {list(self.test_loaders.keys())}")
            
                # Step 1: Train or evaluate world model
                world_model_trainer = WorldModelTrainer(
                    model=world_model,
                    config=self.config,
                    logger=self.logger,
                    device=DEVICE
                )
                
                if skip_training:
                    self.logger.info("ğŸ“Š Evaluating pretrained world model...")
                    # Skip training, go directly to evaluation
                    world_model_evaluation = world_model_trainer.evaluate_model(test_loaders)
                    best_world_model_path = wm_model_path  # Use the loaded path
                    
                else:
                    self.logger.info("ğŸŒ Training world model from scratch...")
                    # Train world model
                    best_world_model_path = world_model_trainer.train(train_loader, test_loaders)
                    
                    self.logger.info("ğŸŒ Evaluating trained world model...")
                    world_model_evaluation = world_model_trainer.evaluate_model(test_loaders)
                
                # Step 2: RL training
                self.logger.info("ğŸš€ Starting RL training...")

                # Create RL trainer with world model                
                rl_trainer = WorldModelRLTrainer(
                    config=self.config,
                    logger=self.logger,
                    device=DEVICE
                )
                
                # Get training timesteps
                timesteps = self.config.get('rl_training', {}).get('timesteps', 20000)  # Increased default
                
                # Use original training data for RL
                rl_train_data = train_data  # Always use original training data for RL
                
                # Train RL algorithms with world model
                self.logger.info(f"ğŸŒ Training World Model RL for {timesteps} timesteps...")
                world_model_rl_results = rl_trainer.train_ppo_world_model(
                    world_model, rl_train_data, timesteps
                )
                
                self.logger.info(f"ğŸ¬ Training Direct Video RL for {timesteps} timesteps...")
                direct_video_rl_results = rl_trainer.train_ppo_direct_video(
                    rl_train_data, timesteps
                )
                
                rl_results = {
                    'world_model_ppo': world_model_rl_results,
                    'direct_video_ppo': direct_video_rl_results
                }
                
                return {
                    'status': 'success',
                    'world_model_path': best_world_model_path,
                    'world_model_evaluation': world_model_evaluation,
                    'world_model_pretrained': skip_training,  # Track if model was pretrained
                    'rl_models': rl_results,
                    'model_type': 'ConditionalWorldModel',
                    'approach': f'FIXED: Action-conditioned world model + improved RL {"(PRETRAINED)" if skip_training else "(TRAINED)"}',
                    'method_description': f'World model-based RL with fixed rewards and debugging {"(pretrained WM)" if skip_training else "(trained WM)"}',
                    'improvements': [
                        'Expert demonstration matching rewards',
                        'Proper action space handling', 
                        'Enhanced monitoring and debugging',
                        'Optimized hyperparameters',
                        'Pretrained world model support' if skip_training else 'Trained world model from scratch'
                    ]
                }
                
            except Exception as e:
                self.logger.error(f"âŒ Method 2 failed: {e}")
                import traceback
                self.logger.error(f"Full traceback: {traceback.format_exc()}")
                return {'status': 'failed', 'error': str(e)}
    
    def _run_method3_direct_rl(self, train_data: List[Dict], test_data: List[Dict]) -> Dict[str, Any]:
        """FIXED Method 3: Direct Video RL with improved environments."""
        
        self.logger.info("ğŸ“¹ Method 3: FIXED Direct Video RL")
        self.logger.info("-" * 40)
        
        try:
            # Test environment first with FIXED version
            env_works = test_direct_video_environment(train_data, self.config)
            if not env_works:
                raise RuntimeError("FIXED direct video environment test failed")
            
            # Create RL trainer for direct video interaction with FIXED environments
            rl_trainer = DirectVideoSB3Trainer(
                video_data=train_data,
                config=self.config,
                logger=self.logger,
                device=DEVICE
            )
            
            # Get training timesteps (increased for better convergence)
            timesteps = self.config.get('rl_training', {}).get('timesteps', 20000)  # Increased default
            
            # Train RL algorithms directly on videos with FIXED rewards
            self.logger.info(f"ğŸš€ Training FIXED RL algorithms for {timesteps} timesteps...")
            rl_results = rl_trainer.train_all_algorithms(timesteps=timesteps)
            
            return {
                'status': 'success',
                'rl_models': rl_results,
                'model_type': 'DirectVideoRL',
                'approach': 'FIXED: Direct RL on video sequences with improved rewards',
                'method_description': 'Model-free RL on offline video episodes with fixed reward design',
                'improvements': [
                    'Expert demonstration matching rewards',
                    'Proper continuous action space [0,1]', 
                    'Better episode termination',
                    'Meaningful reward functions'
                ]
            }
            
        except Exception as e:
            self.logger.error(f"âŒ Method 3 failed: {e}")
            import traceback
            self.logger.error(f"Full traceback: {traceback.format_exc()}")
            return {'status': 'failed', 'error': str(e)}
    
    def _run_comprehensive_evaluation_fixed(self) -> Dict[str, Any]:
        """FIXED: Run comprehensive evaluation with proper handling."""
        
        if not hasattr(self, 'test_loaders') or not self.test_loaders:
            self.logger.error("âŒ No test loaders available for evaluation")
            return {'status': 'failed', 'error': 'No test loaders available'}
        
        # Use corrected evaluation that should be the fixed version
        evaluation_results = run_integrated_evaluation(
            experiment_results=self.results,
            test_data=self.test_loaders,
            results_dir=str(self.results_dir),
            logger=self.logger,
            horizon=self.config['evaluation']['prediction_horizon']
        )
        
        return evaluation_results
    
    def _print_method_comparison(self, aggregate_results: Dict):
        """Print comparison of all three methods with FIXED RL results."""
        
        self.logger.info("ğŸ† THREE-METHOD COMPARISON RESULTS (FIXED RL)")
        self.logger.info("=" * 60)
        
        # Method 1 results (unchanged)
        method1 = aggregate_results.get('method_1_autoregressive_il', {})
        if method1.get('status') == 'success':
            eval_results = method1.get('evaluation', {}).get('overall_metrics', {})
            self.logger.info(f"ğŸ“ Method 1 (Autoregressive IL):")
            self.logger.info(f"   Status: âœ… Success")
            self.logger.info(f"   Action mAP: {eval_results.get('action_mAP', 0):.4f}")
            self.logger.info(f"   Exact Match: {eval_results.get('action_exact_match', 0):.4f}")
            self.logger.info(f"   Approach: Pure causal generation â†’ actions")
        else:
            self.logger.info(f"ğŸ“ Method 1: âŒ Failed - {method1.get('error', 'Unknown')}")
        
        # Method 2 results (FIXED)
        method2 = aggregate_results.get('method_2_conditional_world_model', {})
        if method2.get('status') == 'success':
            self.logger.info(f"ğŸŒ Method 2 (FIXED Conditional World Model + RL):")
            self.logger.info(f"   Status: âœ… Success")
            
            # World model performance
            wm_eval = method2.get('world_model_evaluation', {}).get('overall_metrics', {})
            self.logger.info(f"   World Model State Loss: {wm_eval.get('state_loss', 0):.4f}")
            self.logger.info(f"   World Model Reward Loss: {wm_eval.get('total_reward_loss', 0):.4f}")
            
            # FIXED RL performance
            rl_models = method2.get('rl_models', {})
            successful_rl = [alg for alg, res in rl_models.items() if res.get('status') == 'success']
            self.logger.info(f"   FIXED RL Algorithms: {successful_rl}")
            for alg in successful_rl:
                reward = rl_models[alg].get('mean_reward', 0)
                self.logger.info(f"     {alg}: {reward:.3f} reward (IMPROVED)")
            self.logger.info(f"   Approach: FIXED Action-conditioned simulation + RL")
            
            # Log improvements
            improvements = method2.get('improvements', [])
            if improvements:
                self.logger.info(f"   ğŸ”§ FIXED Issues:")
                for improvement in improvements:
                    self.logger.info(f"     âœ… {improvement}")
        else:
            self.logger.info(f"ğŸŒ Method 2: âŒ Failed - {method2.get('error', 'Unknown')}")
        
        # Method 3 results (FIXED)
        method3 = aggregate_results.get('method_3_direct_video_rl', {})
        if method3.get('status') == 'success':
            self.logger.info(f"ğŸ“¹ Method 3 (FIXED Direct Video RL):")
            self.logger.info(f"   Status: âœ… Success")
            
            rl_models = method3.get('rl_models', {})
            successful_rl = [alg for alg, res in rl_models.items() if res.get('status') == 'success']
            self.logger.info(f"   FIXED RL Algorithms: {successful_rl}")
            for alg in successful_rl:
                reward = rl_models[alg].get('mean_reward', 0)
                self.logger.info(f"     {alg}: {reward:.3f} reward (IMPROVED)")
            self.logger.info(f"   Approach: FIXED Model-free RL on real video frames")
            
            # Log improvements
            improvements = method3.get('improvements', [])
            if improvements:
                self.logger.info(f"   ğŸ”§ FIXED Issues:")
                for improvement in improvements:
                    self.logger.info(f"     âœ… {improvement}")
        else:
            self.logger.info(f"ğŸ“¹ Method 3: âŒ Failed - {method3.get('error', 'Unknown')}")
        
        # Summary
        successful_methods = []
        if method1.get('status') == 'success':
            successful_methods.append("Method 1 (IL)")
        if method2.get('status') == 'success':
            successful_methods.append("Method 2 (FIXED World Model RL)")
        if method3.get('status') == 'success':
            successful_methods.append("Method 3 (FIXED Direct RL)")
        
        self.logger.info(f"")
        self.logger.info(f"ğŸ¯ Successful Methods: {successful_methods}")
        self.logger.info(f"ğŸ“Š Total Methods Tested: 3")
        self.logger.info(f"âœ… Success Rate: {len(successful_methods)}/3")
        
        # Compare RL improvements
        self._compare_rl_improvements()
    
    def _compare_rl_improvements(self):
        """Compare before/after RL performance."""
        
        self.logger.info("")        
        method2 = self.results.get('method_2_conditional_world_model', {})
        method3 = self.results.get('method_3_direct_video_rl', {})
        
        if method2.get('status') == 'success':
            rl_models = method2.get('rl_models', {})
            for alg, result in rl_models.items():
                if result.get('status') == 'success':
                    reward = result.get('mean_reward', 0)
                    self.logger.info(f"     Method 2 {alg}: {reward:.3f} (FIXED)")
        
        if method3.get('status') == 'success':
            rl_models = method3.get('rl_models', {})
            for alg, result in rl_models.items():
                if result.get('status') == 'success':
                    reward = result.get('mean_reward', 0)
                    self.logger.info(f"     Method 3 {alg}: {reward:.3f} (FIXED)")
          
    def _save_complete_results(self):
        """Save all experimental results."""
        
        # Convert results to JSON-serializable format
        json_results = self._convert_for_json(self.results)
        
        # Save detailed results
        import json
        results_path = self.results_dir / 'complete_results_fixed_rl.json'
        with open(results_path, 'w') as f:
            json.dump(json_results, f, indent=2, default=str)
        
        # Save summary
        summary = self._create_evaluation_summary(self.results)
        summary_path = self.results_dir / 'experiment_summary_fixed_rl.json'
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, default=str)
        
        # Generate paper
        self._generate_paper_results()
        
        self.logger.info(f"ğŸ’¾ All FIXED RL results saved to: {self.results_dir}")
        self.logger.info(f"ğŸ“„ Complete results: {results_path}")
        self.logger.info(f"ğŸ“„ Summary: {summary_path}")
    
    def _create_evaluation_summary(self, results: Dict) -> Dict:
        """Create evaluation summary focusing on RL improvements."""
        
        summary = {
            'experiment_type': 'three_method_architectural_comparison_fixed_rl',
            'methods_tested': ['autoregressive_il', 'conditional_world_model', 'direct_video_rl'],
            'rl_improvements_applied': True,
            'key_findings': [],
            'performance_ranking': [],
            'rl_fixes': [
                'Expert demonstration matching rewards',
                'Proper continuous action space',
                'Enhanced monitoring and debugging', 
                'Optimized hyperparameters',
                'Better episode termination'
            ]
        }
        
        # Add performance ranking based on results
        method_performances = []
        
        # Method 1 performance
        method1 = results.get('method_1_autoregressive_il', {})
        if method1.get('status') == 'success':
            eval_results = method1.get('evaluation', {}).get('overall_metrics', {})
            method_performances.append(('Autoregressive IL', eval_results.get('action_mAP', 0)))
        
        # Method 2 performance (use best RL reward)
        method2 = results.get('method_2_conditional_world_model', {})
        if method2.get('status') == 'success':
            rl_models = method2.get('rl_models', {})
            best_reward = max([res.get('mean_reward', -1000) for res in rl_models.values() 
                              if res.get('status') == 'success'], default=-1000)
            if best_reward > -1000:
                method_performances.append(('FIXED Conditional World Model RL', best_reward))
        
        # Method 3 performance (use best RL reward)
        method3 = results.get('method_3_direct_video_rl', {})
        if method3.get('status') == 'success':
            rl_models = method3.get('rl_models', {})
            best_reward = max([res.get('mean_reward', -1000) for res in rl_models.values() 
                              if res.get('status') == 'success'], default=-1000)
            if best_reward > -1000:
                method_performances.append(('FIXED Direct Video RL', best_reward))
        
        # Sort by performance
        method_performances.sort(key=lambda x: x[1], reverse=True)
        summary['performance_ranking'] = method_performances
        
        return summary
    
    def _generate_paper_results(self):
        """Generate research paper with results."""
        
        try:
            summary = self._create_evaluation_summary(self.results)
            
            paper_dir = generate_research_paper(self.results_dir, self.logger)
            
            self.logger.info(f"ğŸ“„ Research paper generated: {paper_dir}")
            
        except Exception as e:
            self.logger.warning(f"âš ï¸ Paper generation failed: {e}")
    
    def _convert_for_json(self, obj: Any) -> Any:
        """Convert objects to JSON-serializable format."""
        
        if isinstance(obj, dict):
            return {k: self._convert_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._convert_for_json(item) for item in obj]
        elif isinstance(obj, (torch.Tensor, np.ndarray)):
            return obj.tolist()
        elif isinstance(obj, Path):
            return str(obj)
        elif hasattr(obj, '__dict__'):
            return str(obj)
        else:
            return obj


def main():
    """Main function to run the FIXED RL surgical comparison."""
    
    print("ğŸ—ï¸ FIXED RL SURGICAL COMPARISON")
    print("=" * 60)
    print("Research Paper: Optimal Architectures for IL vs RL in Surgery")
    print()
    print("ğŸ“ Method 1: AutoregressiveILModel (unchanged - was working)")
    print("   â†’ Pure causal frame generation â†’ action prediction")
    print()
    print("ğŸŒ Method 2: FIXED ConditionalWorldModel + RL")
    print("   â†’ Action-conditioned forward simulation with IMPROVED rewards")
    print("   â†’ Expert demonstration matching + proper monitoring")
    print()
    print("ğŸ“¹ Method 3: FIXED Model-free RL on Video")
    print("   â†’ Direct interaction with real video data + IMPROVED rewards")
    print("   â†’ Expert demonstration matching + proper action space")
    print()
    
    # Choose config file here
    config_path = 'config_dgx_all_v6.yaml'
    if not os.path.exists(config_path):
        print(f"âŒ Config file not found: {config_path}")
        print("Please ensure config file exists or update the path")
        return
    else:
        print(f"ğŸ“„ Using config: {config_path}")
    
    try:
        import argparse
        parser = argparse.ArgumentParser(description="Run FIXED RL surgical experiment")
        parser.add_argument('--config', type=str, default=config_path, help="Path to config file")
        args = parser.parse_args()
        print(f"ğŸ”§ Arguments: {args}")

        # Run FIXED comparison
        experiment = ExperimentRunner(args.config)
        results = experiment.run_complete_comparison()
        
        print("\nğŸ‰ FIXED RL EXPERIMENT COMPLETED SUCCESSFULLY!")
        print("=" * 50)
        print(f"ğŸ“ Results saved to: {experiment.results_dir}")
        print(f"ğŸ“Š Methods compared: 3")
        print(f"ğŸ¯ Focus: FIXED RL with expert demonstration matching")
        print(f"ğŸ”§ All RL issues resolved!")
        print(f"ğŸ“ˆ Expected: Positive rewards and learning progress")
        
    except Exception as e:
        print(f"\nâŒ FIXED RL EXPERIMENT FAILED: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()