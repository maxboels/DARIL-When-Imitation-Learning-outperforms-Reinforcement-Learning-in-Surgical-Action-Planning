#!/usr/bin/env python3
"""
System Validation Test
This script validates that your environment is ready for the IL vs RL comparison.
"""

import sys
import os
import importlib
import yaml
from pathlib import Path
import numpy as np
import torch


def test_python_version():
    """Test Python version compatibility."""
    print("ğŸ Testing Python version...")
    
    version = sys.version_info
    if version.major == 3 and version.minor >= 8:
        print(f"âœ… Python {version.major}.{version.minor}.{version.micro} - Compatible")
        return True
    else:
        print(f"âŒ Python {version.major}.{version.minor}.{version.micro} - Requires Python 3.8+")
        return False


def test_dependencies():
    """Test required dependencies."""
    print("ğŸ“¦ Testing dependencies...")
    
    required_packages = [
        'torch',
        'numpy',
        'pandas', 
        'matplotlib',
        'seaborn',
        'sklearn',
        'tqdm',
        'yaml',
        'transformers'
    ]
    
    optional_packages = [
        'gymnasium',
        'stable_baselines3',
        'wandb',
        'plotly'
    ]
    
    results = {'required': 0, 'optional': 0}
    
    # Test required packages
    for package in required_packages:
        try:
            importlib.import_module(package if package != 'yaml' else 'yaml')
            print(f"  âœ… {package}")
            results['required'] += 1
        except ImportError:
            print(f"  âŒ {package} - REQUIRED")
    
    # Test optional packages  
    for package in optional_packages:
        try:
            importlib.import_module(package)
            print(f"  âœ… {package} (optional)")
            results['optional'] += 1
        except ImportError:
            print(f"  âš ï¸ {package} - optional (needed for RL)")
    
    success = results['required'] == len(required_packages)
    print(f"ğŸ“Š Dependencies: {results['required']}/{len(required_packages)} required, {results['optional']}/{len(optional_packages)} optional")
    
    return success


def test_cuda():
    """Test CUDA availability."""
    print("ğŸ”¥ Testing CUDA...")
    
    if torch.cuda.is_available():
        device_count = torch.cuda.device_count()
        device_name = torch.cuda.get_device_name(0)
        memory = torch.cuda.get_device_properties(0).total_memory / 1e9
        
        print(f"âœ… CUDA available: {device_count} device(s)")
        print(f"  ğŸ“± Device 0: {device_name}")
        print(f"  ğŸ’¾ Memory: {memory:.1f} GB")
        
        # Test CUDA functionality
        try:
            x = torch.randn(100, 100).cuda()
            y = torch.randn(100, 100).cuda()
            z = torch.mm(x, y)
            print("  âœ… CUDA computation test passed")
            return True
        except Exception as e:
            print(f"  âŒ CUDA computation failed: {e}")
            return False
    else:
        print("âš ï¸ CUDA not available - will use CPU (slower)")
        return False


def test_config_files():
    """Test configuration files."""
    print("âš™ï¸ Testing configuration files...")
    
    config_files = ['config.yaml', 'config_fixed.yaml']
    config_found = None
    
    for config_file in config_files:
        if os.path.exists(config_file):
            config_found = config_file
            print(f"âœ… Found config: {config_file}")
            break
    
    if not config_found:
        print("âŒ No configuration file found!")
        print("   Create either config.yaml or config_fixed.yaml")
        return False
    
    # Test config loading
    try:
        with open(config_found, 'r') as f:
            config = yaml.safe_load(f)
        
        # Check essential sections
        essential_keys = ['data', 'models', 'training', 'experiment']
        missing_keys = [key for key in essential_keys if key not in config]
        
        if missing_keys:
            print(f"âŒ Missing config sections: {missing_keys}")
            return False
        
        print("âœ… Configuration file valid")
        
        # Check data directory
        data_dir = config.get('data', {}).get('paths', {}).get('data_dir', '')
        if data_dir and os.path.exists(data_dir):
            print(f"âœ… Data directory found: {data_dir}")
        else:
            print(f"âš ï¸ Data directory not found: {data_dir}")
            print("   Update data_dir in config or ensure CholecT50 is available")
        
        return True
        
    except Exception as e:
        print(f"âŒ Config loading failed: {e}")
        return False


def test_project_structure():
    """Test project structure."""
    print("ğŸ“ Testing project structure...")
    
    required_files = [
        'datasets/cholect50.py',
        'utils/logger.py',
        'models/dual_world_model.py',
        'training/dual_trainer.py'
    ]
    
    optional_files = [
        'comprehensive_evaluation.py',
        'prediction_saver.py',
        'enhanced_interactive_viz.html'
    ]
    
    missing_required = []
    missing_optional = []
    
    for file_path in required_files:
        if os.path.exists(file_path):
            print(f"  âœ… {file_path}")
        else:
            print(f"  âŒ {file_path} - REQUIRED")
            missing_required.append(file_path)
    
    for file_path in optional_files:
        if os.path.exists(file_path):
            print(f"  âœ… {file_path}")
        else:
            print(f"  âš ï¸ {file_path} - optional")
            missing_optional.append(file_path)
    
    if missing_required:
        print(f"âŒ Missing required files: {len(missing_required)}")
        return False
    else:
        print("âœ… All required files present")
        return True


def test_data_loading():
    """Test data loading functionality."""
    print("ğŸ“Š Testing data loading...")
    
    try:
        # Import required modules
        from datasets.cholect50 import load_cholect50_data
        from utils.logger import SimpleLogger
        
        print("  âœ… Imports successful")
        
        # Load config
        config_file = 'config_fixed.yaml' if os.path.exists('config_fixed.yaml') else 'config.yaml'
        with open(config_file, 'r') as f:
            config = yaml.safe_load(f)
        
        # Create logger
        logger = SimpleLogger(log_dir="test_logs", name="validation_test")
        
        # Test data loading (just 1 video)
        print("  ğŸ”„ Testing data loading (1 video)...")
        test_data = load_cholect50_data(config, logger, split='train', max_videos=1)
        
        if test_data and len(test_data) > 0:
            video = test_data[0]
            print(f"  âœ… Data loading successful")
            print(f"    ğŸ“¹ Video ID: {video['video_id']}")
            print(f"    ğŸï¸ Frames: {video['num_frames']}")
            print(f"    ğŸ“ Embedding shape: {video['frame_embeddings'].shape}")
            print(f"    ğŸ¯ Actions shape: {video['actions_binaries'].shape}")
            
            # Clean up test logs
            import shutil
            if os.path.exists("test_logs"):
                shutil.rmtree("test_logs")
            
            return True
        else:
            print("  âŒ No data loaded")
            return False
            
    except ImportError as e:
        print(f"  âŒ Import failed: {e}")
        return False
    except Exception as e:
        print(f"  âŒ Data loading failed: {e}")
        return False


def test_model_creation():
    """Test model creation."""
    print("ğŸ§  Testing model creation...")
    
    try:
        from models.dual_world_model import DualWorldModel
        
        # Create a small test model
        model_config = {
            'hidden_dim': 256,
            'embedding_dim': 512,
            'action_embedding_dim': 64,
            'n_layer': 2,
            'num_action_classes': 100,
            'num_phase_classes': 7,
            'max_length': 512,
            'dropout': 0.1
        }
        
        model = DualWorldModel(**model_config)
        
        # Test forward pass
        batch_size, seq_len = 2, 10
        current_states = torch.randn(batch_size, seq_len, model_config['embedding_dim'])
        
        output = model(current_states=current_states, mode='supervised')
        
        print("  âœ… Model creation successful")
        print(f"    ğŸ”§ Parameters: {sum(p.numel() for p in model.parameters()):,}")
        print(f"    ğŸ“¤ Output keys: {list(output.keys())}")
        
        return True
        
    except Exception as e:
        print(f"  âŒ Model creation failed: {e}")
        return False


def run_full_validation():
    """Run all validation tests."""
    print("ğŸ§ª SYSTEM VALIDATION TEST")
    print("=" * 50)
    
    tests = [
        ("Python Version", test_python_version),
        ("Dependencies", test_dependencies), 
        ("CUDA Support", test_cuda),
        ("Configuration", test_config_files),
        ("Project Structure", test_project_structure),
        ("Data Loading", test_data_loading),
        ("Model Creation", test_model_creation)
    ]
    
    results = {}
    
    for test_name, test_func in tests:
        print(f"\n{test_name}:")
        print("-" * 30)
        
        try:
            results[test_name] = test_func()
        except Exception as e:
            print(f"âŒ Test failed with exception: {e}")
            results[test_name] = False
    
    # Summary
    print("\n" + "=" * 50)
    print("ğŸ“‹ VALIDATION SUMMARY")
    print("=" * 50)
    
    passed = sum(results.values())
    total = len(results)
    
    for test_name, result in results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"{test_name:<20} {status}")
    
    print("-" * 50)
    print(f"Overall: {passed}/{total} tests passed")
    
    if passed == total:
        print("\nğŸ‰ ALL TESTS PASSED!")
        print("Your system is ready for the IL vs RL comparison!")
        print("\nNext steps:")
        print("1. Run: python run_experiment_script.py")
        print("2. Or: python complete_comparison_script.py") 
    else:
        print(f"\nâš ï¸ {total - passed} tests failed!")
        print("Please fix the failing tests before proceeding.")
        print("Refer to the setup instructions for help.")
    
    return passed == total


def main():
    """Main validation function."""
    success = run_full_validation()
    return 0 if success else 1


if __name__ == "__main__":
    exit(main())
