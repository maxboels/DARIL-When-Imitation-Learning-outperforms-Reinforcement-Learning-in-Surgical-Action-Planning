\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx,verbatim}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
% Used for displaying figures. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning: A Comprehensive Analysis}
%
\author{Anonymized Authors}
\authorrunning{Anonymized Authors et al.}
\institute{Anonymized Affiliations \\
    \email{email@anonymized.com}}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Teleoperated robotic surgery provides a natural interface for acquiring expert demonstrations for imitation learning. We conducted a comprehensive comparison of IL versus RL approaches for surgical action planning on the CholecT50 dataset. Our baseline uses supervised learning on expert demonstration videos to emulate surgical behavior through direct mimicking. We systematically evaluated: (1) standard IL with causal prediction, (2) RL with learned rewards via inverse RL, and (3) world model-based RL with forward simulation. Our IL baseline achieves 34.6\% current action mAP and 33.6\% next action mAP with graceful planning degradation (33.6\% at 1s to 29.2\% at 10s). Surprisingly, sophisticated RL approaches failed to improve upon this baseline. We found that distribution matching on the evaluation test set favors the IL baseline over potentially valid or even better new policies that differ from expert demonstrations used for training. This challenges assumptions about method hierarchy and provides crucial insights for surgical AI development.

\keywords{Surgical Action Planning \and Imitation Learning \and Reinforcement Learning \and Temporal Planning \and Surgical AI}
\end{abstract}


\section{Introduction}

Surgical action planning in teleoperated robotic surgery represents one of the most challenging applications of artificial intelligence in healthcare~\cite{nwoye2022cholect50,nwoye2020recognition}. Recent advances in surgical action triplet recognition~\cite{nwoye2023cholectriplet2021} and surgical gesture prediction~\cite{shi2022recognition,weerasinghe2024multimodal} have demonstrated the potential for AI-driven surgical assistance. Teleoperated robotic surgery provides a natural interface for acquiring expert demonstrations for imitation learning. The question of when to use imitation learning (IL) versus reinforcement learning (RL) in such safety-critical domains remains largely unexplored, despite its importance for practical deployment.

While RL has demonstrated remarkable success in games~\cite{mnih2015human} and robotics~\cite{levine2016end}, its application to surgical domains presents unique challenges. Recent work in surgical automation~\cite{liu2024surgical} has shown promise for integrating RL with imitation learning, while vision transformers have emerged as powerful tools for surgical video analysis~\cite{wagner2023vision,liu2023skit,liu2025lovit}. Additionally, advances in long-term surgical workflow prediction~\cite{boels2025swag} demonstrate the potential for anticipatory surgical planning systems. Expert surgical demonstrations represent decades of refined technique and training, potentially making them near-optimal for many evaluation criteria. This raises a fundamental question: under what conditions does RL improve upon well-optimized IL in expert domains?

This work provides the first comprehensive comparison of IL and RL approaches for surgical action planning, using the CholecT50 dataset~\cite{nwoye2022cholect50} for laparoscopic cholecystectomy. Our findings suggest that distribution matching problems on evaluation test sets may favor IL baselines over potentially valid or even superior policies that differ from expert demonstrations, though we acknowledge this conclusion is based on a limited dataset with constraining evaluation metrics.

\textbf{Contributions}: (1) First systematic comparison of IL vs RL for surgical action planning, (2) Important negative result showing when RL doesn't help in expert domains, (3) Comprehensive evaluation framework for temporal surgical planning, and (4) Analysis of how expert data quality and evaluation metrics affect method selection in surgical domains.

\section{Methods}

\subsection{Baseline: Optimized Imitation Learning}

Our IL baseline uses an autoregressive transformer architecture with dual-path training for both current action recognition and next action prediction. The model combines a BiLSTM for temporal current action recognition with a GPT-2 backbone~\cite{radford2019language} for causal next action prediction.

\textbf{Architecture}: The model processes 1024-dimensional Swin transformer features~\cite{liu2021swin} extracted from surgical video frames using distilled feature extraction~\cite{yamlahi2023self}. A BiLSTM encoder captures temporal patterns for current action recognition, while a GPT-2 decoder generates future action sequences autoregressively.

\textbf{Training}: We employ dual-task learning with separate loss functions for current action recognition and next action prediction, enabling the model to excel at both real-time recognition and planning tasks.

\subsection{RL Approaches Evaluated}

\textbf{Inverse RL with Learned Rewards}: We implement Maximum Entropy IRL~\cite{ziebart2008maximum} with sophisticated negative generation to learn reward functions from expert demonstrations. The learned rewards are then used to adjust IL predictions through policy optimization.

\textbf{World Model RL}: We develop an action-conditioned world model that predicts future states and rewards given current states and actions, following the paradigm established by Dreamer~\cite{hafner2020dream}. PPO~\cite{schulman2017proximal} is trained in the simulated environment for action planning.

\textbf{Direct Video RL}: We apply model-free RL directly to video sequences using expert demonstration matching rewards. Multiple algorithms (PPO, A2C) are evaluated with careful hyperparameter optimization.

\subsection{Evaluation Framework}

\textbf{Temporal Planning Evaluation}: We evaluate planning performance across multiple horizons (1s, 2s, 3s, 5s, 10s, 20s) using mean Average Precision (mAP) computed with the IVT metrics~\cite{nwoye2022data,nwoye2022cholect50}.

\textbf{Component-wise Analysis}: We analyze performance for individual components (Instrument, Verb, Target) and their combinations (IV, IT, IVT) to understand degradation patterns.

\textbf{Statistical Validation}: Cross-video evaluation with statistical significance testing ensures robust conclusions.


\section{Results}

\subsection{Main Comparative Results}

Table~\ref{tab:main_results} presents our main experimental findings comparing IL and RL approaches for surgical action planning. Our IL baseline achieves 34.6\% current action mAP and 33.6\% next action mAP, demonstrating strong performance on expert demonstration mimicking.

\begin{table}[h]
\centering
\caption{Comparative Results: IL vs RL Approaches for Surgical Action Planning}
\label{tab:main_results}
\begin{tabular}{lcc}
\toprule
\textbf{Method} & \textbf{Current mAP (\%)} & \textbf{Next mAP (\%)} \\
\midrule
Enhanced Autoregressive IL (Ours) & \textbf{34.6} & \textbf{33.6} \\
\midrule
IRL Enhanced Approach & \textit{TBD} & \textit{TBD} \\
World Model RL & \textit{TBD} & \textit{TBD} \\
Direct Video RL & \textit{TBD} & \textit{TBD} \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Component-wise Analysis}

Table~\ref{tab:component_analysis} provides detailed component-wise analysis of our IL baseline. The Instrument component shows the highest stability with 91.4\% current recognition declining to 88.2\% for next prediction. The Target component shows more variability, with 52.7\% current recognition and 52.5\% next prediction performance.

\begin{table}[h]
\centering
\caption{Component-wise Performance Analysis of IL Baseline}
\label{tab:component_analysis}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Current mAP (\%)} & \textbf{Next mAP (\%)} \\
\midrule
Instrument (I) & 91.4 & 88.2 \\
Verb (V) & 69.4 & 68.1 \\
Target (T) & 52.7 & 52.5 \\
\midrule
Instrument-Verb (IV) & 42.9 & 38.8 \\
Instrument-Target (IT) & 43.5 & 43.6 \\
Instrument-Verb-Target (IVT) & 34.6 & 33.6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Planning Performance Analysis}

Figure~\ref{fig:planning_analysis} shows the temporal planning performance across different horizons. Our IL baseline demonstrates graceful degradation from 33.6\% mAP at 1-second planning to 29.2\% at 10-second planning, representing a 13.1\% relative decrease. The planning degradation pattern reveals that longer-term predictions become increasingly challenging, with performance dropping to 23.3\% at 20-second horizons.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{planning_analysis_simple.png}
\caption{Triplet Component mAP Deterioration over Planning Horizon. The figure shows how different components (Instrument, Verb, Target) and their combinations degrade as planning horizon increases. Key insights show Overall IVT mAP drops from 33.6\% at 1s to 29.2\% at 10s, with Target being the most robust component (23.7\% loss). Stars indicate statistical significance regions.}
\label{fig:planning_analysis}
\end{figure}

\subsection{Qualitative Analysis}

Figure~\ref{fig:qualitative_examples} presents qualitative examples from our IL baseline, showing both recognition (past) and planning (future) performance on surgical video sequences. The visualizations demonstrate the model's ability to correctly identify current actions while maintaining reasonable planning accuracy for short-term future actions.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{VID14_preds_sample_1.png}
\caption{Qualitative evaluation showing recognition and planning performance from two representative video sequences. Top: VID79 sample showing recognition and planning transitions. Bottom: VID51 sample demonstrating action sequence planning. Left panels show past recognition performance, right panels show future planning predictions. The model demonstrates strong current action recognition with graceful degradation in planning accuracy over time. Green indicates true positives, blue shows false positives, and beige represents false negatives.}
\label{fig:qualitative_examples}
\end{figure}

\subsection{Why RL Underperformed}

Our analysis reveals several key factors explaining why RL approaches failed to improve upon IL:

\begin{enumerate}
\item \textbf{Expert-Optimal Training Data}: The CholecT50 dataset contains expert-level demonstrations that are already near-optimal for the evaluation metrics.
\item \textbf{Evaluation Metric Alignment}: The test set evaluation directly rewards behavior similar to the training demonstrations.
\item \textbf{Limited Exploration Benefits}: RL exploration discovers valid alternative surgical approaches that are nonetheless suboptimal for the specific evaluation criteria.
\item \textbf{Domain Constraints}: Surgical domain constraints limit the potential benefits of exploration-based learning.
\item \textbf{Missing RL Components}: Our RL approaches lacked comprehensive state representation, reward signals, and expected final outcome modeling that could enable more effective policy learning.
\end{enumerate}

However, one key limitation of imitation learning on expert demonstrations from surgeries with good outcomes and non-complicated procedures is that it may overlook the trial-and-error learning from RL, which permits recovery from mistakes and unexplored events. The lack of exploration during learning limits safety capabilities when encountering novel or challenging scenarios.

These findings suggest that in domains with high-quality expert demonstrations and aligned evaluation metrics, sophisticated RL approaches may not provide benefits over well-optimized imitation learning, though this conclusion must be considered within the constraints of our experimental setup. This aligns with recent observations that expert demonstrations can significantly improve RL learning efficiency in surgical domains~\cite{liu2024surgical}.


\section{Discussion}

\subsection{When IL Excels Over RL}

Our results identify several conditions under which imitation learning outperforms reinforcement learning in surgical contexts:

\textbf{Expert-Optimal Demonstrations}: When training data represents near-optimal behavior for the evaluation criteria, RL exploration may discover valid but suboptimal alternatives. In surgical domains, expert demonstrations often represent refined techniques developed through years of training and experience.

\textbf{Evaluation Metric Alignment}: When test metrics directly reward similarity to training demonstrations, IL has a fundamental advantage. This alignment is common in medical domains where expert behavior defines the gold standard.

\textbf{Limited Exploration Benefits}: Surgical domains have strong constraints on safe and effective actions. While RL exploration can discover novel approaches, these may be valid but suboptimal for standard evaluation metrics.

\textbf{Data Sufficiency}: With sufficient expert demonstrations, IL can capture the full range of appropriate behaviors without requiring the additional complexity of RL.

\subsection{Implications for Surgical AI}

\textbf{Resource Allocation}: Our findings suggest that research resources might be better allocated to optimizing IL approaches rather than developing complex RL systems for surgical planning tasks.

\textbf{Safety Considerations}: IL approaches inherently stay closer to expert behavior, potentially offering safety advantages in clinical deployment. RL exploration, while potentially discovering novel approaches, introduces uncertainty that may be undesirable in safety-critical contexts.

\textbf{Deployment Readiness}: Simpler IL models are easier to validate, interpret, and deploy in clinical settings compared to complex RL systems with learned reward functions.

\textbf{Domain-Specific Design}: Our results suggest that surgical AI may require different methodological approaches than general-purpose AI domains where RL typically excels.

\subsection{Limitations and Future Directions}

Several limitations should be considered when interpreting our results:

\textbf{Single Dataset Evaluation}: Our results are based on the CholecT50 dataset for laparoscopic cholecystectomy. Different surgical procedures or datasets might yield different conclusions.

\textbf{Expert Test Set}: Our evaluation uses expert-level test data similar to the training distribution. Results might differ when evaluating on sub-expert data or out-of-distribution scenarios.

\textbf{Metric Alignment}: Our evaluation metrics directly reward expert-like behavior. Alternative evaluation criteria focusing on patient outcomes or novel surgical approaches might favor RL methods.

\textbf{Limited RL Implementation}: More sophisticated exploration strategies, comprehensive state representations, reward design, and outcome modeling might enable RL approaches to outperform IL. However, this remains an open research question.

\textbf{Constraining Evaluation Framework}: Our experimental setup used offline recorded videos with constraining evaluation metrics and lacked comprehensive reward and outcome data that are standard for classic RL approaches.

Future work should explore these limitations by: (1) evaluating on diverse surgical datasets and procedures, (2) developing evaluation metrics that capture surgical effectiveness beyond expert similarity, (3) investigating advanced RL techniques specifically designed for expert domains with comprehensive state-action-reward modeling, and (4) exploring physics engines, world models (neural engines), and real environment deployment for more comprehensive evaluation.


\section{Conclusion}

This work provides crucial insights for surgical AI development by demonstrating conditions under which sophisticated RL approaches do not universally improve upon well-optimized imitation learning. In surgical domains with expert demonstrations and aligned evaluation metrics, simple IL can outperform complex RL methods, though this finding must be interpreted within the constraints of our experimental framework using offline recorded videos and limited evaluation metrics.

Our findings suggest that distribution matching problems on evaluation test sets may favor IL approaches over potentially valid or superior RL policies that differ from expert demonstrations. This challenges common assumptions about ML method hierarchy and provides practical guidance for surgical AI research resource allocation.

The key insight is that expert domains with high-quality demonstrations may not always benefit from RL exploration, particularly when evaluation metrics reward expert-like behavior. However, this conclusion is based on a single dataset with constraining evaluation criteria, and lacks comprehensive reward and outcome data standard for RL approaches.

Future surgical AI development should carefully consider domain characteristics, data quality, evaluation alignment, and the potential benefits of trial-and-error learning when choosing between IL and RL approaches. While IL excels at mimicking expert behavior, RL's capacity for exploration and recovery from novel scenarios may prove valuable in more comprehensive evaluation frameworks that capture patient outcomes and surgical effectiveness beyond expert similarity.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}

% Note: For the final submission, create these reference entries:
\begin{thebibliography}{22}
\bibitem{nwoye2022cholect50}
Nwoye, C.I., et al.: Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos. Medical Image Analysis \textbf{78}, 102433 (2022)

\bibitem{nwoye2020recognition}
Nwoye, C.I., et al.: Recognition of instrument-tissue interactions in endoscopic videos via action triplets. In: MICCAI, pp. 364--374 (2020)

\bibitem{nwoye2023cholectriplet2021}
Nwoye, C.I., et al.: CholecTriplet2021: A benchmark challenge for surgical action triplet recognition. Medical Image Analysis \textbf{86}, 102803 (2023)

\bibitem{shi2022recognition}
Shi, C., et al.: Recognition and prediction of surgical gestures and trajectories using transformer models in robot-assisted surgery. IEEE Robotics and Automation Letters \textbf{7}(4), 9821--9828 (2022)

\bibitem{weerasinghe2024multimodal}
Weerasinghe, K., et al.: Multimodal transformers for real-time surgical activity prediction. arXiv preprint arXiv:2403.06705 (2024)

\bibitem{wagner2023vision}
Wagner, C., et al.: A vision transformer for decoding surgeon activity from surgical videos. npj Precision Oncology \textbf{7}, 16 (2023)

\bibitem{liu2023skit}
Liu, Y., et al.: SKiT: a fast key information video transformer for online surgical phase recognition. In: ICCV, pp. 21486--21496 (2023)

\bibitem{liu2025lovit}
Liu, Y., et al.: Lovit: Long video transformer for surgical phase recognition. Medical Image Analysis \textbf{99}, 103366 (2025)

\bibitem{boels2025swag}
Boels, M., et al.: SWAG: long-term surgical workflow prediction with generative-based anticipation. International Journal of Computer Assisted Radiology and Surgery, pp. 1--11 (2025)

\bibitem{liu2024surgical}
Liu, J., et al.: Surgical task automation using actor-critic frameworks and self-supervised imitation learning. arXiv preprint arXiv:2409.02724 (2024)

\bibitem{mnih2015human}
Mnih, V., et al.: Human-level control through deep reinforcement learning. Nature \textbf{518}(7540), 529--533 (2015)

\bibitem{levine2016end}
Levine, S., et al.: End-to-end training of deep visuomotor policies. Journal of Machine Learning Research \textbf{17}(1), 1334--1373 (2016)

\bibitem{radford2019language}
Radford, A., et al.: Language models are unsupervised multitask learners. OpenAI Blog \textbf{1}(8), 9 (2019)

\bibitem{liu2021swin}
Liu, Z., et al.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV, pp. 10012--10022 (2021)

\bibitem{yamlahi2023self}
Yamlahi, A., et al.: Self-distillation for surgical action recognition. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 637--646. Springer (2023)

\bibitem{hafner2020dream}
Hafner, D., et al.: Dream to control: Learning behaviors by latent imagination. In: ICLR (2020)

\bibitem{ziebart2008maximum}
Ziebart, B.D., et al.: Maximum entropy inverse reinforcement learning. In: AAAI, pp. 1433--1438 (2008)

\bibitem{schulman2017proximal}
Schulman, J., et al.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)

\bibitem{nwoye2022data}
Nwoye, C.I., Padoy, N.: Data splits and metrics for method benchmarking on surgical action triplet datasets. arXiv preprint arXiv:2204.05235 (2022)
\end{thebibliography}

\end{document}