\documentclass[runningheads]{llncs}
%
\usepackage[T1]{fontenc}
% T1 fonts will be used to generate the final print and online PDFs,
% so please use T1 fonts in your manuscript whenever possible.
% Other font encodings may result in incorrect characters.
%
\usepackage{graphicx,verbatim}
\usepackage{booktabs}
\usepackage{amsmath,amssymb}
% Used for displaying figures. If possible, figure files should
% be included in EPS format.
%
% If you use the hyperref package, please uncomment the following two lines
% to display URLs in blue roman font according to Springer's eBook style:
%\usepackage{color}
%\renewcommand\UrlFont{\color{blue}\rmfamily}
%\urlstyle{rm}
%
\begin{document}
%
\title{When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning}
%
\author{Anonymized Authors}
\authorrunning{Anonymized Authors et al.}
\institute{Anonymized Affiliations \\
    \email{email@anonymized.com}}

\maketitle              % typeset the header of the contribution
%
\begin{abstract}
Teleoperated robotic surgery provides a natural interface to acquire expert demonstrations for imitation learning (IL), while reinforcement learning (RL) could in principle discover new strategies and achieve beyond expert-level performance. However, the question of when to use IL versus RL in surgical domains remains largely unexplored. We conducted a comprehensive comparison of IL versus RL approaches for surgical action planning on the CholecT50 dataset. Our baseline uses supervised learning on expert demonstration videos to learn surgical behavior through direct observation. We systematically evaluated: (1) standard IL with causal prediction, (2) RL with learned rewards via inverse RL, and (3) world model-based RL with forward simulation. Our IL baseline achieves 34.6\% current action mAP and 33.6\% next action mAP with smooth planning degradation (33.6\% at 1s to 29.2\% at 10s). Surprisingly, sophisticated RL approaches failed to improve upon this baseline. We found that distribution matching on the evaluation test set favors the IL baseline over potentially valid or even better new policies that differ from expert demonstrations used for training. This challenges assumptions about method hierarchy and provides crucial insights for surgical AI development.

\keywords{Surgical Action Planning \and Imitation Learning \and Reinforcement Learning \and Surgical Action Prediction \and Surgical AI}
\end{abstract}


\section{Introduction}

Surgical action planning in teleoperated robotic surgery represents one of the most challenging applications of artificial intelligence in healthcare~\cite{nwoye2022cholect50,nwoye2020recognition}. Recent advances in surgical action triplet recognition~\cite{nwoye2023cholectriplet2021} and surgical gesture prediction~\cite{shi2022recognition,weerasinghe2024multimodal} have demonstrated the potential for AI-driven surgical assistance. Teleoperated robotic surgery provides a natural interface for acquiring expert demonstrations for imitation learning. The question of when to use imitation learning (IL) versus reinforcement learning (RL) in such safety-critical domains remains largely unexplored, despite its importance for practical deployment.

While RL has demonstrated remarkable success in games~\cite{mnih2015human} and robotics~\cite{levine2016end}, its application to surgical domains presents unique challenges. Recent work in surgical automation~\cite{liu2024surgical} has shown promise for integrating RL with imitation learning, while vision transformers have emerged as powerful tools for surgical video analysis~\cite{wagner2023vision,liu2023skit,liu2025lovit}. Additionally, advances in long-term surgical workflow prediction~\cite{boels2025swag} demonstrate the potential for anticipatory surgical planning systems. Expert surgical demonstrations represent decades of refined technique and training, potentially making them near-optimal for many evaluation criteria. This raises a fundamental question: under what conditions does RL improve upon well-optimized IL in expert domains?

This work provides the first comprehensive comparison of IL and RL approaches for surgical action planning, using the CholecT50 dataset~\cite{nwoye2022cholect50} for laparoscopic cholecystectomy. Our findings suggest that distribution matching problems on evaluation test sets may favor IL baselines over potentially valid or even superior policies that differ from expert demonstrations.

\textbf{Contributions}: (1) First systematic comparison of IL vs RL for surgical action planning, (2) Important negative result showing when RL doesn't help in expert domains, (3) Comprehensive evaluation framework for temporal surgical planning, and (4) Analysis of how expert data quality and evaluation metrics affect method selection in surgical domains.

% \begin{abstract}
% Surgical action triplet prediction has primarily focused on recognition tasks for activity analysis. However, real-time surgical assistance requires next action prediction for planning and control applications. This work presents the first comprehensive comparison of Imitation Learning (IL) versus Reinforcement Learning (RL) approaches for surgical next action prediction, evaluating both recognition accuracy and planning capability. We propose an autoregressive IL baseline achieving 0.979 mAP on CholecT50, and systematically compare it against multiple RL variants including world model-based RL, direct video RL, and inverse RL enhancement. Our analysis reveals that while IL excels at single-step prediction, RL provides scenario-specific improvements for complex surgical situations requiring multi-step reasoning. These findings inform optimal AI approach selection for clinical surgical assistance applications.

% \keywords{Surgical AI \and Action Prediction \and Imitation Learning \and Reinforcement Learning \and Planning}
% \end{abstract}

% \section{Introduction}

% Surgical action triplet prediction—the task of identifying instrument-verb-target relationships in surgical videos—has emerged as a fundamental component of computer-assisted surgery systems. While prior work has predominantly focused on recognition tasks for retrospective analysis~\cite{ref_cholect50,ref_rendezvous}, real-time surgical assistance requires prospective prediction capabilities for planning and control applications.

% The challenge of surgical next action prediction presents unique constraints compared to recognition: (1) single-step inference latency requirements for real-time response, (2) the need for multi-horizon planning in complex scenarios, and (3) safety-critical decision making under uncertainty. These requirements raise fundamental questions about the optimal learning paradigm: should surgical AI systems learn through imitation of expert demonstrations (IL) or through trial-and-error optimization of reward functions (RL)?

% This work presents the first systematic comparison of IL versus RL approaches for surgical action triplet prediction, with emphasis on next action prediction for planning applications. Our contributions include:

% \begin{enumerate}
% \item An autoregressive IL baseline achieving state-of-the-art performance (0.979 mAP) on CholecT50 next action prediction
% \item Comprehensive evaluation of multiple RL variants: world model-based RL, direct video RL, and inverse RL enhancement
% \item Novel evaluation framework comparing recognition accuracy versus planning capability across different prediction horizons  
% \item Clinical insights on when RL provides advantages over IL for surgical assistance applications
% \end{enumerate}

\section{Methods}

\subsection{Dataset}

We evaluate our approaches on the CholecT50 dataset~\cite{nwoye2022cholect50}, which contains 50 laparoscopic cholecystectomy videos with expert-level surgical demonstrations. The dataset provides frame-level annotations for surgical action triplets, consisting of 100 distinct triplet classes combining 6 instruments, 10 verbs, and 15 targets. Videos are sampled at 1 frame per second, with the training set (40 videos) containing 78,968 frames and the test set (10 videos) containing 21,895 frames, totaling approximately 100,863 annotated frames. The dataset represents high-quality expert demonstrations from experienced surgeons performing full procedures.

\subsection{Problem Formulation}

We formulate surgical action triplet prediction as a sequential decision making problem. Given a sequence of surgical video frames $\{f_1, f_2, ..., f_t\}$, the task is to predict future action triplets $\{a_{t+1}, a_{t+2}, ..., a_{t+H}\}$ where $H$ represents the prediction horizon.

Each action triplet $a_i = (I_i, V_i, T_i)$ consists of an instrument $I_i \in \mathcal{I}$, verb $V_i \in \mathcal{V}$, and target $T_i \in \mathcal{T}$ from predefined vocabularies. We evaluate both single-step prediction ($H=1$) for recognition comparison and multi-step prediction ($H>1$) for planning assessment.

\subsection{Autoregressive Imitation Learning Baseline}

Our IL approach models surgical action prediction as a causal sequence generation problem. The architecture combines frame-level feature extraction with autoregressive action generation:

\begin{equation}
p(a_{t+1}|f_1, ..., f_t, a_1, ..., a_t) = \text{GPT-2}(\text{FrameEmb}(f_1, ..., f_t), a_1, ..., a_t)
\end{equation}

The model consists of three components:
\begin{enumerate}
\item \textbf{Frame Processing}: Pre-trained visual features are processed through learned embeddings to create temporal representations
\item \textbf{GPT-2 Backbone}: A transformer decoder models causal dependencies between frames and actions
\item \textbf{Action Prediction}: Separate heads predict instrument, verb, and target components with IVT-based optimization
\end{enumerate}

Training optimizes the standard imitation learning objective:
\begin{equation}
\mathcal{L}_{IL} = -\sum_{t=1}^{T} \log p(a_t|f_{1:t}, a_{1:t-1}; \theta)
\end{equation}

\subsection{Reinforcement Learning Approaches}

We evaluate three RL variants for surgical action prediction:

\subsubsection{World Model-Based RL}
This approach learns a conditional world model predicting future states given current state and action:
\begin{equation}
p(s_{t+1}, r_t|s_t, a_t) = \text{WorldModel}(s_t, a_t; \phi)
\end{equation}

The world model enables planning through rollouts, with policy optimization using PPO on the learned dynamics. Rewards are designed to match expert demonstrations and encourage task-relevant behavior.

\subsubsection{Direct Video RL}
This model-free approach learns policies directly from video observations without explicit world models. The environment provides frame sequences as states, with actions representing predicted triplets. Rewards incorporate both demonstration matching and task-specific objectives.

\subsubsection{Inverse RL Enhancement}
Building on the IL baseline, this approach learns reward functions from expert trajectories using Maximum Entropy IRL, then applies lightweight policy improvement through GAIL. This enables scenario-specific enhancements while maintaining IL performance for routine cases.

\subsection{Evaluation Framework}

We introduce a dual evaluation protocol comparing recognition accuracy and planning capability:

\textbf{Recognition Evaluation}: Standard mAP computation on single-step predictions, comparing with existing CholecT50 benchmarks.

\textbf{Planning Evaluation}: Multi-horizon prediction assessment measuring:
\begin{itemize}
\item Temporal consistency across prediction horizons (1, 3, 5, 10 steps)
\item Trajectory-level planning accuracy using mAP degradation analysis
\item Scenario-specific performance on complex vs. routine surgical phases
\end{itemize}


\subsection{Baseline: Optimized Imitation Learning}

Our IL baseline uses an autoregressive transformer architecture with dual-path training for both current action recognition and next action prediction. The model combines a BiLSTM for temporal current action recognition with a GPT-2 backbone~\cite{radford2019language} for causal next action prediction. We refer to this approach as Dual-task Autoregressive Imitation Learning (DARIL).

\textbf{Architecture}: The model processes 1024-dimensional Swin transformer features~\cite{liu2021swin} extracted from surgical video frames using distilled feature extraction~\cite{yamlahi2023self}. A BiLSTM encoder captures temporal patterns for current action recognition, while a GPT-2 decoder generates future action sequences autoregressively.

\textbf{Training}: We employ dual-task learning with separate loss functions for current action recognition and next action prediction, enabling the model to excel at both real-time recognition and planning tasks.

\subsection{RL Approaches Evaluated}

\textbf{Latent World Model + RL}: We develop an action-conditioned world model that predicts future states and rewards given current states and actions, following the paradigm established by Dreamer~\cite{hafner2020dream}. The goal is to create a latent simulator that enables policy learning in the learned environment. PPO~\cite{schulman2017proximal} is trained in this simulated latent environment for action planning.

\textbf{Direct RL on Videos}: We apply model-free RL directly to video sequences using expert demonstration matching rewards. Multiple algorithms (PPO, A2C) are evaluated with careful hyperparameter optimization.

\textbf{Inverse RL with Learned Rewards}: We implement Maximum Entropy IRL~\cite{ziebart2008maximum} with sophisticated negative generation to learn reward functions from expert demonstrations. Negative examples are generated by sampling actions that deviate from expert demonstrations, and the learned reward function is used to guide policy optimization during training by providing dense feedback signals that encourage expert-like behavior while penalizing deviations.

\subsection{Evaluation Framework}
1965
\textbf{Temporal Planning Evaluation}: We evaluate planning performance across multiple horizons (1s, 2s, 3s, 5s, 10s, 20s) using mean Average Precision (mAP) computed with the IVT metrics~\cite{nwoye2022data,nwoye2022cholect50}.

\textbf{Component-wise Analysis}: We analyze performance for individual components (Instrument, Verb, Target) and their combinations (IV, IT, IVT) to understand degradation patterns.

\textbf{Statistical Validation}: Cross-video evaluation with statistical significance testing ensures robust conclusions. We perform bootstrap sampling across video splits and apply paired t-tests to assess statistical significance of performance differences between methods.


\section{Results}

\subsection{Main Comparative Results}

Table~\ref{tab:main_results} presents our main experimental findings comparing IL and RL approaches for surgical action planning. Our DARIL baseline achieves 34.6\% current action mAP and 33.6\% next action mAP, demonstrating strong performance on expert demonstration learning. Figure~\ref{fig:method_comparison} provides a comprehensive view of how each method performs across different planning horizons, highlighting the consistent superiority of our DARIL approach.

\begin{table}[h]
\centering
\caption{Comparative Results: IL vs RL Approaches for Surgical Action Planning. All values are IVT mAP scores in \%. mAP$_{t=0}$ refers to current action recognition, mAP$_{t+1s}$ to 1-second prediction, etc.}
\label{tab:main_results}
\begin{tabular}{lcccc}
\toprule
\textbf{Method} & \textbf{mAP$_{t=0}$} & \textbf{mAP$_{t+1s}$} & \textbf{mAP$_{t+5s}$} & \textbf{mAP$_{t+10s}$} \\
\midrule
DARIL (Ours) & \textbf{34.6} & \textbf{33.6} & \textbf{31.2} & \textbf{29.2} \\
\midrule
DARIL + Direct Video RL & \textit{33.2} & \textit{22.6} & \textit{19.3} & \textit{15.9} \\
DARIL + IRL & \textit{33.1} & \textit{32.1} & \textit{29.6} & \textit{28.1} \\
Latent World Model + RL & \textit{33.1} & \textit{14.0} & \textit{9.1} & \textit{3.1} \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{method_comparison_planning.png}
\caption{Planning Performance Comparison Across Methods and Time Horizons. The figure shows mAP performance for each method (DARIL, DARIL+IRL, Latent World Model+RL, Direct Video RL) across different planning horizons (1s, 2s, 3s, 5s, 10s, 20s). DARIL demonstrates superior performance across all time horizons, with RL approaches showing varying degrees of performance degradation. Error bars indicate 95\% confidence intervals across video splits.}
\label{fig:method_comparison}
\end{figure}

\subsection{Component-wise Analysis}

Table~\ref{tab:component_analysis} provides detailed component-wise analysis of our DARIL baseline. The Instrument component shows the highest stability with 91.4\% current recognition declining to 88.2\% for next prediction. The Target component shows more variability, with 52.7\% current recognition and 52.5\% next prediction performance.

\begin{table}[h]
\centering
\caption{Component-wise Performance Analysis of DARIL Baseline. All values are mAP scores in \% for each component and combination.}
\label{tab:component_analysis}
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{mAP$_{t=0}$} & \textbf{mAP$_{t+1s}$} \\
\midrule
Instrument (I) & 91.4 & 88.2 \\
Verb (V) & 69.4 & 68.1 \\
Target (T) & 52.7 & 52.5 \\
\midrule
Instrument-Verb (IV) & 42.9 & 38.8 \\
Instrument-Target (IT) & 43.5 & 43.6 \\
Instrument-Verb-Target (IVT) & 34.6 & 33.6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Planning Performance Analysis}

Figure~\ref{fig:planning_analysis} shows the temporal planning performance across different horizons. Our DARIL baseline demonstrates smooth degradation from 33.6\% mAP at 1-second planning to 29.2\% at 10-second planning, representing a 13.1\% relative decrease. The planning degradation pattern reveals that longer-term predictions become increasingly challenging, with performance dropping to 23.3\% at 20-second horizons.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{planning_analysis_simple.png}
\caption{Triplet Component mAP Deterioration over Planning Horizon. The figure shows how different components (Instrument, Verb, Target) and their combinations degrade as planning horizon increases. Key insights show Overall IVT mAP drops from 33.6\% at 1s to 29.2\% at 10s, with Target being the most robust component (23.7\% loss). Stars indicate statistical significance regions.}
\label{fig:planning_analysis}
\end{figure}

\subsection{Qualitative Analysis}

Figure~\ref{fig:qualitative_examples} presents qualitative examples from our DARIL baseline, showing both recognition (past) and planning (future) performance on surgical video sequences. The visualizations demonstrate the model's ability to correctly identify current actions while maintaining reasonable planning accuracy for short-term future actions.

\begin{figure}[h]
\centering
\includegraphics[width=\textwidth]{VID14_preds_sample_1.png}
\caption{Qualitative evaluation showing recognition and planning performance from a representative video sequence. VID51 sample demonstrating action sequence planning. Left panels show past recognition performance, right panels show future planning predictions. The model demonstrates strong current action recognition with smooth degradation in planning accuracy over time. Green indicates true positives, blue shows false positives, and beige represents false negatives.}
\label{fig:qualitative_examples}
\end{figure}

\subsection{Why RL Underperformed}

Our analysis reveals several key factors explaining why RL approaches failed to improve upon IL:

\begin{enumerate}
\item \textbf{Expert-Optimal Training Data}: The CholecT50 dataset contains expert-level demonstrations that are already near-optimal for the evaluation metrics.
\item \textbf{Evaluation Metric Alignment}: The test set evaluation directly rewards behavior similar to the training demonstrations.
\item \textbf{Limited Exploration Benefits}: RL exploration discovers valid alternative surgical approaches that are nonetheless suboptimal for the specific evaluation criteria.
\item \textbf{Domain Constraints}: Surgical domain constraints limit the potential benefits of exploration-based learning.
\item \textbf{Missing RL Components}: Our RL approaches lacked comprehensive state representation, reward signals, and expected final outcome modeling that could enable more effective policy learning.
\end{enumerate}

However, one key limitation of imitation learning on expert demonstrations from surgeries with good outcomes and non-complicated procedures is that it may overlook the trial-and-error learning from RL, which permits recovery from mistakes and unexplored events. The lack of exploration during learning limits safety capabilities when encountering novel or challenging scenarios.

These findings suggest that in domains with high-quality expert demonstrations and aligned evaluation metrics, sophisticated RL approaches may not provide benefits over well-optimized imitation learning, though this conclusion must be considered within the constraints of our experimental setup. This aligns with recent observations that expert demonstrations can significantly improve RL learning efficiency in surgical domains~\cite{liu2024surgical}.


\section{Discussion}

\subsection{When IL Excels Over RL}

Our results identify several conditions under which imitation learning outperforms reinforcement learning in surgical contexts:

\textbf{Expert-Optimal Demonstrations}: When training data represents near-optimal behavior for the evaluation criteria, RL exploration may discover valid but suboptimal alternatives. In surgical domains, expert demonstrations often represent refined techniques developed through years of training and experience.

\textbf{Evaluation Metric Alignment}: When test metrics directly reward similarity to training demonstrations, IL has a fundamental advantage. This alignment is common in medical domains where expert behavior defines the gold standard.

\textbf{Limited Exploration Benefits}: Surgical domains have strong constraints on safe and effective actions. While RL exploration can discover novel approaches, these may be valid but suboptimal for standard evaluation metrics.

\textbf{Data Sufficiency}: With sufficient expert demonstrations, IL can capture the full range of appropriate behaviors without requiring the additional complexity of RL.

\subsection{Implications for Surgical AI}

\textbf{Resource Allocation}: Our findings suggest that research resources might be better allocated to optimizing IL approaches rather than developing complex RL systems for surgical planning tasks.

\textbf{Safety Considerations}: IL approaches inherently stay closer to expert behavior, potentially offering safety advantages in clinical deployment. RL exploration, while potentially discovering novel approaches, introduces uncertainty that may be undesirable in safety-critical contexts.

\textbf{Deployment Readiness}: Simpler IL models are easier to validate, interpret, and deploy in clinical settings compared to complex RL systems with learned reward functions.

\textbf{Domain-Specific Design}: Our results suggest that surgical AI may require different methodological approaches than general-purpose AI domains where RL typically excels.

\subsection{Limitations and Future Directions}

Several limitations should be considered when interpreting our results:

\textbf{Single Dataset Evaluation}: Our results are based on the CholecT50 dataset for laparoscopic cholecystectomy. Different surgical procedures or datasets might yield different conclusions.

\textbf{Expert Test Set}: Our evaluation uses expert-level test data similar to the training distribution. Results might differ when evaluating on sub-expert data or out-of-distribution scenarios.

\textbf{Metric Alignment}: Our evaluation metrics directly reward expert-like behavior. Alternative evaluation criteria focusing on patient outcomes or novel surgical approaches might favor RL methods.

\textbf{Limited RL Implementation}: More sophisticated exploration strategies, comprehensive state representations, reward design, and outcome modeling might enable RL approaches to outperform IL. However, this remains an open research question.

\textbf{Constraining Evaluation Framework}: Our experimental setup used offline recorded videos with constraining evaluation metrics and lacked comprehensive reward and outcome data that are standard for classic RL approaches.

Future work should explore these limitations by: (1) evaluating on diverse surgical datasets and procedures, (2) developing evaluation metrics that capture surgical effectiveness beyond expert similarity, (3) investigating advanced RL techniques specifically designed for expert domains with comprehensive state-action-reward modeling, and (4) exploring physics engines, world models (neural engines), and real environment deployment for more comprehensive evaluation.


\section{Conclusion}

This work provides crucial insights for surgical AI development by demonstrating conditions under which sophisticated RL approaches do not universally improve upon well-optimized imitation learning. In surgical domains with expert demonstrations and aligned evaluation metrics, simple IL can outperform complex RL methods, though this finding must be interpreted within the constraints of our experimental framework using offline recorded videos and limited evaluation metrics.

Our findings suggest that distribution matching problems on evaluation test sets may favor IL approaches over potentially valid or superior RL policies that differ from expert demonstrations. This challenges common assumptions about ML method hierarchy and provides practical guidance for surgical AI research resource allocation.

The key insight is that expert domains with high-quality demonstrations may not always benefit from RL exploration, particularly when evaluation metrics reward expert-like behavior. However, this conclusion is based on a single dataset with constraining evaluation criteria, and lacks comprehensive reward and outcome data standard for RL approaches.

Future surgical AI development should carefully consider domain characteristics, data quality, evaluation alignment, and the potential benefits of trial-and-error learning when choosing between IL and RL approaches. While IL excels at mimicking expert behavior, RL's capacity for exploration and recovery from novel scenarios may prove valuable in more comprehensive evaluation frameworks that capture patient outcomes and surgical effectiveness beyond expert similarity.

%
% ---- Bibliography ----
%
% BibTeX users should specify bibliography style 'splncs04'.
% References will then be sorted and formatted in the correct style.
%
\bibliographystyle{splncs04}
\bibliography{references}

% Note: For the final submission, create these reference entries:
\begin{thebibliography}{22}
\bibitem{nwoye2022cholect50}
Nwoye, C.I., et al.: Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos. Medical Image Analysis \textbf{78}, 102433 (2022)

\bibitem{nwoye2020recognition}
Nwoye, C.I., et al.: Recognition of instrument-tissue interactions in endoscopic videos via action triplets. In: MICCAI, pp. 364--374 (2020)

\bibitem{nwoye2023cholectriplet2021}
Nwoye, C.I., et al.: CholecTriplet2021: A benchmark challenge for surgical action triplet recognition. Medical Image Analysis \textbf{86}, 102803 (2023)

\bibitem{shi2022recognition}
Shi, C., et al.: Recognition and prediction of surgical gestures and trajectories using transformer models in robot-assisted surgery. IEEE Robotics and Automation Letters \textbf{7}(4), 9821--9828 (2022)

\bibitem{weerasinghe2024multimodal}
Weerasinghe, K., et al.: Multimodal transformers for real-time surgical activity prediction. arXiv preprint arXiv:2403.06705 (2024)

\bibitem{wagner2023vision}
Wagner, C., et al.: A vision transformer for decoding surgeon activity from surgical videos. npj Precision Oncology \textbf{7}, 16 (2023)

\bibitem{liu2023skit}
Liu, Y., et al.: SKiT: a fast key information video transformer for online surgical phase recognition. In: ICCV, pp. 21486--21496 (2023)

\bibitem{liu2025lovit}
Liu, Y., et al.: Lovit: Long video transformer for surgical phase recognition. Medical Image Analysis \textbf{99}, 103366 (2025)

\bibitem{boels2025swag}
Boels, M., et al.: SWAG: long-term surgical workflow prediction with generative-based anticipation. International Journal of Computer Assisted Radiology and Surgery, pp. 1--11 (2025)

\bibitem{liu2024surgical}
Liu, J., et al.: Surgical task automation using actor-critic frameworks and self-supervised imitation learning. arXiv preprint arXiv:2409.02724 (2024)

\bibitem{mnih2015human}
Mnih, V., et al.: Human-level control through deep reinforcement learning. Nature \textbf{518}(7540), 529--533 (2015)

\bibitem{levine2016end}
Levine, S., et al.: End-to-end training of deep visuomotor policies. Journal of Machine Learning Research \textbf{17}(1), 1334--1373 (2016)

\bibitem{radford2019language}
Radford, A., et al.: Language models are unsupervised multitask learners. OpenAI Blog \textbf{1}(8), 9 (2019)

\bibitem{liu2021swin}
Liu, Z., et al.: Swin transformer: Hierarchical vision transformer using shifted windows. In: ICCV, pp. 10012--10022 (2021)

\bibitem{yamlahi2023self}
Yamlahi, A., et al.: Self-distillation for surgical action recognition. In: International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 637--646. Springer (2023)

\bibitem{hafner2020dream}
Hafner, D., et al.: Dream to control: Learning behaviors by latent imagination. In: ICLR (2020)

\bibitem{ziebart2008maximum}
Ziebart, B.D., et al.: Maximum entropy inverse reinforcement learning. In: AAAI, pp. 1433--1438 (2008)

\bibitem{schulman2017proximal}
Schulman, J., et al.: Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 (2017)

\bibitem{nwoye2022data}
Nwoye, C.I., Padoy, N.: Data splits and metrics for method benchmarking on surgical action triplet datasets. arXiv preprint arXiv:2204.05235 (2022)
\end{thebibliography}

\end{document}