# ===================================================================
# Fixed Configuration for Complete IL vs RL Comparison
# Enables both supervised imitation learning and RL training
# ===================================================================

# Debug mode
debug: false

# Training mode: 'supervised', 'rl', or 'mixed'
training_mode: 'rl'  # Train both IL and RL for comparison

# Data preprocessing
preprocess:
  extract_rewards: false  # Set to true if rewards need to be extracted from the dataset
  rewards:
    imitation:
      action_distribution: true
    expert_knowledge:
      risk_score: true
      frame_risk_agg: 'max'
    grounded:
      phase_progression: true
      global_progression: true
      phase_completion: true
      phase_transition: true
  value:
    global_outcome: true

# Experiment configuration
experiment:
  max_videos: 2  # to be removed later (use from train or test)
  train:
    max_videos: 2  # Increase for full training
  test:
    max_videos: 1   # Test on more videos
  
  # Dual World Model Training
  dual_world_model:
    train: true
    best_model_path: null  # Will be set after training
  
  # Imitation Learning (ENABLED for IL vs RL comparison)
  il_experiments:
    enabled: true  # ✅ ENABLED for IL vs RL comparison
    il_model_path: "logs/2025-06-02_14-07-20/checkpoints/supervised_best_epoch_1.pt"  # Path to the best IL model
  
  # RL Experiments (ENABLED for comparison)
  rl_experiments:
    enabled: true  # ✅ ENABLED for IL vs RL comparison
    eval_episodes: 20
  
  # Legacy settings (keep for compatibility)
  recognition:
    train: false
    inference: false
  world_model:
    train: false
    inference: false

# Training parameters
training:
  epochs: 1  # Reduced for faster iteration
  batch_size: 16
  learning_rate: 0.0001  # Start with lower LR for stability
  log_every_n_steps: 100
  
  # Learning rate scheduling
  scheduler:
    type: "cosine"  # or "linear_warmup"
    warmup_steps: 1000
    
  # Regularization
  weight_decay: 0.01
  gradient_clip_val: 1.0
  dropout: 0.1
  
  # Logging and checkpointing
  num_workers: 4
  pin_memory: true
  log_every_n_steps: 100
  log_dir: "logs"
  checkpoint_dir: "checkpoints"
  eval_epoch_interval: 1  # Evaluate more frequently
  save_model: true

# Add these sections to enable fair evaluation
fair_evaluation:
  enabled: true
  include_traditional_metrics: true    # Keep your existing metrics
  include_clinical_metrics: true       # Add clinical outcome metrics
  
  clinical_outcome_weights:
    phase_progression: 2.0
    innovation: 0.5

# RL Training Configuration (UPDATED)
rl_training:
  outcome_based_rewards: true          # Use clinical rewards instead of action mimicry
  # Environment settings
  rl_horizon: 50
  reward_mode: 'dense'  # 'dense' or 'sparse'
  normalize_rewards: true
  early_termination: true
  timesteps: 10000  # Default timesteps for RL training
  
  # Reward weights for environment
  reward_weights:
    phase_completion: 1.0
    risk_penalty: -0.5
  
  # Algorithm-specific settings
  ppo:
    learning_rate: 0.0003
    value_coef: 0.5
    
  sac:
    learning_rate: 0.0003
    alpha: 0.2

# Data configuration
data:
  context_length: 20
  train_shift: 1
  padding_value: 0.0
  max_horizon: 15
  

  paths:
    data_dir: "/home/maxboels/datasets/CholecT50"
    class_labels_file_path: "./data/labels.json"
    fold: 0
    metadata_file: "embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv"
    video_global_outcome_file: "embeddings_f0_swin_bas_129_with_enhanced_global_metrics.csv"
  
  frame_risk_agg: 'max'

# Model configurations (ADDED MISSING SECTION)
models:
  # Method 1: Autoregressive IL Model
  autoregressive_il:
    hidden_dim: 768
    embedding_dim: 1024
    n_layer: 6
    num_action_classes: 100
    num_phase_classes: 7
    dropout: 0.1
    max_length: 1024
  
  # Method 2: Conditional World Model  
  conditional_world_model:
    hidden_dim: 768
    embedding_dim: 1024
    action_embedding_dim: 128
    n_layer: 6
    num_action_classes: 100
    num_phase_classes: 7
    dropout: 0.1
    max_sequence_length: 512

# Evaluation configuration
evaluation:
  prediction_horizon: 15  # Added missing key
  
  # Supervised evaluation
  supervised:
    action_prediction: true
    
  # RL evaluation
  rl:
    rollout_horizon: 15
    use_best_actions: true
  
  # Comparison metrics
  comparison:
    statistical_tests: true
    effect_size_threshold: 0.2  # Cohen's d
    
  # General evaluation
  world_model:
    use_memory: false
    overall_horizon: 1

# Supervised Learning Configuration
supervised_learning:
  # Data augmentation for better generalization
  data_augmentation: true

# Research comparison settings
research_comparison:
  # Methods to compare
  methods: ['autoregressive_il', 'conditional_world_model', 'direct_video_rl']

# Advanced configurations
advanced:
  # Memory and optimization
  mixed_precision: false

# Hardware optimization
hardware:
  # GPU settings
  persistent_workers: true

# Specific configurations for different training phases
training_phases:
  # Phase 1: Supervised pre-training
  supervised_pretraining: true

# Results and visualization
results:
  save_predictions: true