# FIXED DEBUG CONFIGURATION
# Use this config for debugging RL training issues

debug: true

# Training mode
training_mode: 'rl'

# Data preprocessing - DISABLED for faster debugging
preprocess:
  extract_rewards: false  # Keep false for debugging
  rewards:
    imitation:
      action_distribution: true
    expert_knowledge:
      risk_score: true
      frame_risk_agg: 'max'
    grounded:
      phase_progression: true
      global_progression: true
      phase_completion: true
      phase_transition: true

# Experiment configuration - REDUCED for debugging
experiment:
  max_videos: 2  # Keep small for debugging
  train:
    max_videos: 2  # Only 2 videos for debugging
  test:
    max_videos: 1  # Only 1 test video
  
  # IL experiments
  il_experiments:
    enabled: true
    il_model_path: null  # Will train new model or use existing
  
  # RL experiments - ENABLED with debug settings
  rl_experiments:
    enabled: true
    run_after_supervised: true
    algorithms: ['ppo', 'dqn']  # Test both
    timesteps: 5000  # REDUCED for debugging (was 10000)
    eval_episodes: 5  # REDUCED for debugging

# Training parameters - REDUCED for debugging
training:
  epochs: 1  # Keep minimal
  batch_size: 8  # REDUCED for debugging
  learning_rate: 0.0001
  log_every_n_steps: 50  # More frequent logging
  
  scheduler:
    type: "cosine"
    warmup_steps: 100  # REDUCED
    
  weight_decay: 0.01
  gradient_clip_val: 1.0
  dropout: 0.1
  
  num_workers: 2  # REDUCED for debugging
  pin_memory: true
  log_dir: "debug_logs"
  checkpoint_dir: "debug_checkpoints"
  eval_epoch_interval: 1
  save_model: true

# RL Training Configuration - OPTIMIZED for debugging
rl_training:
  outcome_based_rewards: true
  rl_horizon: 30  # REDUCED from 50 for faster episodes
  reward_mode: 'dense'
  normalize_rewards: false  # Disable for debugging
  early_termination: false  # Disable for debugging
  
  # Reward weights - SIMPLIFIED
  reward_weights:
    phase_completion: 1.0
    phase_initiation: 0.5
    phase_progression: 1.0
    global_progression: 0.8
    action_probability: 0.3
    risk_penalty: -0.5
  
  # PPO settings - OPTIMIZED for debugging
  ppo:
    learning_rate: 0.0003
    n_steps: 128  # REDUCED from 2048
    batch_size: 32  # REDUCED from 64
    n_epochs: 4   # REDUCED from 10
    gamma: 0.99
    gae_lambda: 0.95
    clip_range: 0.2
    entropy_coef: 0.01
    value_coef: 0.5
    
  # DQN settings - OPTIMIZED for debugging
  dqn:
    learning_rate: 0.0003
    buffer_size: 5000  # REDUCED from 100000
    learning_starts: 200  # REDUCED from 1000
    batch_size: 32  # REDUCED from 256
    tau: 1.0
    gamma: 0.99
    exploration_fraction: 0.2
    exploration_initial_eps: 1.0
    exploration_final_eps: 0.1

# Evaluation configuration - SIMPLIFIED
evaluation:
  supervised:
    action_prediction:
      top_ks: [1, 3, 5]  # REDUCED
      horizons: [1, 3, 5]  # REDUCED
      temperature: 1.0
      nucleus_p: 0.9
    
  rl:
    rollout_horizon: 10  # REDUCED from 15
    eval_horizons: [1, 3, 5]  # REDUCED
    eval_episodes: 5  # REDUCED from 20
    use_best_actions: true
  
  comparison:
    statistical_tests: false  # DISABLED for debugging
    significance_level: 0.05
    effect_size_threshold: 0.2
    
  world_model:
    use_memory: false
    max_horizon: 10  # REDUCED from 15
    overall_horizon: 1

# Data configuration - SIMPLIFIED
data:
  context_length: 10  # REDUCED from 20
  train_shift: 1
  padding_value: 0.0
  max_horizon: 10  # REDUCED from 15
  
  paths:
    data_dir: "/home/maxboels/datasets/CholecT50"
    class_labels_file_path: "./data/labels.json"
    fold: 0
    metadata_file: "embeddings_f0_swin_bas_129_phase_complet_phase_transit_prog_prob_action_risk_glob_outcome.csv"
    video_global_outcome_file: "embeddings_f0_swin_bas_129_with_enhanced_global_metrics.csv"
  
  risk_score_path: null
  max_videos: 2
  frame_risk_agg: 'max'

# Model configurations - SIMPLIFIED
models:
  dual_world_model:
    hidden_dim: 512  # REDUCED from 768
    embedding_dim: 1024
    action_embedding_dim: 64  # REDUCED from 128
    n_layer: 4  # REDUCED from 6
    max_length: 512  # REDUCED from 1024
    dropout: 0.1
    
    enable_autoregressive_prediction: true
    enable_rl_prediction: true
    enable_reward_prediction: true
    
    num_action_classes: 100
    num_phase_classes: 7
    
    loss_weights:
      state: 1.0
      action: 1.0
      reward: 0.5
      phase: 0.3

# Hardware optimization - CONSERVATIVE for debugging
hardware:
  gpu_memory_fraction: 0.7  # REDUCED for stability
  allow_growth: true
  num_threads: 2  # REDUCED
  prefetch_factor: 1  # REDUCED
  persistent_workers: false  # DISABLED for debugging

# Advanced configurations - SIMPLIFIED
advanced:
  mixed_precision: false  # DISABLED for debugging
  gradient_accumulation_steps: 1
  find_unused_parameters: false
  distributed: false
  world_size: 1
  rank: 0
  seed: 42
  deterministic: false
  
  wandb:
    enabled: false  # DISABLED for debugging
  
  analysis:
    save_attention_maps: false
    save_hidden_states: false
    analyze_action_patterns: false  # DISABLED for debugging
    generate_trajectory_videos: false

# Results and visualization - MINIMAL
results:
  save_predictions: true
  create_visualizations: false  # DISABLED for debugging
  generate_report: true
  latex_tables: false  # DISABLED for debugging