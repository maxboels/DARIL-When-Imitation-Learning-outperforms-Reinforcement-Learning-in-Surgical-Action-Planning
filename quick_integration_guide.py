#!/usr/bin/env python3
"""
QUICK INTEGRATION GUIDE for Optimized RL
Replace your existing RL training with this optimized version

DEADLINE: 3 days to MICCAI workshop
GOAL: Achieve 15-30% mAP (vs current 6%)

QUICK STEPS:
1. Save the optimized_rl_env.py and optimized_rl_trainer.py files
2. Replace the _run_method2_wm_rl function in your run_experiment_v7.py
3. Run the experiment with optimized RL

Expected improvement: 6% â†’ 15-30% mAP
"""

import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any

# Import your existing components
from models.conditional_world_model import ConditionalWorldModel
from datasets.world_model_dataset import create_world_model_dataloaders

# Import the new optimized components
from optimized_rl_trainer import OptimizedRLTrainer


def run_optimized_method2_wm_rl(config: Dict, logger, train_data: List[Dict], test_data: List[Dict], device: str = 'cuda') -> Dict[str, Any]:
    """
    OPTIMIZED Method 2: Conditional World Model + Expert-Focused RL
    
    This replaces your existing _run_method2_wm_rl function with the optimized version.
    Expected to achieve 15-30% mAP vs previous 6%.
    """
    
    logger.info("ğŸ¯ OPTIMIZED Method 2: Conditional World Model + Expert-Focused RL")
    logger.info("-" * 70)
    
    try:
        # Check if pretrained world model is configured
        wm_config = config.get('experiment', {}).get('world_model', {})
        wm_model_path = wm_config.get('wm_model_path', None)
        
        # Determine if we should skip world model training
        skip_wm_training = wm_model_path and Path(wm_model_path).exists()
        
        if skip_wm_training:
            logger.info(f"ğŸ“‚ Using pretrained world model from: {wm_model_path}")
            world_model = ConditionalWorldModel.load_model(wm_model_path, device=device)
            logger.info("âœ… Pretrained world model loaded successfully")
            train_data_for_loader = None  # Skip world model training
        else:
            logger.info("ğŸ‹ï¸ Will train world model from scratch")
            world_model = ConditionalWorldModel(
                hidden_dim=config['models']['conditional_world_model']['hidden_dim'],
                embedding_dim=config['models']['conditional_world_model']['embedding_dim'],
                action_embedding_dim=config['models']['conditional_world_model']['action_embedding_dim'],
                n_layer=config['models']['conditional_world_model']['n_layer'],
                num_action_classes=config['models']['conditional_world_model']['num_action_classes'],
                dropout=config['models']['conditional_world_model']['dropout']
            ).to(device)
            train_data_for_loader = train_data
        
        # Create datasets for world model training/evaluation
        train_loader, test_loaders, simulation_loader = create_world_model_dataloaders(
            config=config['data'],
            train_data=train_data_for_loader,
            test_data=test_data,
            batch_size=config['training']['batch_size'],
            num_workers=config['training']['num_workers']
        )
        
        # Step 1: Train or load world model
        if not skip_wm_training:
            logger.info("ğŸŒ Training world model from scratch...")
            from training.world_model_trainer import WorldModelTrainer
            
            world_model_trainer = WorldModelTrainer(
                model=world_model,
                config=config,
                logger=logger,
                device=device
            )
            
            best_world_model_path = world_model_trainer.train(train_loader, test_loaders)
            world_model_evaluation = world_model_trainer.evaluate_model(test_loaders)
        else:
            logger.info("ğŸ“Š Evaluating pretrained world model...")
            from training.world_model_trainer import WorldModelTrainer
            
            world_model_trainer = WorldModelTrainer(
                model=world_model,
                config=config,
                logger=logger,
                device=device
            )
            world_model_evaluation = world_model_trainer.evaluate_model(test_loaders)
            best_world_model_path = wm_model_path
        
        # Step 2: OPTIMIZED RL training with expert imitation focus
        logger.info("ğŸ¯ Starting OPTIMIZED RL training with expert imitation...")
        logger.info("ğŸ¯ Expected improvement: 6% â†’ 15-30% mAP")
        
        # Create optimized RL trainer
        optimized_rl_trainer = OptimizedRLTrainer(
            config=config,
            logger=logger,
            device=device
        )
        
        # Get training timesteps (increase for better convergence)
        timesteps = config.get('rl_training', {}).get('timesteps', 50000)  # Default 50k for better results
        
        logger.info(f"ğŸš€ Training OPTIMIZED RL algorithms for {timesteps} timesteps...")
        logger.info("ğŸ“ Key improvements:")
        logger.info("   âœ… Hierarchical action space for sparse surgical actions")
        logger.info("   âœ… Heavy expert demonstration matching rewards")
        logger.info("   âœ… Behavioral cloning integration")
        logger.info("   âœ… Curriculum learning")
        logger.info("   âœ… Optimized hyperparameters")
        
        # Train optimized RL algorithms
        rl_results = optimized_rl_trainer.train_all_optimized_algorithms(
            world_model, train_data, timesteps
        )
        
        # Calculate best estimated mAP
        best_estimated_map = 0.0
        best_method = None
        
        for method, result in rl_results.items():
            if result.get('status') == 'success':
                metrics = result.get('expert_imitation_metrics', {})
                estimated_map = metrics.get('estimated_map', 0.0)
                if estimated_map > best_estimated_map:
                    best_estimated_map = estimated_map
                    best_method = method
        
        return {
            'status': 'success',
            'world_model_path': best_world_model_path,
            'world_model_evaluation': world_model_evaluation,
            'world_model_pretrained': skip_wm_training,
            'rl_models': rl_results,
            'model_type': 'OptimizedConditionalWorldModel',
            'approach': f'OPTIMIZED: Expert-focused RL with behavioral cloning {"(PRETRAINED WM)" if skip_wm_training else "(TRAINED WM)"}',
            'method_description': f'World model-based RL with expert imitation focus {"(pretrained WM)" if skip_wm_training else "(trained WM)"}',
            'optimization_results': {
                'best_method': best_method,
                'best_estimated_map': best_estimated_map,
                'improvement_over_baseline': f"{best_estimated_map:.1%} vs 6% baseline",
                'target_achieved': best_estimated_map > 0.15
            },
            'improvements': [
                'Hierarchical action space for sparse surgical actions',
                'Expert demonstration matching rewards (F1, precision, recall)',
                'Behavioral cloning integration with warmup phase',
                'Curriculum learning with staged difficulty',
                'Optimized hyperparameters for imitation learning',
                'Heavy focus on expert imitation vs exploration',
                f'Expected mAP improvement: {best_estimated_map:.1%} vs 6% baseline'
            ]
        }
        
    except Exception as e:
        logger.error(f"âŒ Optimized Method 2 failed: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        return {'status': 'failed', 'error': str(e)}


def quick_test_optimized_rl(config_path: str = 'config_dgx_all_v7.yaml'):
    """
    Quick test function to verify the optimized RL works.
    Run this first to test before integrating into main experiment.
    """
    
    print("ğŸš€ QUICK TEST: Optimized RL for Expert Imitation")
    print("=" * 60)
    print("This tests the optimized RL before full integration")
    print("Expected: Significant improvement over 6% mAP baseline")
    print("")
    
    try:
        import yaml
        from datasets.cholect50 import load_cholect50_data
        
        # Load config
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Simple logger
        class QuickLogger:
            def __init__(self, log_dir):
                self.log_dir = log_dir
                Path(log_dir).mkdir(exist_ok=True)
            def info(self, msg): print(f"INFO: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
        
        logger = QuickLogger("quick_test_results")
        
        # Load small dataset for quick test
        print("ğŸ“‚ Loading test dataset...")
        train_data = load_cholect50_data(
            config, logger, 
            split='train', 
            max_videos=3  # Small test
        )
        
        test_data = load_cholect50_data(
            config, logger,
            split='test', 
            max_videos=1  # Small test
        )
        
        if not train_data:
            print("âŒ No training data loaded - check your data paths")
            return False
        
        print(f"âœ… Loaded {len(train_data)} train + {len(test_data)} test videos")
        
        # Run optimized RL test
        print("ğŸ¯ Running optimized RL test...")
        result = run_optimized_method2_wm_rl(
            config=config,
            logger=logger,
            train_data=train_data,
            test_data=test_data,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        if result.get('status') == 'success':
            optimization_results = result.get('optimization_results', {})
            best_estimated_map = optimization_results.get('best_estimated_map', 0.0)
            target_achieved = optimization_results.get('target_achieved', False)
            
            print("ğŸ‰ QUICK TEST COMPLETED!")
            print(f"ğŸ“Š Best estimated mAP: {best_estimated_map:.1%}")
            print(f"ğŸ¯ Target achieved (>15%): {'YES' if target_achieved else 'NO'}")
            
            if target_achieved:
                print("âœ… READY FOR MICCAI! Integrate into main experiment")
            elif best_estimated_map > 0.10:
                print("ğŸ“ˆ GOOD PROGRESS! Close to target")
            else:
                print("âš ï¸ NEEDS MORE TUNING")
            
            return True
        else:
            print(f"âŒ Quick test failed: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"âŒ Quick test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# INTEGRATION INSTRUCTIONS
def integration_instructions():
    """
    Print step-by-step integration instructions.
    """
    
    print("ğŸ”§ INTEGRATION INSTRUCTIONS")
    print("=" * 50)
    print()
    print("STEP 1: Save the optimized files")
    print("   - Save 'optimized_rl_env.py' in your project directory")
    print("   - Save 'optimized_rl_trainer.py' in your project directory")
    print()
    print("STEP 2: Replace RL method in run_experiment_v7.py")
    print("   Find the '_run_method2_wm_rl' function and replace it with:")
    print("   'run_optimized_method2_wm_rl' from this file")
    print()
    print("STEP 3: Quick test (RECOMMENDED)")
    print("   Run: python quick_integration_guide.py")
    print("   This will test the optimized RL on a small dataset")
    print()
    print("STEP 4: Update your config")
    print("   In config_dgx_all_v7.yaml, set:")
    print("   rl_training:")
    print("     timesteps: 50000  # Increase for better results")
    print("     action_space_type: 'hierarchical'  # New action space")
    print()
    print("STEP 5: Run full experiment")
    print("   python run_experiment_v7.py --config config_dgx_all_v7.yaml")
    print()
    print("ğŸ¯ EXPECTED RESULTS:")
    print("   - Previous RL mAP: ~6%")
    print("   - Optimized RL mAP: 15-30%")
    print("   - Significant improvement for MICCAI submission!")
    print()
    print("â° DEADLINE: 3 days to MICCAI workshop")
    print("ğŸ¯ MISSION: Beat supervised learning (40% mAP) or get close!")


if __name__ == "__main__":
    print("ğŸ¯ QUICK INTEGRATION FOR OPTIMIZED RL")
    print("=" * 60)
    
    # Print integration instructions
    integration_instructions()
    
    print("\n" + "="*60)
    print("ğŸ§ª RUNNING QUICK TEST...")
    
    # Run quick test
    success = quick_test_optimized_rl()
    
    if success:
        print("\nğŸ‰ QUICK TEST PASSED! Ready for integration")
        print("ğŸ’¡ Follow the integration instructions above")
        print("ğŸš€ Run the full experiment to achieve 15-30% mAP!")
    else:
        print("\nâŒ QUICK TEST FAILED!")
        print("ğŸ”§ Check your data paths and dependencies")
        print("ğŸ’¬ Contact for debugging support")
