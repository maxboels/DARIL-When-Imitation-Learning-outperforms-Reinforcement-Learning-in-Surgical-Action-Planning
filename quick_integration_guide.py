#!/usr/bin/env python3
"""
QUICK INTEGRATION GUIDE for Optimized RL
Replace your existing RL training with this optimized version

DEADLINE: 3 days to MICCAI workshop
GOAL: Achieve 15-30% mAP (vs current 6%)

QUICK STEPS:
1. Save the optimized_rl_env.py and optimized_rl_trainer.py files
2. Replace the _run_method2_wm_rl function in your run_experiment_v7.py
3. Run the experiment with optimized RL

Expected improvement: 6% ‚Üí 15-30% mAP
"""

import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any

# Import your existing components
from models.conditional_world_model import ConditionalWorldModel
from datasets.world_model_dataset import create_world_model_dataloaders

# Import the new optimized components
from optimized_rl_trainer import OptimizedRLTrainer


def run_optimized_method2_wm_rl(config: Dict, logger, train_data: List[Dict], test_data: List[Dict], device: str = 'cuda') -> Dict[str, Any]:
    """
    OPTIMIZED Method 2: Conditional World Model + Expert-Focused RL
    
    This replaces your existing _run_method2_wm_rl function with the optimized version.
    Expected to achieve 15-30% mAP vs previous 6%.
    """
    
    logger.info("üéØ OPTIMIZED Method 2: Conditional World Model + Expert-Focused RL")
    logger.info("-" * 70)
    
    try:
        # Check if pretrained world model is configured
        wm_config = config.get('experiment', {}).get('world_model', {})
        wm_model_path = wm_config.get('wm_model_path', None)
        
        # Determine if we should skip world model training
        skip_wm_training = wm_model_path and Path(wm_model_path).exists()
        
        if skip_wm_training:
            logger.info(f"üìÇ Using pretrained world model from: {wm_model_path}")
            world_model = ConditionalWorldModel.load_model(wm_model_path, device=device)
            logger.info("‚úÖ Pretrained world model loaded successfully")
            train_data_for_loader = None  # Skip world model training
        else:
            logger.info("üèãÔ∏è Will train world model from scratch")
            world_model = ConditionalWorldModel(
                hidden_dim=config['models']['conditional_world_model']['hidden_dim'],
                embedding_dim=config['models']['conditional_world_model']['embedding_dim'],
                action_embedding_dim=config['models']['conditional_world_model']['action_embedding_dim'],
                n_layer=config['models']['conditional_world_model']['n_layer'],
                num_action_classes=config['models']['conditional_world_model']['num_action_classes'],
                dropout=config['models']['conditional_world_model']['dropout']
            ).to(device)
            train_data_for_loader = train_data
        
        # Create datasets for world model training/evaluation
        train_loader, test_loaders, simulation_loader = create_world_model_dataloaders(
            config=config['data'],
            train_data=train_data_for_loader,
            test_data=test_data,
            batch_size=config['training']['batch_size'],
            num_workers=config['training']['num_workers']
        )
        
        # Step 1: Train or load world model
        if not skip_wm_training:
            logger.info("üåç Training world model from scratch...")
            from training.world_model_trainer import WorldModelTrainer
            
            world_model_trainer = WorldModelTrainer(
                model=world_model,
                config=config,
                logger=logger,
                device=device
            )
            
            best_world_model_path = world_model_trainer.train(train_loader, test_loaders)
            world_model_evaluation = world_model_trainer.evaluate_model(test_loaders)
        else:
            logger.info("üìä Evaluating pretrained world model...")
            from training.world_model_trainer import WorldModelTrainer
            
            world_model_trainer = WorldModelTrainer(
                model=world_model,
                config=config,
                logger=logger,
                device=device
            )
            world_model_evaluation = world_model_trainer.evaluate_model(test_loaders)
            best_world_model_path = wm_model_path
        
        # Step 2: OPTIMIZED RL training with expert imitation focus
        logger.info("üéØ Starting OPTIMIZED RL training with expert imitation...")
        logger.info("üéØ Expected improvement: 6% ‚Üí 15-30% mAP")
        
        # Create optimized RL trainer
        optimized_rl_trainer = OptimizedRLTrainer(
            config=config,
            logger=logger,
            device=device
        )
        
        # Get training timesteps (increase for better convergence)
        timesteps = config.get('rl_training', {}).get('timesteps', 50000)  # Default 50k for better results
        
        logger.info(f"üöÄ Training OPTIMIZED RL algorithms for {timesteps} timesteps...")
        logger.info("üéì Key improvements:")
        logger.info("   ‚úÖ Hierarchical action space for sparse surgical actions")
        logger.info("   ‚úÖ Heavy expert demonstration matching rewards")
        logger.info("   ‚úÖ Behavioral cloning integration")
        logger.info("   ‚úÖ Curriculum learning")
        logger.info("   ‚úÖ Optimized hyperparameters")
        
        # Train optimized RL algorithms
        rl_results = optimized_rl_trainer.train_all_optimized_algorithms(
            world_model, train_data, timesteps
        )
        
        # Calculate best estimated mAP
        best_estimated_map = 0.0
        best_method = None
        
        for method, result in rl_results.items():
            if result.get('status') == 'success':
                metrics = result.get('expert_imitation_metrics', {})
                estimated_map = metrics.get('estimated_map', 0.0)
                if estimated_map > best_estimated_map:
                    best_estimated_map = estimated_map
                    best_method = method
        
        return {
            'status': 'success',
            'world_model_path': best_world_model_path,
            'world_model_evaluation': world_model_evaluation,
            'world_model_pretrained': skip_wm_training,
            'rl_models': rl_results,
            'model_type': 'OptimizedConditionalWorldModel',
            'approach': f'OPTIMIZED: Expert-focused RL with behavioral cloning {"(PRETRAINED WM)" if skip_wm_training else "(TRAINED WM)"}',
            'method_description': f'World model-based RL with expert imitation focus {"(pretrained WM)" if skip_wm_training else "(trained WM)"}',
            'optimization_results': {
                'best_method': best_method,
                'best_estimated_map': best_estimated_map,
                'improvement_over_baseline': f"{best_estimated_map:.1%} vs 6% baseline",
                'target_achieved': best_estimated_map > 0.15
            },
            'improvements': [
                'Hierarchical action space for sparse surgical actions',
                'Expert demonstration matching rewards (F1, precision, recall)',
                'Behavioral cloning integration with warmup phase',
                'Curriculum learning with staged difficulty',
                'Optimized hyperparameters for imitation learning',
                'Heavy focus on expert imitation vs exploration',
                f'Expected mAP improvement: {best_estimated_map:.1%} vs 6% baseline'
            ]
        }
        
    except Exception as e:
        logger.error(f"‚ùå Optimized Method 2 failed: {e}")
        import traceback
        logger.error(f"Full traceback: {traceback.format_exc()}")
        return {'status': 'failed', 'error': str(e)}


def quick_test_optimized_rl(config_path: str = 'config_dgx_all_v7.yaml'):
    """
    Quick test function to verify the optimized RL works.
    Run this first to test before integrating into main experiment.
    """
    
    print("üöÄ QUICK TEST: Optimized RL for Expert Imitation")
    print("=" * 60)
    print("This tests the optimized RL before full integration")
    print("Expected: Significant improvement over 6% mAP baseline")
    print("")
    
    try:
        import yaml
        from datasets.cholect50 import load_cholect50_data
        
        # Load config
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Simple logger
        class QuickLogger:
            def __init__(self, log_dir):
                self.log_dir = log_dir
                Path(log_dir).mkdir(exist_ok=True)
            def info(self, msg): print(f"INFO: {msg}")
            def warning(self, msg): print(f"WARNING: {msg}")
            def error(self, msg): print(f"ERROR: {msg}")
        
        logger = QuickLogger("quick_test_results")
        
        # Load small dataset for quick test
        print("üìÇ Loading test dataset...")
        train_data = load_cholect50_data(
            config, logger, 
            split='train', 
            max_videos=3  # Small test
        )
        
        test_data = load_cholect50_data(
            config, logger,
            split='test', 
            max_videos=1  # Small test
        )
        
        if not train_data:
            print("‚ùå No training data loaded - check your data paths")
            return False
        
        print(f"‚úÖ Loaded {len(train_data)} train + {len(test_data)} test videos")
        
        # Run optimized RL test
        print("üéØ Running optimized RL test...")
        result = run_optimized_method2_wm_rl(
            config=config,
            logger=logger,
            train_data=train_data,
            test_data=test_data,
            device='cuda' if torch.cuda.is_available() else 'cpu'
        )
        
        if result.get('status') == 'success':
            optimization_results = result.get('optimization_results', {})
            best_estimated_map = optimization_results.get('best_estimated_map', 0.0)
            target_achieved = optimization_results.get('target_achieved', False)
            
            print("üéâ QUICK TEST COMPLETED!")
            print(f"üìä Best estimated mAP: {best_estimated_map:.1%}")
            print(f"üéØ Target achieved (>15%): {'YES' if target_achieved else 'NO'}")
            
            if target_achieved:
                print("‚úÖ READY FOR MICCAI! Integrate into main experiment")
            elif best_estimated_map > 0.10:
                print("üìà GOOD PROGRESS! Close to target")
            else:
                print("‚ö†Ô∏è NEEDS MORE TUNING")
            
            return True
        else:
            print(f"‚ùå Quick test failed: {result.get('error')}")
            return False
            
    except Exception as e:
        print(f"‚ùå Quick test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


# INTEGRATION INSTRUCTIONS
def integration_instructions():
    """
    Print step-by-step integration instructions.
    """
    
    print("üîß INTEGRATION INSTRUCTIONS")
    print("=" * 50)
    print()
    print("STEP 1: Save the optimized files")
    print("   - Save 'optimized_rl_env.py' in your project directory")
    print("   - Save 'optimized_rl_trainer.py' in your project directory")
    print()
    print("STEP 2: Replace RL method in run_experiment_v7.py")
    print("   Find the '_run_method2_wm_rl' function and replace it with:")
    print("   'run_optimized_method2_wm_rl' from this file")
    print()
    print("STEP 3: Quick test (RECOMMENDED)")
    print("   Run: python quick_integration_guide.py")
    print("   This will test the optimized RL on a small dataset")
    print()
    print("STEP 4: Update your config")
    print("   In config_dgx_all_v7.yaml, set:")
    print("   rl_training:")
    print("     timesteps: 50000  # Increase for better results")
    print("     action_space_type: 'hierarchical'  # New action space")
    print()
    print("STEP 5: Run full experiment")
    print("   python run_experiment_v7.py --config config_dgx_all_v7.yaml")
    print()
    print("üéØ EXPECTED RESULTS:")
    print("   - Previous RL mAP: ~6%")
    print("   - Optimized RL mAP: 15-30%")
    print("   - Significant improvement for MICCAI submission!")
    print()
    print("‚è∞ DEADLINE: 3 days to MICCAI workshop")
    print("üéØ MISSION: Beat supervised learning (40% mAP) or get close!")


if __name__ == "__main__":
    print("üéØ QUICK INTEGRATION FOR OPTIMIZED RL")
    print("=" * 60)
    
    # Print integration instructions
    integration_instructions()
    
    print("\n" + "="*60)
    print("üß™ RUNNING QUICK TEST...")
    
    # Run quick test
    success = quick_test_optimized_rl()
    
    if success:
        print("\nüéâ QUICK TEST PASSED! Ready for integration")
        print("üí° Follow the integration instructions above")
        print("üöÄ Run the full experiment to achieve 15-30% mAP!")
    else:
        print("\n‚ùå QUICK TEST FAILED!")
        print("üîß Check your data paths and dependencies")
        print("üí¨ Contact for debugging support")
